filename|text|bias_and_fairness|reliability_and_monitoring|privacy_and_security|transparency_and_explainability|responsible_implementation
9918734183106676.txt|Anna Zink   UNIVERSITY OF CHICAGO BOOTH SCHOOL OF BUSINESS, CENTER FOR APPLIED AI  Sarah Morriss   URBAN INSTITUTE   Anuj Gangopadhyaya  LOYOLA UNIVERSITY CHICAGO  with Ziad Obermeyer  UNIVERSITY OF CALIFORNIA, BERKELEY  September 2023  AI applications in health care are prone to biases that could perpetuate health  disparities. In this paper we study the ways in which AI may maintain, perpetuate, or  worsen inequitable outcomes in health care. We review current approaches to  evaluating and mitigating biased AI and potential applications of AI to address health  equities. Finally, we discuss current incentives for equitable AI and potential changes in  the regulation and policy space. As AI becomes increasingly embedded in the daily  operations of health care systems, it is imperative that we understand its risks and  evaluate its impact on health equity.   Introduction  Data has always been central to medicine, and practitioners have been using prediction models since  before the 1990s (Gail et al. 1989; Kononenko 2001). However, the digitization of health records and  claims data in the early 2000s greatly increased access to and use of health care data, spurring new  investment in prediction tools for the health care industry. Artificial Intelligence (AI) algorithms, which  HEALTH POLICY CENTER   Building Equitable Artificial Intelligence  in Health Care   Addressing Current Challenges and Exploring Future Opportunities  2  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE     include the subfields of machine learning and deep learning, use large datasets to make predictions, and  have been developed for insurers, hospitals, and physician groups to assist with decisions about patient  care, resource staffing, diagnosing conditions, and more. While the authors were unable to find precise  evidence on the full extent of AI diffusion in the health care sector today, we know that the FDA has  approved over 500 AI-enabled devices as of 2023, and that AI models for predicting medication  adherence, disease onset, and hospitalizations have been developed for insurers and population health  management groups (Gervassi et al. 2022).   Unequal Treatment at 20  This work is part of a series of publications that commemorates the 20th anniversary of the 2003  Institute of Medicine report, Unequal Treatment: Confronting Racial and Ethnic Disparities in Health  Care. This report found that people of color received lower-quality health care than white patients,  even when access-related factors were held constant. Two decades later, we still observe the same  inequities, which has motivated thought leaders to imagine how to redesign the health care system so it  works equitably.  The Institute of Medicine published Unequal Treatment: Confronting Racial and Ethnic Disparities in  Health Care in 2003 (Institute of Medicine 2003). The report contains no mention of AI or machine  learning. It was not that AI did not exist at that time—IBM’s AI algorithm Deep Blue beat the best chess  player in the world that same year—but the authors and many others at the time were unaware of the  role it would play in health care 20 years later. It was not until 2016, when ProPublica published an  article showing that a common risk assessment tool used in bail decisions was more likely to wrongly  flag Black defendants as high risk for reoffending, that the research community recognized the  immediacy of the issue (O’Neil 2016).1 Since then, there have been an overwhelming number of  examples across health care settings and clinical areas warning that AI could perpetuate or widen  disparities in health (AHRQ 2022). With the advent of ChatGPT and other generative AI models poised  to become embedded in company and individual workflows, evaluating the impact of AI on health equity  is as pressing as ever.2   In this paper, we explain the relationship between AI and health equity—in particular, the ways in  which AI may maintain, perpetuate, or worsen inequitable outcomes in health. We discuss current  approaches to understanding and addressing this issue. Our focus is on AI applications in health care;  however, many of the concepts and solutions presented are applicable to a broader set of algorithms  used in health care, including rule-based clinical decision support systems, which we also discuss. We  further review examples of AI applications that could be built to address health equity. Finally, we  discuss existing and possible incentives for equitable AI.   BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE   3    BOX 1  Definitions  Artificial Intelligence  The combination of computer science and robust datasets (e.g., structured data, images, text) to enable  problem-solving by developing algorithms which seek to create expert systems that make predictions or  classifications based on input data. In health care, AI is often used to analyze medical images or free text  to automate clinical tasks such as diagnosis, triage, note-taking, and even communication.  Machine Learning  A branch of AI and computer science that focuses on the use of data and algorithms to imitate the way  that humans learn, gradually improving its accuracy. In health care, machine learning is often used to  predict the likelihood of a health event or condition.  Sources: “What is Artificial Intelligence (AI)?” IBM. Accessed July 27, 2023. https://www.ibm.com/topics/artificial-intelligence;  “What is Machine Learning?” IBM. Accessed July 27, 2023. https://www.ibm.com/topics/machine-learning?lnk=fle.   AI and Health Equity   AI has a wide range of applications in health care, including medical image analysis, virtual assistants,  clinical decision support, predictive analytics, remote patient-monitoring, and health communication.  Even AI applications that don’t directly impact medical decisionmaking can affect health care use. For  example, AI algorithms are used to optimize appointment schedules in doctors’ offices. The potential  impact of these applications on health outcomes is significant: a patient who never gets to the doctor’s  office loses an opportunity for diagnosis and treatment. In the office, AI-generated information could  change a doctor’s decision about a course of treatment, influence which patients an insurer approves for  additional services or expensive treatments, or affect appointment wait times.   Given AI’s potential influence on patient care, it is important to evaluate the quality of predictions  generated by AI. We consider two common problem areas in AI applications that could perpetuate or  worsen heath disparities: (1) failure to generalize and (2) incorrect or inadequate prediction targets. We  highlight, using several examples, the consequences these errors have for health equity. We define  health equity as the state in which everyone has the opportunity to attain their best possible health.3  Equitable AI, therefore, is AI that moves us toward, rather than away from, that state, either by  incorporating health equity principles when building and deploying AI applications, or by developing AI  applications that target health equity directly.   When Predictions Don’t Generalize  AI algorithms learn from existing data in order to make new predictions. In health care, many AI  algorithms are trained on convenient samples of data collected for purposes outside of the prediction  problem. When AI algorithms make predictions on populations that differ from the training data, the  4  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE     predictive performance of the AI algorithm can decrease. Take, for example, AI for melanoma diagnosis:  on its surface, this is an area well suited for AI, since pictures of melanoma can be analyzed for cancer  risk. Many researchers rely on large skin-image repositories to train their AI algorithms; however, some  of these databases primarily collect images from fair-skinned populations in the United States, Europe,  and Australia. Research has shown that AI trained on these data performed worse on dark-skinned  populations (Adamson and Smith 2018). Including more training data for darker-skinned populations  improved the prediction quality for dark-skinned populations. However, generalizability is not just a  question of assessing who is in the dataset, but of understanding how the AI application will be used,  and whether predictions built using the training data will translate to these other settings. For example,  skin-image data used to train AI predictions may be collected in dermatologists’ offices, among patients  who have access to this type of specialist care, but the AI application could be deployed widely (on, for  example, a smartphone) to users who do not have similar access to specialty care and for whom the  training dataset may not generalize.  The inability to generalize predictions often results in exacerbating health inequities, since the  populations excluded or underrepresented in training data are those that have been historically  excluded or disadvantaged. The lack of diversity in clinical trials is well documented, and many historical  medical studies that inform clinical practice today predominately included white men (Dresser 1992).  Furthermore, because of extensive barriers to accessing health care data, AI researchers often train AI  models on a select few public databases (Johnson et al. 2016). These data are often collecting from  single sites, and AI algorithms are at risk of reduced predictive performance when used at other sites  (Röösli, Bozkurt, and Hernandez-Boussard 2022; Song et al. 2020). Even large national datasets like  health claims data only collect data on beneficiaries using the health care system, meaning that  populations without access, or with less access, are likely underrepresented in data compared to other  groups.   While data being representative is important to improve the predictive performance of AI  algorithms, it could also help increase clinician and patient trust in AI tools (Bibbins-Domingo Helman,  and Dzau 2022; Schwartz et al. 2023). Research has found that diversity in clinical trials increased  physician willingness to prescribe drugs, and patient trust in their efficacy, which could reduce  disparities in prescribing rates and increase trust in the medical system (Alsan et al. 2022). A parallel  argument could be made for AI: diversity in AI training data is not just a means to improve predictive  performance but a necessary step to increase trust in AI.   Incorrect or Inadequate Prediction Targets   Prediction targets are the measures AI algorithms are trained to predict. Common prediction targets  include the presence of a condition and the occurrence of an adverse event like rehospitalization. The  use of mismeasured or proxy outcomes in AI can reinforce disparities in care if those outcomes are  themselves influenced by factors driven by structural racism and inequities (Mullainathan and  Obermeyer 2021).   BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE   5    Take, for example, the common practice of using health insurance claims data to measure the  presence or absence of a condition like diabetes or heart disease. An individual is marked as having the  condition if they have a claim with diagnosis codes for that condition within a specified time window.  This means that observing heart disease or diabetes is conditional on the patient visiting the doctor and  the doctor recording the diagnosis codes on the claims. Patients with less access to care or facing  provider bias will be, on average, underdiagnosed.   There are many AI applications that use this type of data to predict the presence of a condition. For  example, several AI-based chest X-ray prediction models use public radiology datasets with recorded  diagnosis of a condition or event. One study evaluated these AI algorithms and found that female  patients, Black patients, Hispanic patients, and patients with lower socioeconomic status (with Medicaid  insurance) were more likely to be incorrectly predicted as not having a condition or event by the AI  algorithm compared to other patients (Seyyed-Kalantari et al. 2021). Because underserved patients  don’t go to the doctor’s office, they are less likely to have a chronic condition recorded on a health care  claim and are therefore not identified in AI algorithms using this as a prediction target. The AI algorithm  predicted the presence of the chronic condition on the claim, not the existence of the condition itself.   In the previous example, the correct prediction target had been selected—the occurrence of a  condition—but it was mismeasured in the data used to train the AI algorithm. In other problems, the  prediction target is not measurable. In such situations, one commonly selects a “proxy” target in place of  the actual target of interest. Proxy targets are measured variables correlated with the target of interest.  The use of proxy variables is very common in health care as well as in other fields, such as employment  and housing, where targets like “ability” or “good tenant” are not measurable.   As an example, population health management models are frequently used by private health  insurers to allocate care management services. The goal of these models is to predict patients who  would benefit from more care management services, but because improvement in health is difficult to  measure, health insurers use a proxy outcome. A common proxy outcome is annual health care costs.  One study found that a population health management algorithm proxying health care need with costs  allocated more care to white patients than to Black patients conditional on health needs (Obermeyer et  al. 2019). Because the proxy target was correlated with access to and use of health care services, it  identified frequent users of health care services, who were less likely to be Black patients given current  inequities in health care access (Manuel 2018).   Similar issues occur for predictors (the variables used to predict the target), which might cause  some variables to have differential power for different groups of patients. For example, family health  history is a common risk factor for many cancers. As a result, it is used to determine the timing and  frequency of preventive services and as an input in many risk models. However, family history has been  found to be more likely mismeasured for Black patients relative to white patients (Chavez-Yenter et al.  2022; Andoh 2023). Therefore, predictions relying on family history will not work as well for Black  patients, unless other risk predictors are able to make up for this loss of information.   6  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE     Evaluation   For many, AI represents a black box that produces remarkable and sometimes frightening results. The  challenges in understanding what AI is and how it works should not prevent physicians, clinicians, health  systems, researchers, patients, and other stakeholders from evaluating it with the same rigor and  precision as we do any other treatment, software, or tool in medicine.   How do we determine whether AI is equitable or not? Ideally, we would estimate the causal impact  of the introduction of an AI algorithm on equity (Kasy and Abebe 2021). Identifying the causal impact  would require either a randomized controlled trial or quasi-experimental methods and data on the  intervention. We would need data on those treated (i.e., units exposed to AI), those not treated (i.e.,  units not exposed to AI), group identifiers, empirical measures of equity, and any confounders (Groos et  al. 2018). These analyses require large investments to collect and measure data and long study time  horizons to observe outcomes. There has been little empirical work in this vein, likely due to financial  and operational constraints as well as challenges regarding data access.   Instead, research has focused on evaluations using more readily available data; typically, data  collected predeployment or at the point of deployment. These analyses primarily focus on (1) the  difference in predictive performance of an AI algorithm for different groups, and/or (2) the difference in  AI-suggested allocation of care or services for different groups. Differences in performance or  allocation across groups are quantitatively assessed using a set of measures, commonly referred to as  fairness measures (Barocas, Hardt, and Narayanan 2019; Verma and Rubin 2018). Fairness measures  often conflict with each other. For example, in the study on the population health management risk  predictor (Obermeyer et al. 2019), the algorithm was very good at predicting health care costs, and  fairness measures based on predictive performance would have found the algorithm fair: it was able to  predict health care costs equally well for both Black and white patients. It wasn’t until the researchers  considered how services were allocated that they noticed that more white patients were being referred  than Black patients given the same measured level of sickness. Because the prediction target was  biased, fairness measures based on predictive performance were unable to detect any problems. Which  fairness measures to prioritize will depend on the clinical settings and goals of the developer and/or  policymaker and, in general, it is best to use a suite of measures.4  The focus on measurement in this space may feel familiar to the field of health care quality  measurement.5 Health care quality, like fairness, is hard to measure and define. To try to piece together  a picture of health care quality more broadly, many different quality measures are used to capture not  only what a provider does to maintain or improve health but also health outcomes themselves. Health  care quality measures have become a central component of alternative payment models and other  policy initiatives to date. They are viewed as a necessary but imperfect means to assess quality:  measures are correlated with health care quality, but they can induce a “teach to the test” mentality,  and it is hard to attribute quality differences to organizations rather than to differences in the health of  the patients they serve. We can view fairness measures for AI with a similar lens. Measurement creates  a clearer picture of equitable AI, but it would be misguided to over-rely on select metrics and lose sight  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE   7    of the outcome we care about most: more equitable health. It is also difficult to estimate the effect of AI  on health disparities through observation alone.   Defining Groups   A pressing challenge in the evaluation of AI is the lack of group data needed to evaluate whether an AI  algorithm is equitable or not (Lu et al. 2022). These data are used to define and identify groups that  experience systemic discrimination in health care. Race/ethnicity/language data (commonly referred to  as REL data) and other group identifiers are not always collected or not collected with the same quality  as other variables in health care data.6 Without a means to measure groups, there is no way to evaluate  the impact of AI on that group. What’s more, there is the question of how to define groups. The federal  government as well as many states have put forth new standards on the measurement and definition of  REL data (SHADAC 2022).7 Many are calling for more granular race and ethnicity categories, since  coarse racial-ethnic group definitions commonly used in research fail to measure differences within  groups (Movva et al. 2023). Measures of intersectionality or finer groups can provide more information  but also raise statistical challenges (Ghasemi et al. 2021; Spielman, Folch, and Nagle 2014). In general,  efforts to disaggregate race and ethnicity data have been met with support and a necessary amount of  caution.8   Building More-Equitable AI  There are numerous resources available to help incorporate health equity principles into the AI  workflow (Chen et al. 2021; Diakopoulos et al. n.d.; Nelson et al. 2020; Obermeyer et al. 2021; Rajkomar  et al. 2018; Wang et al. 2022). One example is the bias evaluation checklist developed by Wang and  colleagues that helps one classify the risk (low, medium, high) of specific sources of bias that might occur  in the process of developing and deploying an algorithm (Wang et al. 2022). To best anticipate potential  risks, it is important to have contextual knowledge about inequities that exist in the given prediction  setting. There are also a number of technical solutions that have been proposed to reduce issues of bias  (Huang et al. 2022). These solutions can be bucketed into categories based on where they are addressed  in AI development (i.e., problem selection, data collection, defining the prediction target, algorithm  development, and post-deployment). Most of the solutions that have been presented to date focus on  outcome definition and algorithmic development, in part because these solutions can be addressed  immediately at the point of AI development. Other solutions, such as collecting more—and better—data  require time and financial investments. More evidence on how these strategies impact the design of AI  and improve health equity is needed.   Governance   Operationalizing algorithmic bias solutions requires coordination across multiple stages of  development and the work of many teams. What’s more, evaluation and monitoring practices are  ongoing: data shift, model degradation, and changes in user behavior can affect the fidelity of AI output  over time (Subbaswamy and Saria 2020). Such efforts require organizational oversight and buy-in  8  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE     (McCradden et al. 2020). Researchers have published internal auditing frameworks for organizations to  use (Raji et al. 2020). For hospitals that already have governance structures in place, expanding  processes to incorporate AI applications might be a natural extension: pioneers in this space have  shared plans for AI oversight (Bedoya et al. 2022). The ability and interest of organizations to establish  such processes remains to be seen.  Can AI Self-Correct?  Could health care AI be trained to recognize its own inequities and self-correct? AI alignment is a  subfield in AI research that addresses whether and how to incorporate human preferences and ethical  goals in the development of AI. Ethical preferences must be computed empirically and incorporated into  AI algorithm objectives. However, we’ve discussed how fairness measures that encode these values can  contradict each other. While a monitoring system could be set up to warn of changes in metrics or data  shift, a human is still required to specify priorities. Priorities could be set by AI developers or regulated  by policy, but in the end, human intervention will always be needed to imbue AI with equitable  principles.   Race as a Predictor  In prediction problems, one is typically taught to include any variable that has predictive power. Race  and ethnicity are commonly correlated with clinical outcomes and can be accurately predicted from  most datasets. Therefore, they have historically been included as clinical risk predictors. However, a re- examination is underway regarding the use of patient racial and ethnic information in AI and other  algorithms, out of concern that their inclusion in risk prediction models might increase health disparities  (Vyas, Eisenstein, and Jones 2020).   The interpretation of race as a variable in a prediction model is often not clearly articulated. This  has led to harmful misinterpretations of race differences as biological in origin in many medical models  (Cerdeña, Plaisime, and Tsai 2020). As an alternative to these “race-based” medical models, race- conscious medicine explicitly defines race as a sociological and power construct (Cerdeña, Plaisime, and  Tsai 2020). This focuses research on the effects of structural racism and reduces health inequities.  Differences in predicted risk by race group reflect health disparities resulting from the deleterious  effect of racism on health (Bailey et al. 2017; Borrell et al. 2021). Structural racism has been shown to be  a key determinant of health disparities; for example, segregated housing has led to a higher risk of  asthma (Bailey et al. 2017). Race, then, is a proxy for health disparities and the effects of structural  racism.   If this is the case, then we could try to find better predictors than race to account for disparities in  health due to racism and other social determinants of health. As an example, consider the recent update  to calculating estimated glomerular filtration rate (eGFR) used to assess kidney functioning. Previously,  the algorithm used a race-based adjustment to inflate eGFR estimates for Black patients; however, the  removal of race from eGFR calculations underestimated eGFR in Black patients. Neither of these  solutions is desired: one leads to potential underuse of treatment and the other to overuse. However,  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE   9    researchers found that including cystatin C in the estimation of GFR removed differences in predictive  accuracy by race (Williams et al. 2021). In other settings, the physiological risk factors or other clinical  measures that explain risk differences between race groups might not always be identified or available  for prediction, in which case researchers could try to measure the source of the health disparity directly:  for example, if researchers believe that unobserved differences in health risks are a result of structural  racism, then this could be incorporated into the risk predictor. How to measure structural racism in  health care data is an open question that a number of researchers are actively working on (Groos et al.  2018; Hardeman et al. 2022).  When new predictor variables explain risk differences, race is no longer predictive and drops out of  the AI algorithm on its own accord. But in the situation where new variables are not available or  measured, race can still be an effective means to measure differences in risk for patients and may be the  only way to account for health disparities in prediction models (Manski 2022). Thus, race as a predictor  can help predict key outcomes of interest and point researchers in productive directions to understand  and measure the source of disparities.   Research Applications  How can AI be harnessed to improve health equity? AI has already played a positive role in promoting  equity by providing researchers with new tools to explore and address biases that exist in the health  care system today (Chen, Joshi, and Ghassemi 2020). For example, AI-built techniques to analyze image  data allowed researchers to discover that current methods for diagnosing knee pain in MRIs failed to  identify knee pain experienced by many Black patients (Pierson et al. 2021). This study identified  differences in patient-reported knee pain versus clinician diagnosis of knee pain, and attributed a  portion of these differences to information in the knee X-ray that was not considered by doctors during  diagnosis. This research could impact clinical practice (i.e., how doctors read knee X-rays) and the set of  patients identified with osteoporosis, which would have implications on health outcomes if it affected  patient treatment decisions. Using AI to discover new information in medical image data might change  the way these data are interpreted by medical professionals.   This research suggests that there is more that could be learned from medical image data than what  doctors are trained to look for. What’s more, AI can be used to tap into these data, which might be less  biased than traditional data sources that reflect inequities in our health care system. For example, what  if, instead of using claims data to identify levels of sickness, we used image and lab data while correcting  for racial differences in access to these services? These data have proven to be very predictive of health  events and conditions and might provide a truer signal to what we want to measure: health. For  instance, ECGs are commonly used to predict heart failure. What other conditions might they also be  able to predict? Would it be possible to use the ECG as input to a predictor instead of documented  conditions, which might be poorly or unevenly documented in claims data?  Image and lab data offer one opportunity for new, potentially less biased sources of data, but AI  could also be used to collect data that is uncorrelated with access to health care settings. There are  10  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE     many examples of applications like wearable or at-home monitoring devices that collect health data  outside of the hospital or doctor’s office. Patient access to these devices might be limited, but 97  percent of Americans own a cell phone, and 93 percent use the internet.9 One study showed that search  data could be used to understand health information needs (Abebe et al. 2019). Patient advocacy  forums and other online spaces could provide data on conditions, including common symptoms, that are  not documented in the claims data today. These data could be vulnerable to their own biases and should  be thoroughly evaluated before use, in addition to requiring data privacy safeguards.10   AI can not only be used to analyze and collect new data but also to remove biases in existing data.  Society has long recognized that humans make biased decisions. One solution has been to blind  decisionmakers from the sources of information we don’t want them to use. For example, we blind  journal reviewers from authors’ identities and orchestra judges from candidates’ looks. AI can be used  as a tool for similar purposes. For example, AI methods have been developed to remove gender and  racial differences in the documentation of clinical notes before they are used for prediction problems  (Zhang et al. 2020). Other researchers found that AI algorithms could detect which lab site had  produced images for biopsy (Howard et al. 2021). While this information might be valuable in some  settings, for certain prediction problems it should be removed. These methods do not address biases  that arise from differences in access to care—and therefore the presence or absence of a clinical note or  a lab test—but focus on removing the predictive link between the data and a sensitive attribute such as  race or gender conditional on the data having been collected.  The use of AI for communication is another robust area of research (Butow and Hoque 2020). The  ability of AI to interpret and analyze sound and text provides unique opportunities to recognize and  identify cultural subtleties and nuances in communication with patients. Culture is a key factor in  successful health communication for both medical decisionmaking and health promotion and has been  identified as one strategy for reducing health disparities (Betsch et al. 2016; Kreuter and McClure  2004). Leveraging AI to understand and therefore improve health communication could help patients  become more active participants in their health and health care decisionmaking.   Finally, AI can create data to explain the decisions of AI and humans alike (Singla et al. 2020). For  example, by studying how AI-generated perturbations of data impact prediction results, we can learn  more about what predictors are influencing the result (Chen, Joshi, and Ghassemi 2020). AI can also  help form new hypotheses about human behavior (Ludwig and Mullainathan 2023). For example,  researchers determined what specific features of a face affected bail decisions using AI-generated  faces. These data hold facial features constant (e.g., skin color, gender representation, and aging) while  varying one specific feature (e.g., thinness of face) to test for the effect of that feature on bail  decisions.11 Analogous studies could be done in health care (Miller et al. 2019). Understanding the  source of potential biases can help identify biased decisionmaking.   Targeting Adoption  Research shows that 20 percent of doctors in the US treat 80 percent of Black patients and that these  doctors are more resource-constrained (Bach et al. 2004). We also know that there is high variation in  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE   11    diagnostic skill and that mistakes are more likely to occur among low-skilled, low-resourced doctors.  Thus, moving low-skilled doctors to higher skill levels presents an opportunity to improve the average  quality of care received among Black patients. AI offers a chance to close that gap (Chan, Gentzkow, and  Yu 2019). As an example, research has found that AI can be used to help physicians diagnose heart  attack (Mullainathan and Obermeyer 2022). AI can not only improve skill level within specialties, but  also provide specialist tools for primary care doctors. For example, the AI-enabled medical software  IDx-DR helps primary care doctors identify patients at risk of eye retinopathy, a condition typically  diagnosed only by eye care professionals (Grzybowski and Brona 2021). Medicare and other private  insurers have started paying for IDx-DR and other similar types of AI products. Reimbursement will  likely improve adoption, but to ensure the biggest impact, adoption could be incentivized in areas where  it would be most beneficial, through grant funding or increased reimbursement rates in geographic  areas that lack specialists or have low-quality care.   Incentives for Equitable AI  Twenty years ago, the Institute of Medicine documented many examples of racial and ethnic disparities  in Unequal Treatment. Knowledge of disparity and the moral imperative for equity has not, to date,  spurred noteworthy changes in the trajectories of these outcomes: we continue to observe large racial  and ethnic disparities in access to care, access to quality care, and health outcomes. Why then should we  expect that the knowledge of inequitable AI alone would spur action?   What incentives exist to date for equitable AI? Equitable AI solutions take time and expertise: one  study team found that an algorithmic audit on two hospital-based care models took 115 person-hours  and 8–10 months to complete (Lu et al. 2022). Thus, from a business standpoint, the decision to invest in  equitable AI solutions will depend on the private returns of equitable AI. This includes responding to  changes in consumer sentiment and demand for equitable care plus reducing exposure to systematic  and costly patient grievances cases. Entering 2023, hospitals faced higher expenses, depressed finances,  and negative margins (Kaufman Hall 2022). Health insurers appear to be in slightly better financial  positions. It is hard to predict how much investment will be made in this area without external pressure.   If there isn’t a current business incentive for equitable AI, then government action might serve to  reduce the costs and/or increase the benefits of equitable AI. Recent announcements of public and  private funding will likely help incentivize more research and work in this area. The White House  recently announced that it would initiate new investments to fund responsible AI research and  development. This includes $140 million in funding for the National Science Foundation to launch  several institutes focused on public assessments of existing generative AI systems.12 Private  organizations are also providing funding to evaluate AI used in diagnostic decisionmaking.13   Regulating Equitable AI   There are several regulatory options available to federal agencies to promote equity. As an example,  let’s consider approaches that the Centers for Medicare & Medicaid Services (CMS) could employ.  12  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE     Under the Affordable Care Act, CMS has experimented with offering rewards and/or levying penalties  to compel health systems to provide better-quality care. Health care quality is typically represented  through measures that feed into public-facing scorecards or are used by CMS to set penalties and  bonuses. These measures have not historically included social drivers of health, despite their huge  importance for health outcomes (Hood et al. 2016). However, several new measures have been  proposed to assess the social determinants of health in order to allocate resources to improve health  equity.14 CMS could also create a measure of equitable AI to incentivize the adoption of equitable AI  practices directly. This measure could be incorporated into public reporting programs such as the  Hospital Inpatient Quality Reporting Program so that patients might use information on whether  organizations are adopting equitable AI practices to exert demand-side pressure. The measure could  also be incorporated into bonus and penalty formulas to incentivize organizations to invest in better AI  processes. While these approaches focus on solutions available to CMS and, therefore, only apply  directly to the Medicare and Medicaid programs, they could still affect a wide range of organizations:  the majority of Medicaid beneficiaries are on privately run managed care organizations, and soon the  majority of Medicare beneficiaries are expected to be enrolled in Medicare Advantage plans (MedPAC  2023).15 Furthermore, almost every hospital treats Medicare and Medicaid patients and would be  affected by these policies.   Various agencies and branches of the federal government have indicated that regulating AI is a top  concern for them. In 2022, President Joe Biden released a blueprint for an AI Bill of Rights outlining five  principles to guide the design, use, and deployment of AI to protect people against its harms.16 The Food  and Drug Administration has published a beta version of how it plans to regulate AI used in health care  (FDA 2021). Other organizations such as the Center for Medicare and Medicaid Innovation are  considering how to incorporate these principles into pilot programs.17 The Department of Health and  Human Services has issued a notice of proposed rulemaking to revise Section 1557 of the Patient  Protection and Affordable Care Act. This revision, if finalized, would explicitly prohibit discrimination in  the use of clinical algorithms to support decisionmaking in covered entities.18 State legislatures have  also shown interest in this issue: in August 2022, the California Attorney General issued a letter to all  hospitals requesting that they share how they are identifying and addressing racial and ethnic  disparities in commercial decisionmaking tools.19 It is still an open question how federal and local  agencies will regulate algorithms moving forward.  Conclusion  AI is an increasingly important part of health care decisionmaking. It is therefore vital that AI  applications are developed and monitored with health equity in mind. While there has been much work  in this area to date, from research on identifying and improving algorithmic bias to resources for  building more equitable AI, to the development of incentives for equitable AI—be that regulation or  funding—there is still much to be done. We need more research measuring the impact of AI on health  equity. This includes quasi-experimental methods and randomized controlled trials as well as qualitative  studies engaging affected communities such as doctors and patients to understand the specific  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE   13    mechanisms by which AI systems affect them. We should also be carefully monitoring adoption patterns  of AI use among health care professionals and patients to understand whether AI’s benefits are fairly  distributed. Furthermore, we should not only be focused on the evaluation of AI applications that  already exist, but also think critically about the set of problems we are trying to solve with AI: can we  focus on applications that push us toward health equity rather than just ensuring new applications don’t  create further harm? Finally, while this article focuses on solutions to improving AI, it is imperative that  we do not lose focus on addressing the underlying issues at the root of this discussion: the pervasive  health inequities that exist in this country as a result of structural racism and discrimination.   Notes    1 Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner, “Machine Bias,” ProPublica, May 23, 2016,  https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.   2 Benji Edwards, “GPT-4 Will Hunt for Trends in Medical Records Thanks to Microsoft and Epic,” Ars Technica, April  18, 2023, https://arstechnica.com/information-technology/2023/04/gpt-4-will-hunt-for-trends-in-medical- records-thanks-to-microsoft-and-epic/.   3 P. Braveman, E. Arkin, T. Orleans, D. Proctor, and A. Plough, “What Is Health Equity?,” Robert Wood Johnson  Foundation, May 1, 2017, https://www.rwjf.org/en/insights/our-research/2017/05/what-is-health-equity-.html.   4 Deborah Hellman, “Measuring Algorithmic Fairness,” Virginia Law Review 106 (4), June 1, 2020,  https://virginialawreview.org/articles/measuring-algorithmic-fairness/.   5 “Types of Health Care Quality Measures,” Agency for Healthcare Research and Quality (AHRQ), last reviewed  July 2015, https://www.ahrq.gov/talkingquality/measures/types.html.   6 Neil P. Rowen, Brianna Van Stekelenburg, Raman Nohria, Robert S. Saunders, and Rebecca G. Whitaker, “How to  Improve Race, Ethnicity, and Language Data and Disparities Interventions,” Health Affairs Forefront (blog), Health  Affairs, September 14, 2022, https://www.healthaffairs.org/content/forefront/improve-race-ethnicity-and- language-data-and-disparities-interventions.   7 Office of Management and Budget (OMB), “Initial Proposals for Updating OMB’s Race and Ethnicity Statistical  Standards,” Federal Register (blog), National Archives, January 27, 2023,  https://www.federalregister.gov/documents/2023/01/27/2023-01635/initial-proposals-for-updating-ombs- race-and-ethnicity-statistical-standards.   8 Farah Kader, Lan N. Đoàn, Matthew Lee, Matthew K. Chin, Simona C. Kwon, and Stella S. Yi, “Disaggregating  Race/Ethnicity Data Categories: Criticisms, Dangers, and Opposing Viewpoints,” Health Affairs Forefront (blog),  Health Affairs, March 25, 2022, https://www.healthaffairs.org/content/forefront/disaggregating-race-ethnicity- data-categories-criticisms-dangers-and-opposing.  9 “Mobile Fact Sheet,” Pew Research Center, April 7, 2021, https://www.pewresearch.org/internet/fact- sheet/mobile/; “Internet/Broadband Fact Sheet,” Pew Research Center, April 7, 2021,  https://www.pewresearch.org/internet/fact-sheet/internet-broadband/.   10 Emma Woollacott, “Apple Sued over ‘Racial Bias’ of Apple Watch,” Forbes, December 29, 2022,  https://www.forbes.com/sites/emmawoollacott/2022/12/29/apple-sued-over-racial-bias-of-apple-watch/.   11 “The Face Effect: When a Face Is a Felony,” Center for Applied Artificial Intelligence, accessed April 27, 2023,  https://faceeffect.ai/.   12 “FACT SHEET: Biden-Harris Administration Announces New Actions to Promote Responsible AI Innovation that  Protects Americans’ Rights and Safety,” The White House, May 4, 2023, https://www.whitehouse.gov/briefing- room/statements-releases/2023/05/04/fact-sheet-biden-harris-administration-announces-new-actions-to- promote-responsible-ai-innovation-that-protects-americans-rights-and-safety/.     14  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE       13 “Augmented Intelligence in Medicine and Healthcare Initiative (AIM-HI),” Kaiser Permanente—Division of  Research, 2023, https://divisionofresearch.kaiserpermanente.org:443/projects/aim- hi?kp_shortcut_referrer=kp.org/aim-hi.   14 Debbie Chang and Rachel Nuzum, “Now Is the Time for Measuring Social Drivers of Health in Medicare,  Medicaid, and the Children’s Health Insurance Program,” To the Point (blog), The Commonwealth Fund, March  28, 2022, https://www.commonwealthfund.org/blog/2022/now-time-measuring-social-drivers-health- medicare-medicaid-and-childrens-health-insurance.   15 “Share of Medicaid Population Covered under Different Delivery Systems,” State Health Facts, KFF, as of July 1,  2022, https://www.kff.org/medicaid/state-indicator/share-of-medicaid-population-covered-under-different- delivery-systems/.   16 “Blueprint for an AI Bill of Rights,” Office of Science and Technology Policy, The White House, accessed January  16, 2023, https://www.whitehouse.gov/ostp/ai-bill-of-rights/.  17 Melissa Majerol and Dora Lynn Hughes, “CMS Innovation Center Tackles Implicit Bias,” Health Affairs Forefront  (blog), Health Affairs, July 5, 2022, https://www.healthaffairs.org/content/forefront/cms-innovation-center- tackles-implicit-bias.  18 “Section 1557 of the Patient Protection and Affordable Care Act,” US Department of Health and Human Services,  last reviewed February 3, 2023, https://www.hhs.gov/civil-rights/for-individuals/section-1557/index.html.   19 “Attorney General Bonta Launches Inquiry into Racial and Ethnic Bias in Healthcare Algorithms,” Office of the  Attorney General, State of California Department of Justice, August 31, 2022, https://oag.ca.gov/news/press- releases/attorney-general-bonta-launches-inquiry-racial-and-ethnic-bias-healthcare.   References  Abebe, Rediet, Shawndra Hill, Jennifer Wortman Vaughan, Peter M. Small, and H. Andrew Schwartz. 2018. “Using  Search Queries to Understand Health Information Needs in Africa.” arXiv doi:10.48550/arXiv.1806.05740.  Adamson, Adewole S., and Avery Smith. 2018. “Machine Learning and Health Care Disparities in Dermatology.”  JAMA Dermatology 154 (11): 1247–48. doi:10.1001/jamadermatol.2018.2348.  AHRQ (Agency for Healthcare Research and Quality). 2022. Impact of Healthcare Algorithms on Racial and  Ethnic Disparities in Health and Healthcare. Rockville, MD: AHQR.  https://effectivehealthcare.ahrq.gov/products/racial-disparities-health-healthcare/protocol.  Alsan, Marcella, Maya Durvasula, Harsh Gupta, Joshua Schwartzstein, and Heidi L. Williams. 2022. “Representation  and Extrapolation: Evidence from Clinical Trials.” Working Paper 30575. Cambridge, MA: National Bureau of  Economic Research. doi:10.3386/w30575.  Andoh, Joana E. 2023. “The Stories We Don’t Know.” JAMA 329 (18): 1551. doi:10.1001/jama.2023.5891   Bach, Peter B., Hoangmai H. Pham, Deborah Schrag, Ramsey C. Tate, and J. Lee Hargraves. 2004. “Primary Care  Physicians Who Treat Blacks and Whites.” New England Journal of Medicine 351 (6): 575–84.  doi:10.1056/NEJMsa040609.  Bailey, Zinzi D., Nancy Krieger, Madina Agénor, Jasmine Graves, Natalia Linos, and Mary T. Bassett. 2017.  “Structural Racism and Health Inequities in the USA: Evidence and Interventions.” Lancet 389 (10077): 1453–63.  doi:10.1016/S0140-6736(17)30569-X.  Barocas, Solon, Moritz Hardt, and Arvind Narayanan. 2019. Fairness and Machine Learning: Limitations and  Opportunities. fairmlbook.org. https://fairmlbook.org/.   Bedoya, Armando D., Nicoleta J. Economou-Zavlanos, Benjamin A. Goldstein, Allison Young, J. Eric Jelovsek, Cara  O’Brien, Amanda B. Parrish, Scott Elengold, Kay Lytle, Suresh Balu, Erich Huang, Eric G. Poon, and Michael J.  Pencina. 2022. “A Framework for the Oversight and Local Deployment of Safe and High-Quality Prediction  Models.” Journal of the American Medical Informatics Association 29 (9): 1631–36. doi:10.1093/jamia/ocac078.   BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE   15    Betsch, Cornelia, Robert Böhm, Collins O. Airhihenbuwa, Robb Butler, Gretchen B. Chapman, Niels Haase, Benedikt  Herrmann, Tasuku Igarashi, Shinobu Kitayama, Lars Korn, Ülla-Karin Nurm, Bernd Rohrmann, Alexander J.  Rothman, Sharon Shavitt, John A. Updegraff, and Ayse K. Uskul. 2016. “Improving Medical Decision Making and  Health Promotion through Culture-Sensitive Health Communication: An Agenda for Science and Practice.”  Medical Decision Making 36 (7): 811–33. doi:10.1177/0272989X15600434.  Bibbins-Domingo, Kirsten, Alex Helman, and Victor J. Dzau. 2022. “The Imperative for Diversity and Inclusion in  Clinical Trials and Health Research Participation.” JAMA 327 (23): 2283–84. doi:10.1001/jama.2022.9083.  Borrell, Luisa N., Jennifer R. Elhawary, Elena Fuentes-Afflick, Jonathan Witonsky, Nirav Bhakta, Alan H. B. Wu,  Kirsten Bibbins-Domingo, José R. Rodríguez-Santana, Michael A. Lenoir, James R. Gavin, III, Rick A. Kittles, Noah  A. Zaitlen, David S. Wilkes, Neil R. Powe, Elad Ziv, and Esteban G. Burchard. 2021. “Race and Genetic Ancestry in  Medicine—A Time for Reckoning with Racism.” New England Journal of Medicine 384 (5): 474–80.  doi:10.1056/NEJMms2029562.  Butow, Phyllis, and Ehsan Hoque. 2020. “Using Artificial Intelligence to Analyse and Teach Communication in  Healthcare.” Breast 50:49–55. doi:10.1016/j.breast.2020.01.008.  Cerdeña, Jessica P., Marie V. Plaisime, and Jennifer Tsai. 2020. “From Race-Based to Race-Conscious Medicine:  How Anti-Racist Uprisings Call Us to Act.” Lancet 396 (10257): 1125–28. doi:10.1016/S0140-6736(20)32076-6.  Chan Jr, David C., Matthew Gentzkow, and Chuan Yu. 2019. “Selection with Variation in Diagnostic Skill: Evidence  from Radiologists.” Working Paper 26467. Cambridge, MA: National Bureau of Economic Research.  doi:10.3386/w26467.  Chavez-Yenter, Daniel, Melody S. Goodman, Yuyu Chen, Xiangying Chu, Richard L. Bradshaw, Rachelle Lorenz  Chambers, Priscilla A. Chan, Brianne M. Daly, Michael Flynn, Amanda Gammon, Rachel Hess, Cecelia Kessler,  Wendy K. Kohlmann, Devin M. Mann, Rachel Monahan, Sara Peel, Kensaku Kawamoto, Guilherme Del Fiol,  Meenakshi Sigireddi, Saundra S. Buys, Ophira Ginsburg, and Kimberly A. Kaphingst. 2022. “Association of  Disparities in Family History and Family Cancer History in the Electronic Health Record with Sex, Race, Hispanic  or Latino Ethnicity, and Language Preference in 2 Large US Health Care Systems.” JAMA Network Open 5 (10):  e2234574. doi:10.1001/jamanetworkopen.2022.34574.  Chen, Irene Y., Shalmali Joshi, and Marzyeh Ghassemi. 2020. “Treating Health Disparities with Artificial  Intelligence.” Nature Medicine 26:16–17. doi:10.1038/s41591-019-0649-2.  Chen, Irene Y., Emma Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, and Marzyeh Ghassemi. 2021. “Ethical  Machine Learning in Healthcare.” Annual Review of Biomedical Data Science 4:123–44. doi:10.1146/annurev- biodatasci-092820-114757.  Diakopoulos, Nicholas, Sorelle Friedler, Marcelo Arenas, Solon Barocas, Michael Hay, Bill Howe, H. V. Jagadish, Kris  Unsworth, Arnaud Sahuguet, Suresh Venkatasubramanian, Christo Wilson, Cong Yu, and Bendert Zevenbergen.  n.d. “Principles for Accountable Algorithms and a Social Impact Statement for Algorithms.”  Fairness,  Accountability, and Transparency in Machine Learning (FAT/ML). Accessed August 12, 2023.  https://www.fatml.org/resources/principles-for-accountable-algorithms.  Dresser, Rebecca. 1992. “Wanted: Single, White Male for Medical Research.” Hastings Center Report 22 (1): 24–29.  doi:10.2307/3562720.  FDA (US Food & Drug Administration). 2021. Artificial Intelligence/Machine Learning (AI/ML)–Based Software as a  Medical Device (SaMD) Action Plan. Silver Spring, MD: FDA.  Gail, Mitchell H., Louise A. Brinton, David P. Byar, Donald K. Corle, Sylvan B. Green, Catherine Schairer, and John J.  Mulvihill. 1989. “Projecting Individualized Probabilities of Developing Breast Cancer for White Females Who  Are Being Examined Annually.” Journal of the National Cancer Institute 81 (24): 1879–86.  doi:10.1093/jnci/81.24.1879.  Gervasi, Stephanie S., Irene Y. Chen, Aaron Smith-McLallen, David Sontag, Ziad Obermeyer, Michael Vennera, and  Ravi Chawla. 2022. “The Potential for Bias in Machine Learning and Opportunities for Health Insurers to  Address It.” Health Affairs 41 (2): 212–18. doi:10.1377/hlthaff.2021.01287.  16  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE     Ghasemi, Elham, Reza Majdzadeh, Fatemeh Rajabi, AbouAli Vedadhir, Reza Negarandeh, Ensiyeh Jamshidi,  Amirhossein Takian, and Zahra Faraji. 2021. “Applying Intersectionality in Designing and Implementing Health  Interventions: A Scoping Review.” BMC Public Health 21 (1407). doi:10.1186/s12889-021-11449-6.  Groos, Maya, Maeve Wallace, Rachel Hardeman, and Katherine P. Theall. 2018. “Measuring Inequity: A Systematic  Review of Methods Used to Quantify Structural Racism.” Journal of Health Disparities Research and Practice 11 (2).  https://digitalscholarship.unlv.edu/jhdrp/vol11/iss2/13.  Grzybowski, Andrzej, and Piotr Brona. 2021. “Analysis and Comparison of Two Artificial Intelligence Diabetic  Retinopathy Screening Algorithms in a Pilot Study: IDx-DR and Retinalyze.” Journal of Clinical Medicine 10 (11):  2352. doi:10.3390/jcm10112352.  Hardeman, Rachel R., Patricia A. Homan, Tongtan Chantarat, Brigette A. Davis, and Tyson H. Brown. 2022.  “Improving the Measurement of Structural Racism to Achieve Antiracist Health Policy.” Health Affairs 41 (2):  179–86. doi:10.1377/hlthaff.2021.01489.  Hood, Carlyn M., Keith P. Gennuso, Geoffrey R. Swain, and Bridget B. Catlin. 2016. “County Health Rankings:  Relationships between Determinant Factors and Health Outcomes.” American Journal of Preventive Medicine 50  (2): 129–35. doi:10.1016/j.amepre.2015.08.024.  Howard, Frederick M., James Dolezal, Sara Kochanny, Jefree Schulte, Heather Chen, Lara Heij, Dezheng Huo, Rita  Nanda, Olufunmilayo I. Olopade, Jakob N. Kather, Nicole Cipriani, Robert L. Grossman, and Alexander T.  Pearson. 2021. “The Impact of Site-Specific Digital Histology Signatures on Deep Learning Model Accuracy and  Bias.” Nature Communications 12 (4423). https://www.nature.com/articles/s41467-021-24698-1.  Huang, Jonathan, Galal Galal, Mozziyar Etemadi, and Mahesh Vaidyanathan. 2022. “Evaluation and Mitigation of  Racial Bias in Clinical Machine Learning Models: Scoping Review.” JMIR Medical Informatics 10 (5): e36388.  doi:10.2196/36388.  Institute of Medicine  Committee on Understanding and Eliminating Racial and Ethnic Disparities in Health Care.  2003. “Unequal Treatment: Confronting Racial and Ethnic Disparities in Health Care”. Washington, DC: National  Academies Press. https://doi.org/10.17226/12875.  Johnson, Alistair E. W., Tom J. Pollard, Lu Shen, Li-wei H. Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin  Moody, Peter Szolovits, Leo Anthony Celi, and Roger G. Mark. 2016. “MIMIC-III, a Freely Accessible Critical  Care Database.” Scientific Data 3 (160035). doi:10.1038/sdata.2016.35.  Kasy, Maximilian, and Rediet Abebe. 2021. “Fairness, Equality, and Power in Algorithmic Decision-Making.” In  Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. Association for Computing  Machinery, March 2021: 576–586. doi:10.1145/3442188.3445919.  Kaufman Hall. 2022. The Current State of Hospital Finances: Fall 2022 Update. Chicago: American Hospital  Association. https://www.aha.org/guidesreports/2022-09-15-current-state-hospital-finances-fall-2022-update.  Kononenko, Igor. 2001. “Machine Learning for Medical Diagnosis: History, State of the Art and Perspective.”  Artificial Intelligence in Medicine 23 (1): 89–109. https://doi.org/10.1016/S0933-3657(01)00077-X.  Kreuter, Matthew W., and Stephanie M. McClure. 2004. “The Role of Culture in Health Communication.” Annual  Review of Public Health 25:439–55. doi:10.1146/annurev.publhealth.25.101802.123000.  Lu, Jonathan, Amelia Sattler, Samantha Wang, Ali Raza Khaki, Alison Callahan, Scott Fleming, Rebecca Fong,  Benjamin Ehlert, Ron C. Li, Lisa Shieh, Kavitha Ramchandran, Michael F. Gensheimer, Sarah Chobot, Stephen  Pfohl, Siyun Li, Kenny Shum, Nitin Parikh, Priya Desai, Briththa Seevaratnam, Melanie Hanson, Margaret Smith,  Yizhe Xu, Arjun Gokhale, Steven Lin, Michael A. Pfeffer, Winifred Teuteberg, Nigam H. Shah. 2022.  “Considerations in the Reliability and Fairness Audits of Predictive Models for Advance Care Planning.” Frontiers  in Digital Health 4. https://doi.org/10.3389/fdgth.2022.943768.  Ludwig, Jens, and Sendhil Mullainathan. 2023. “Machine Learning as a Tool for Hypothesis Generation.” Working  Paper No. 31017. Cambridge, MA: National Bureau of Economic Research.  https://www.nber.org/papers/w31017.   Manski, Charles F. 2022. “Patient-Centered Appraisal of Race-Free Clinical Risk Assessment.” Health Economics.  https://pubmed.ncbi.nlm.nih.gov/35791466/.  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE   17    Manuel, Jennifer I. 2018. “Racial/Ethnic and Gender Disparities in Health Care Use and Access.” Health Services  Research 53 (3): 1407–1429. https://pubmed.ncbi.nlm.nih.gov/28480588/.  McCradden, Melissa D., Shalmali Joshi, Mjaye Mazwi, and James A. Anderson. 2020. “Ethical Limitations of  Algorithmic Fairness Solutions in Health Care Machine Learning.” Lancet Digital Health 2 (5): e221–e223.  https://doi.org/10.1016/S2589-7500(20)30065-0.  MedPAC (Medicare Payment Advisory Commission). 2023. March 2023 Report to the Congress: Medicare Payment  Policy. Washington, DC: MedPAC.  Miller, Andrew, Ziad Obermeyer, John Cunningham, and Sendhil Mullainathan. 2019. “Discriminative  Regularization for Latent Variable Models with Applications to Electrocardiography.” Proceedings of Machine  Learning Research 97:4585–94. https://proceedings.mlr.press/v97/miller19a.html.   Movva, Rajiv, Divya Shanmugam, Kaihua Hou, Priya Pathak, John Guttag, Nikhil Garg, and Emma Pierson. 2023.  “Coarse Race Data Conceals Disparities in Clinical Risk Score Performance.” arXiv. .  https://doi.org/10.48550/arXiv.2304.09270.  Mullainathan, Sendhil, and Ziad Obermeyer. 2021. “On the Inequity of Predicting A While Hoping for B.” AEA Papers  and Proceedings 111:37–42. doi:10.1257/pandp.20211078.  Mullainathan, Sendhil, and Ziad Obermeyer. 2022. “Diagnosing Physician Error: A Machine Learning Approach to  Low-Value Health Care.” The Quarterly Journal of Economics 137 (2): 679–727. doi: 10.1093/qje/qjab046.  Nelson, Amy Hawn, Della Jenkins, Sharon Zanti, Matthew Katz, Emily Berkowitz, T. C. Burnett, and Dennis Culhane.  2020. A Toolkit for Centering Racial Equity Throughout Data Integration. Philadelphia, PA: Actionable Intelligence  for Social Policy. https://aisp.upenn.edu/wp-content/uploads/2022/07/AISP-Toolkit_5.27.20.pdf.   Obermeyer, Ziad, R. Nissan, M. Stern, S. Eaneff, Emily Joy Bembeneck, and Sendhil Mullainathan. 2021. Algorithmic  Bias Playbook. Chicago, IL: Chicago Booth, Center for Applied Artificial Intelligence.   Obermeyer, Ziad, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. “Dissecting Racial Bias in an  Algorithm Used to Manage the Health of Populations.” Science 366 (6464): 447–53.  doi:10.1126/science.aax2342.   O’Neil, Cathy. 2016. Weapons of Math Destruction. Harlow, England: Penguin Books.  Pierson, Emma, David M. Cutler, Jure Leskovec, Sendhil Mullainathan, and Ziad Obermeyer. 2021. “An Algorithmic  Approach to Reducing Unexplained Pain Disparities in Underserved Populations.” Nature Medicine 27:136–40.  doi:10.1038/s41591-020-01192-7.   Raji, Inioluwa Deborah, Andrew Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila  Smith-Loud, Daniel Theron, and Parker Barnes. 2020. “Closing the AI Accountability Gap: Defining an End-to- End Framework for Internal Algorithmic Auditing.” In Proceedings of the 2020 Conference on Fairness,  Accountability, and Transparency 33–44. doi:10.1145/3351095.3372873.  Rajkomar, Alvin, Michaela Hardt, Michael D. Howell, Greg Corrado, and Marshall H. Chin. 2018. “Ensuring Fairness  in Machine Learning to Advance Health Equity.” Annals of Internal Medicine 169 (12): 866–72. doi:10.7326/M18- 1990.  Röösli, Eliane, Selen Bozkurt, and Tina Hernandez-Boussard. 2022. “Peeking into a Black Box, the Fairness and  Generalizability of a MIMIC-III Benchmarking Model.” Scientific Data 9 (24). doi:10.1038/s41597-021-01110-7.  Schwartz, Aaron L., Marcella Alsan, Alanna A. Morris, and Scott D. Halpern. 2023. “Why Diverse Clinical Trial  Participation Matters.” New England Journal of Medicine 388:1252–54. doi:10.1056/NEJMp2215609.  Seyyed-Kalantari, Laleh, Haoran Zhang, Matthew B. A. McDermott, Irene Y. Chen, and Marzyeh Ghassemi. 2021.  “Underdiagnosis Bias of Artificial Intelligence Algorithms Applied to Chest Radiographs in Under-Served Patient  Populations.” Nature Medicine 27:2176–82. doi:10.1038/s41591-021-01595-0.  SHADAC (State Health Access Data Assistance Center). Collection of Race, Ethnicity, Language (REL) Data on Medicaid  Applications: New and Updated Information on Medicaid Data Collection Practices in the States, Territories, and District  of Columbia. Issue Brief, November 30, 2022. State Health and Value Strategies.   18  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE     Singla, Sumedha, Brian Pollack, Junxiang Chen, and Kayhan Batmanghelich. 2020. “Explanation by Progressive  Exaggeration.” arXiv. Last revised February 10, 2020. doi:10.48550/arXiv.1911.00483.  Song, Xing, Alan S. L. Yu, John A. Kellum, Lemuel R. Waitman, Michael E. Matheny, Steven Q. Simpson, Yong Hu, and  Mei Liu. 2020. “Cross-Site Transportability of an Explainable Artificial Intelligence Model for Acute Kidney Injury  Prediction.” Nature Communications 11 (5668). doi:10.1038/s41467-020-19551-w.  Spielman, Seth E., David Folch, and Nicholas Nagle. 2014. “Patterns and Causes of Uncertainty in the American  Community Survey.” Applied Geography 46:147–57. doi:10.1016/j.apgeog.2013.11.002.  Subbaswamy, Adarsh, and Suchi Saria. 2020. “From Development to Deployment: Dataset Shift, Causality, and  Shift-Stable Models in Health AI.” Biostatistics 21 (2): 345–52. doi:10.1093/biostatistics/kxz041.  Verma, Sahil, and Julia Rubin. 2018. “Fairness Definitions Explained.” Proceedings of the International Workshop  on Software Fairness. 1–7. https://doi.org/10.1145/3194770.3194776   Vyas, Darshali A., Leo G. Eisenstein, and David S. Jones. 2020. “Hidden in Plain Sight—Reconsidering the Use of  Race Correction in Clinical Algorithms.” New England Journal of Medicine 383:874–82.  doi:10.1056/NEJMms2004740.  Wang, H. Echo, Matthew Landers, Roy Adams, Adarsh Subbaswamy, Hadi Kharrazi, Darrell J. Gaskin, and Suchi  Saria. 2022. “A Bias Evaluation Checklist for Predictive Models and Its Pilot Application for 30-Day Hospital  Readmission Models.” Journal of the American Medical Informatics Association 29 (8): 1323–33.  doi:10.1093/jamia/ocac065.  Williams, Winifred W., Joseph W. Hogan, and Julie R. Ingelfinger. 2021. “Time to Eliminate Health Care Disparities  in the Estimation of Kidney Function.” New England Journal of Medicine 385:1804–06.  https://doi.org/10.1056/NEJMe2114918.  Zhang, Haoran, Amy X. Lu, Mohamed Abdalla, Matthew McDermott, and Marzyeh Ghassemi. 2020. “Hurtful  Words: Quantifying Biases in Clinical Contextual Word Embeddings.” arXiv. Published online March 11, 2020.  https://doi.org/10.48550/arXiv.2003.11515.    BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE   19    About the Authors  Anna Zink is a principal researcher at the Center for Applied AI at the University of Chicago Booth  School of Business where she works on their Algorithmic Bias Initiative along with other projects  related to the development and adoption of AI in health care. She is interested in the possibilities and  pitfalls of machine learning techniques to evaluate and improve decisionmaking in health care and  insurance design. Her research on health plan payment studies how to balance fairness and efficiency  goals in developing risk-adjustment formulas. Zink received her PhD in health policy from Harvard  University.   Sarah Morriss is a research assistant in the Health Policy Center at the Urban Institute. She analyzes  data and provides assistance with questionnaire development for Urban’s Health Reform Monitoring  Survey and Well-Being and Basic Needs Survey. She also contributes to policy briefs and papers on  topics related to health equity, health care access, and families’ experiences with federal safety net  programs. Her research interests include disability and mental health policy issues. Moriss has a  bachelor’s degree in economics and public policy from the University of Chicago.  Anuj Gangopadhyaya is an assistant professor of economics at Loyola University Chicago and was  previously a senior research associate in the Health Policy Center at the Urban Institute. His research  focuses on the impact of safety net programs on health and well-being, family income, and education  achievement outcomes for children in low-income families. He has focused on the impact of Medicaid  eligibility expansion on children’s education achievement, maternal and child health effects of the  earned income tax credit program, and the impact of the Affordable Care Act Medicaid expansion on  adult labor supply and fertility rates of women of reproductive age. Gangopadhyaya received his PhD in  economics from the University of Illinois at Chicago.  Ziad Obermeyer is an associate professor and Blue Cross of California distinguished professor at UC  Berkeley, where he works at the intersection of machine learning and health. His research uses machine  learning as a tool to help doctors make better decisions, and help researchers make new discoveries—by  “seeing” the world the way algorithms do. He has also shown how widely used algorithms affecting  millions of patients automate and scale up racial bias. That work has affected how many organizations  build and use algorithms, and how lawmakers and regulators hold AI accountable. He is a Chan  Zuckerberg Biohub investigator and a faculty research fellow at the National Bureau of Economic  Research, and he was named an Emerging Leader by the National Academy of Medicine. Previously, he  was assistant professor at Harvard Medical School, and continues to practice emergency medicine in  underserved communities.    20  BUILDING EQUITABLE ARTIFICIAL INTELLIGENCE IN HEALTH CARE     Acknowledgments  This paper was prepared for the Urban Institute’s “Unequal Treatment at 20” initiative with generous  support from the Robert Wood Johnson Foundation, the Commonwealth Fund, the Episcopal Health  Foundation, and the California Health Care Foundation. We are grateful to them and to all our funders,  who make it possible for Urban to advance its mission. The views expressed are those of the authors and  should not be attributed to the Urban Institute, its trustees, or its funders. Funders do not determine  research findings or the insights and recommendations of Urban experts. Further information on the  Urban Institute’s funding principles is available at urban.org/funding principles. The authors gratefully  acknowledge helpful comments, suggestions, and guidance from the Chicago Booth Center for Applied  AI, Advisory Committee members Nicole Stern and Jose Guillem, Urban’s Community Advisory Board,  and Urban Institute reviewers Judah Axelrod, Brian Smedley, Kima Joy Taylor, and Faith Mitchel.   The views expressed are those of the authors and should not be attributed to the Urban Institute,  its trustees, or its funders. Funders do not determine research findings or the insights and  recommendations of Urban experts. Further information on the Urban Institute’s funding principles is  available at urban.org/fundingprinciples.  ABOUT THE URBAN INSTITUTE  The Urban Institute is a nonprofit research organization that provides data and  evidence to help advance upward mobility and equity. We are a trusted source for  changemakers who seek to strengthen decisionmaking, create inclusive economic  growth, and improve the well-being of families and communities. For more than 50  years, Urban has delivered facts that inspire solutions—and this remains our charge  today.  Copyright © September 2023. Urban Institute. Permission is granted for  reproduction of this file, with attribution to the Urban Institute.   500 L’Enfant Plaza SW  Washington, DC 20024  www.urban.org|1|1|0|0|1
AI-Supplement-Dr.-Alaa-Youssef-Presentation-508.txt|Breakout Session 4: Track B Ethical Considerations in the Design and  Conduct Clinical Trials of AI: A Qualitative Study  of Investigators' Experiences with Autonomous  AI for Diabetic Retinopathy Dr. Alaa Youssef Post-Doctoral Scholar, Stanford University School of Medicine Ethical Considerations in the Design and  Conduct of Clinical Trials of AI: Investigators'  Experiences with Autonomous AI for Pediatric  Diabetic Retinopathy Presenter: Alaa Youssef, PhD   Investigators: Danton Char, MD,M A S  S T A N F O R D  U N I V E R S I T Y  S C H O O L  O F  M E D I C I N E Risa Wolf, MD J O H N  H O P K I N S  H O S P I T A L FUNDING  ACKNOWLEDGEMENT NIH ODSS to National Eye Institute R01EY033233-01  Parent Award PI: Risa Wolf, MD Supplementary Award Title: Presenter:  Alaa Youssef, PhD  Post-Doctoral Fellow , Department of Radiology Stanford University School  of Medicine Recipient Investigator:  Danton Char, MD,M A S  Associate Professor of Anesthesia and Medical Ethics,  Stanford University School  of Medicine NIH Clinical Center PROJECT SUMMARY Study Objectives To determine the ethical considerations investigators  encountered and negotiated, designing and  conducting the first NIH-funded RCT of an  autonomous AI and related clinical trials.  Research Question How do clinical investigators recognize and  navigate ethical issues in the design and conduct of  clinical trials of AI? AI for Childrens  diabetiC Eye  ExamS (ACCESS)  Trial  Wolf, R.M., et al. (2024) METHODS Study Design  • Qualitative study using semi-structured interviews  with investigators involved in the design and  conduct of clinical trials of AI for diabetic  retinopathy screening. Participants • We employed purposeful sampling to engage  investigators from the ACCESS study. • We used snowball sampling for additional insights  from those involved in related trials of  autonomous AI.  RESULTS • We interviewed a total of eleven participants. • Six were from the NIH-funded ACCESS RCT,  including investigators, regulators, biostatistician. • Three investigators from an RCT in a developing  country. • Two investigators from the private sector. KEY THEMES There were unresolved ethical questions for all  seven principles. These issues included: • Measuring social value • Establishing scientific validity • Ensuring fair subject selection  • Determining risk-benefit ratios • Obtaining  informed consent Social Value What are social values of AI that should guide  design of study outcomes, and how could these  outcomes be measured? • Difficulty in defining and agreeing on the social value AI adds to  clinical trials. • Challenges in designing study outcomes that effectively  measure the social value of AI. ”I think we should see what [social value] is  from the patient’s perspective that would be  beneficial and then identify and measure that  potentially with a quantitative metric.” (008) Scientific Validity What do you compare the AI to, to ensure  that the trial is scientifically valid? • The challenge of integrating AI model outputs into  existing clinical workflows adds complexity to trials. • Difficulty in finding an appropriate benchmark for AI,  unlike more straightforward comparisons in drug trials “I think — you know — there are some issues  with AI around [scientific validity] because it’s  more of a systems intervention...it’s hard to see  whether individual randomization really  makes sense.” (001) Fair Subject  Selection How do you select trial participants fairly,  when current access to care (and gold  standard validations) is already disparate? • Fair subject selection emerged as a critical focus area. • Challenges in using AI tools to expand screening access due  to existing health disparities and socioeconomic factors. “It’s tricky to ensure equitable access to AI screening,  especially for those less likely to receive regular diabetes  care. Monitoring the prevalence of diabetic retinopathy  post-AI implementation and understanding the reasons  behind disparities in screening rates and follow-up care  are crucial.” (002) Informed Consent What are important barriers to informed  consent in clinical trials of AI? • Key barriers include privacy concerns and the challenge of  explaining AI's technical aspects for informed decisions. • Difficulty in ensuring understanding of privacy and  confidentiality with AI tools. • Transparency in participants' data use for AI development. “The short answer is... that the informed consent that  we’ve been using for a very long time, and really not  suitable for digital health and not suitable for AI for  many, many reasons.  And IRB committees in general  don’t understand AI, so the whole system needs to be  reconsidered.” (009) CONCLUSION This study highlights practical ethical challenges investigators need  to consider and negotiate in conducting clinical trials of AI,  exemplified by the diabetic retinopathy screening use-case.  FUTURE WORK • Expand empirical research to understand ethical  challenges in diverse clinical settings. • Develop comprehensive ethical frameworks tailored  to AI's unique attributes in clinical research. • Strengthen normative guidelines to safeguard patient  safety in AI trials. REFERENCES • Youssef, A., Nichol, A., Martinez, N., Larson, D.B., Abramoff, M., Wolf, R., Char, D. Ethical Considerations in the Design and Conduct Clinical Trials of AI : A Qualitative Study of Investigators' Experiences with Autonomous AI for Diabetic Retinopathy. (In Submission) • Wolf, R.M., Channa, R., Liu, T.Y.A. et al. Autonomous artificial intelligence increases screening and follow-up for diabetic retinopathy in youth: the ACCESS randomized control trial. Nat Commun 15, 421 (2024). https://doi.org/10.1038/s41467-023-44676-z • Abràmoff MD, Tobey D, Char DS. Lessons Learned About Autonomous AI: Finding a Safe, Efficacious, and Ethical Path Through the Development Process. Am J Ophthalmol. 2020 Jun;214:134-142. doi: 10.1016/j.ajo.2020.02.022. • Alaa Youssef, Michael Abramoff & Danton Char (2023) Is the Algorithm Good in a Bad World, or Has It Learned to be Bad? The Ethical Challenges of “Locked” Versus “Continuously Learning” and “Autonomous” Versus “Assistive” AI Tools in Healthcare, The American Journal of Bioethics, 23:5, 43-45, DOI: 10.1080/15265161.2023.2191052 • Emanuel EJ, Wendler D, Grady C. What Makes Clinical Research Ethical? JAMA. 2000;283(20):2701–2711. doi:10.1001/jama.283.20.2701 Contact|1|0|0|1|1
preparedness-metrics-report.txt|"Measuring Preparedness for Public  Health and Health Care Emergencies   The Current State of Preparedness Metrics in the United  States and Considerations for the Future       Report submitted August 30, 2024    Submitted to:  Allison Kolbe  Office of the Assistant Secretary for Planning and Evaluation (ASPE)  U.S. Department of Health and Human Services  200 Independence Ave. S.W.  Washington, DC 20201  Work performed under:  Contract No. HHSP233201500035I, Task Order No. 75P00122F37070  Disclaimer  This research was performed by Mathematica. The findings and conclusions of this research are those of  the authors and do not necessarily represent the views of ASPE or the U.S. Department of Health and  Human Services (HHS). Links and references to information from non-governmental organizations are  provided for informational purposes and are not an HHS endorsement, recommendation, or preference  for the non-governmental organizations.  Authors  Katie Morrison Lee, MPP, Sue Felt-Lisk, MPA, Camille Veri, MPA, Sheila Hoag, MA, Emily Crabtree, MPH  Acknowledgments   The authors wish to thank the Technical Expert Panel members (Appendix D) for their valuable  contributions to this work, as well as Chris Palo, Effie Metropoulos, Jill Miller, Dorothy Bellow, and Sheryl  Friedlander at Mathematica. We would also like to acknowledge the following individuals:    Kevin Yeskey, MD, MDB Inc.  Nicholas Cagliuso, PhD, MPH, MDB Inc.  Allison Kolbe, PhD, ASPE  Deborah Porterfield, MD, MPH, ASPE  Contract Officer’s Representative, Marsha Clarke, PhD, ASPE  Kim Nguyen, DVM, MPH, ASPE  Kathleen Miller PhD, MS, ASPE  Laurin Grabowsky, MSc, ASPE  Jessica White, MPP, ASPE    Contents  Mathematica® Inc.  ii    Contents  Executive Summary .......................................................................................................................................................................................... v  A.  Background .................................................................................................................................................................................... v  B.  The current state of public health and health care preparedness metrics in the United States ................. v  C.  Strategies to improve measurement of public health and health care preparedness ................................. viii  I.  Introduction .............................................................................................................................................................................................. 1  II.  The Current State of Public Health and Health Care Preparedness Metrics in the United States ......................... 5  A.  What public health and health care preparedness tools are currently available in the United  States? ............................................................................................................................................................................................... 6  B.  What are the gaps in existing public health and health care preparedness metrics? .................................. 16  C.  What lessons learned from the COVID-19 pandemic can inform measurement of emergency  preparedness and response at STLT public health agencies in the future? ...................................................... 22  III.  Strategies to Improve Measurement of Public Health and Health Care Preparedness .......................................... 24  A.  What key attributes should new public health and health care preparedness measures have,  and what gaps would they address? ................................................................................................................................. 24  B.  What strategies could potentially be explored to improve measurement of public health and  health care preparedness? .................................................................................................................................................... 27  C.  Discussion .................................................................................................................................................................................... 35  References ......................................................................................................................................................................................................... 36  Appendix A. Methods .................................................................................................................................................................................. A.1  Appendix B. Tools to Measure State, Local, Tribal, and Territorial Public Health and Health Care  Preparedness in the United States............................................................................................................................... B.1  Appendix C. Summary of Literature Assessing the Extent to Which Preparedness Indices Predicted  Outcomes During the COVID-19 Pandemic ............................................................................................................ C.1  Appendix D. Technical Expert Panel Participants ............................................................................................................................ D.1  Appendix E. Technical Expert Panel Agenda ...................................................................................................................................... E.1    Exhibits  Mathematica® Inc.  iii  Exhibits    Exhibit ES.1. List and description of tools to assess STLT public health and healthcare preparedness ........................ vi  Exhibit ES.2. Summary of key characteristics of existing public health and health care preparedness  metrics and their gaps and limitations .................................................................................................................................. vi  Exhibit ES.3. Four strategies that could potentially advance preparedness measurement, and potential  follow-up efforts for consideration ....................................................................................................................................... viii  Exhibit I.1. What is public health and health care preparedness? ................................................................................................. 1  Exhibit I.2. FEMA’s National Preparedness Goal and the five phases of emergency preparedness ................................ 1  Exhibit I.3. An all-hazards approach to preparedness ........................................................................................................................ 2  Exhibit I.4. Entities that might make up a local public health emergency preparedness system ..................................... 3  Exhibit I.5. Why measure preparedness? ................................................................................................................................................. 3  Exhibit I.6. Study research questions ......................................................................................................................................................... 4  Exhibit II.1. Types of preparedness metrics ............................................................................................................................................ 5  Exhibit II.2. List and description of tools to assess STLT public health and healthcare preparedness ........................... 6  Exhibit II.3. Prominent global tools to measure national public health and health care preparedness ........................ 7  Exhibit II.4. Characteristics of sets of capabilities from FEMA, CDC, and ASPR ....................................................................... 8  Exhibit II.5. Example of national preparedness scores using national trend data on the NHSPI website .................. 11  Exhibit II.6. Examples of measures that assess various aspects of the five phases of emergency  management from select preparedness tools, including the NHSPI, COPI, HPP measure set,  and ADEPT ....................................................................................................................................................................................... 12  Exhibit II.7. Examples of tools to measure health and social vulnerability or resilience78 ................................................ 13  Exhibit II.8. Examples of tools that measure components of STLT preparedness for specific disasters ..................... 14  Exhibit II.9. Examples of data sources used to measure preparedness in existing tools, by level (state  and/or local) that the data are available ............................................................................................................................. 15  Exhibit II.10. Spotlight on the technical expert panel: Factors that are inadequately captured in existing  metrics ............................................................................................................................................................................................... 16  Exhibit II.11. Examples of metrics that quantify public trust in government ......................................................................... 19  Exhibit II.12. Local health departments’ perception of preparedness by threat ................................................................... 20  Exhibit III.1. Key attributes of preparedness measures and the gaps they addressa .......................................................... 25  Exhibit III.2. Four strategies that could potentially advance preparedness measurement ............................................... 27  Exhibits  Mathematica® Inc.  iv  Exhibit III.3. Survey instruments to measure the strength of partnerships among STLT public health  departments and their partners.............................................................................................................................................. 28  Exhibit III.4. Contextual factors affecting public health preparedness and response outcomes ................................... 29  Exhibit III.5. Potential approaches to address strategy #1, and likely resource intensity of each ................................. 29  Exhibit III.6. Example of health equity in local hazard-specific preparedness metrics: ...................................................... 31  Exhibit III.7. Potential approaches to address strategy #2 and likely resource intensity of each .................................. 32  Exhibit III.8. What is an after-action report? ........................................................................................................................................ 32  Exhibit III.9. Examples of non-public data sources that could be leveraged to improve measurement of  preparedness .................................................................................................................................................................................. 33  Exhibit III.10. Potential approaches to address strategy #3, and likely resource-intensity of each .............................. 34  Exhibit III.11. Potential approaches to address strategy #4, and likely resource-intensity of each .............................. 35  Exhibit A.1. Identification of literature via databases and supplemental searches ............................................................ A.2  Exhibit A.2. Characteristics of preparedness metrics assessed in the analysis of themes and gaps ........................... A.3    Executive Summary  Mathematica® Inc.  v  Executive Summary  A. Background  A wide range of recent domestic disasters—from wildfires to the COVID-19 pandemic—have highlighted  the challenge of preparing for large-scale public health emergencies. Inadequate preparation for these  disasters has resulted in preventable loss of life, diminished public trust in federal, state, tribal, local, and  territorial (STLT) governments, and ongoing confusion about actions needed to improve preparedness.   To help the federal government and STLT jurisdictions better prepare for emergencies, there is a need to  understand how prepared different jurisdictions are for various emergencies. Understanding a  jurisdiction’s level of preparedness can inform resource allocation and identify actions that the federal  government and STLT jurisdictions can take to bolster preparedness, such as developing formal response  plans, training public health and health staff, or forming contractual agreements with partner  organizations. However, assessing whether a jurisdiction is prepared for different emergencies is  inherently complex and there is a lack of consensus among practitioners and scholars on how to approach  preparedness measurement. Measurement tools introduced in recent decades have numerous limitations,  such as inconsistently defining preparedness and its goals, relying on subjective agency assessments of  the standards and capabilities that contribute to preparedness, and failing to provide an evidence base for  measures. The cross-sectoral, cross-jurisdictional nature of public health systems adds to the complexity  of preparedness measurement: because of the many agencies and organizations involved in emergency  response efforts, it is challenging to understand how performance should be measured and accountability  distributed across these partners. Further, the singularity of public health emergencies makes it difficult to  assess whether key takeaways from one disaster will apply to the next.  In response to these challenges, the U.S. Department of Health and Human Services Office of the Assistant  Secretary for Planning and Evaluation (ASPE) funded a study to address 1) the current state of metrics for  public health and health care preparedness in the United States, including gaps in existing metrics and  limitations of existing metrics identified during the COVID-19 pandemic and 2) strategies to potentially  improve measurement of public health and health care preparedness and address the gaps and  limitations in current metrics. The methods for this study include a synthesis of key findings from a  targeted environmental scan of domestic preparedness metrics and a technical expert panel (TEP) made  up of representatives from federal agencies, public health and healthcare organizations, and academic  institutions, with diverse experience in preparedness measurement and emergency response.   B. The current state of public health and health care preparedness metrics in the  United States  We conducted an environmental scan to understand tools currently available to assess STLT preparedness,  including indices, measure sets, and other instruments such as self-administered preparedness surveys.I  We identified nine tools to measure STLT emergency preparedness, including three that have also been  used to assess national preparedness in the United States (Exhibit ES.1).     I We define indices as tools that assess preparedness across a range of measures and create a composite score  summarizing a jurisdiction’s preparedness. Measure sets similarly assess preparedness across a variety of measures  but do not produce a summary statistic.   Executive Summary  Mathematica® Inc.  vi  Exhibit ES.1. List and description of tools to assess STLT public health and healthcare  preparedness  Tool  Description  Tools for internal and external stakeholders  Community Outbreak  Preparedness Index (COPI)   Assesses county-level preparedness for infectious disease outbreaks using  publicly available data  Hospital Medical Surge  Preparedness Index (HMSPI)   Evaluates the capacity of hospitals to handle patient surges during mass casualty  events, using over 120 measures from publicly available data sources  Hospital Preparedness Program  (HPP) performance measure set  Assesses preparedness of HPP funding recipients based on 22 performance  measures reported by recipients and disseminated broadly  National Health Security  Preparedness Index (NHSPI)   Generates a composite preparedness score for states, territories, and the nation  overall based on 130 measures derived from publicly available data sources  Trust for America’s Health  (TFAH) Ready or Not tool  Evaluates states’ preparedness for public health emergencies using a targeted set  of 10 measures largely derived from the NHSPI  Self-administered tools for internal use by STLT jurisdictions  Assessment for Disaster  Engagement with Partners Tool  (ADEPT)   Summarizes the frequency and nature of activities related to disaster  preparedness, response, and recovery that local health departments engage in  with community-based organizations, using a 15-item index for use by local  health departments  Connectivity Measurement Tool  Quantifies the level of connectivity of different organizations and systems  involved in public health preparedness across 28 items  Preparedness Capacity  Assessment Survey (PCAS)   Creates an aggregate score summarizing preparedness of local health  departments  Rapid Urban Health Security  Assessment (RUHSA)   Evaluates local-level health security capacities across 46 measures  We present a summary of key characteristics of these tools, as well as gaps and limitations as identified by  the literature and the TEP, in Exhibit ES.2.  Exhibit ES.2. Summary of key characteristics of existing public health and health care  preparedness metrics and their gaps and limitations  Characteristic  Summary of existing metrics and their gaps/ limitations  Purpose and  users  Tools vary in their target audience:  • Five tools are intended for internal and external stakeholders. Results from these tools are  publicly disseminated for use by a broad audience of federal and STLT policy makers, public  health and health care organizations, and the general public.   • Four tools require self-administration and are intended for internal users, such as local health  department staff and their partners.   Gaps and limitations: Tools intended for broad internal and external audiences may not feel  actionable for STLT users that face challenges interpreting and adapting scores to their local  contexts.   Executive Summary  Mathematica® Inc.  vii  Characteristic  Summary of existing metrics and their gaps/ limitations  Jurisdiction  levels  Of the nine STLT tools:   • Two tools (the NHSPI and the TFAH tool) assess preparedness at the state and territorial level;  in addition, the HPP measure set can be aggregated at the state level.  • Seven tools assess preparedness within states and territories at the local level (e.g., county, local  health department, or hospital level).  • Results from two tools—the NHSPI and HPP measure set—are routinely aggregated at the  national level to present a snapshot of national preparedness. In addition, the TFAH tool groups  states into tiers based on scores for each measure, which can be used to assess national  preparedness (for example, by assessing the number or percentage of states in the highest or  lowest performing tier for each measure to assess relative strengths and weaknesses across the  United States).  • None of the tools were adapted for tribal communities.  Gaps and limitations: There is no comprehensive all-hazards index to measure and guide local  jurisdictions’ emergency preparedness efforts. In addition, there were no preparedness tools  tailored to tribal communities.  Factors  measured   Tools vary in the breadth of factors that they measure. For example:  • The NHSPI and COPI take a comprehensive approach to measuring preparedness across the  emergency management cycle (i.e., prevention, protection, mitigation, response, and recovery)  and include “proactive” measures of preparedness that assess social vulnerability and resilience.   • Other tools are more focused on specific aspects of preparedness (for example, the HMSPI  focuses on surge capacity and the Connectivity Measurement Tool focuses on perceptions of  partnerships).  Gaps and limitations: Existing tools inadequately capture several important factors that affect  preparedness, including strength of cross-sector collaboration, individual readiness and training of  the workforce, administrative capacity, political factors, social vulnerability, and public trust.  Types of  disasters  addressed  Eight out of nine preparedness tools take an all-hazards approach to measurement, assessing  measures of preparedness applicable to a wide range of disasters.   Gaps and limitations: There is a lack of disaster-specific tools; all-hazards tools may not reliably  predict outcomes for all types of emergencies and may be challenging for STLT users to interpret.  Data sources  and availability  Four tools leverage data from nearly 100 different public sources, including national surveys,  government agencies, and associations; the other five tools are designed for self-reporting/self- administration.  Gaps and limitations: There is a lack of publicly available data at the local level. In addition, there  are limitations in the availability of timely data, with some sources being updated infrequently.  In addition to the gaps noted above, the COVID-19 pandemic exposed other weaknesses that need to be  addressed to improve emergency preparedness and preparedness measurement. For example:  / Many preparedness tools—including the NHSPI, TFAH, and other prominent global tools—were not  valid predictors of COVID-19 outcomes, such as excess mortality rates. This underscores the need to  explore ways to improve measurement within existing tools and consider whether all-hazards tools like  the NHSPI are the best way to assess preparedness for the wide range of unique emergencies that the  country is likely to face.   / A variety of critical factors that affect outcomes are not accounted for in current preparedness  measures, such as partnerships, political will, and public trust, among others. Moving forward, it will be  important to consider ways to measure these factors and incorporate them in preparedness metrics.  / The disparate impacts of the COVID-19 pandemic on socially vulnerable communities, who suffered  higher incidence of COVID-19 infections and deaths, highlight the need to embed equity in how  Executive Summary  Mathematica® Inc.  viii  jurisdictions prepare for emergencies and thus, in how we measure communities’ preparedness and  assess their vulnerabilities.   / Finally, the COVID-19 pandemic exposed significant weaknesses in the public health data and  surveillance infrastructure, as evidenced by challenges with reporting and tracking lab test results, lack  of interoperability across health and public health reporting systems, and gaps in the types of data that  are collected and tracked. Investments in data infrastructure could help improve preparedness metrics,  especially at the local level where measurement is limited by the availability of standardized, timely data.  C. Strategies to improve measurement of public health and health care  preparedness  Given the gaps and limitations in existing tools and inherent challenges in measuring preparedness, there  is an opportunity to apply lessons learned from the COVID-19 pandemic to pursue development of  improved metrics. These efforts must be rooted in an understanding of the ideal attributes of public  health preparedness measures, so that there are set criteria against which future metrics could be  evaluated. We present ten attributes in this report, informed by current public health performance  measurement literature. Then, considering these key attributes and feedback from the TEP, we outline  four strategies and examples of associated follow-up efforts that could potentially advance preparedness  measurement, summarized in Exhibit ES.3.  Exhibit ES.3. Four strategies that could potentially advance preparedness measurement, and  potential follow-up efforts for consideration  Strategies  Potential follow-up efforts  1. Address gaps in existing  metrics by developing or  refining important  measures of preparedness  and supplementing  preparedness metrics with  contextual data.  Low-intensity efforts could include:  • Advancing individual training and measurement of training by working with  professional associations.  • Evaluating existing online preparedness curricula to set a foundation for  measurement of individual preparedness.  • Exploring degree program accreditation as a tool to improve readiness of future  public health professionals and set a foundation for a national measure of  individual preparedness.  Medium-intensity efforts could include:  • Developing new trainings to fill gaps, supporting improvement on future  measurement of individual preparedness.  • Advancing measurement on the strength of essential partnerships  • Investigating contextual factors critical to response and outcomes.  High-intensity efforts could include:  • Improving measurement of administrative response capabilities and providing  support to help STLT jurisdictions overcome barriers.  • Developing a national-level measure or measures corresponding to administrative  response capability.  Executive Summary  Mathematica® Inc.  ix  Strategies  Potential follow-up efforts  2. Improve how health equity  is addressed in  preparedness metrics by  engaging underserved  communities in continuous  efforts to advance  measurement and  considering social  vulnerability data together  with preparedness  measures.  A low-intensity effort could include:  • Developing recommendations for an effective approach to present social and  health vulnerability indicators with or within preparedness indices.  A medium- to high-intensity effort (depending on the number of communities  included) could include:  • Identifying locally appropriate metrics focused on health equity to advance  equity-focused preparedness measurement in communities, such as metrics  summarizing the preparedness level of neighborhoods disproportionately  impacted by COVID-19 and at elevated risk for specific types of emergencies (for  example, flooding in a low-lying area or floodplain).  3. Improve source data and  use additional analyses to  enhance the availability,  responsiveness, and  salience of preparedness  metrics.  Low-intensity efforts could include:  • Exploring the feasibility of using artificial intelligence with After Action Reports  (AARs), to facilitate scaled up qualitative analysis to identify themes.  • Exploring stakeholder receptiveness to implementing a metadata template for  AARs, to facilitate synthesizing patterns across AARs.  • Exploring the feasibility and benefits of using non-public data sources, such as  data from the Real-World Incident Reporting and Evaluation tool or others, to  advance the evidence base for preparedness metrics.   Medium-intensity efforts could include:  • Analyzing AARs on a large scale to identify key themes.  • Facilitating improvement of AARs’ quality and availability, through an organized  peer review process and support to ensure AARs are created and shared following  all disasters.  • Undertaking research using non-public data sources to advance the evidence  base for preparedness metrics.  A high-intensity effort could include:  • Identifying and developing automated data solutions that would reduce reporting  burden.  4. Enhance actionability and  understandability of  metrics by developing and  disseminating information  on exemplars.  A low-intensity effort could include:  • Conducting a needs assessment to identify jurisdiction types, organizations, and  disaster types most in need of exemplar models, and a landscape assessment to  identify existing strong examples and find important gaps.  A medium-intensity effort could include:  • Developing case studies to fill identified needs for exemplar models and  disseminate them to relevant audiences.  Notes:  Low-intensity=likely to require one to three staff working for less than a year; high-intensity=those that involve large-scale  data collections or system changes; medium-intensity=efforts likely to fall between the low- and high-intensity ranges. Low- intensity and italicized efforts could begin when resources are available. Italicized medium and high-intensity efforts  indicate those not dependent on low-intensity efforts. Medium- and high-intensity efforts not italicized would best be  structured using results from the low-intensity efforts listed.  Implementing these strategies would require collaboration across a range of stakeholders, including  federal agencies, STLT jurisdictions, public health and health care organizations and their partners, and  researchers. In addition, these strategies would require investments that need to be considered against  the many competing priorities that public health systems face. The low-intensity efforts listed above often  set up and help structure suggested medium- and high-intensity efforts and would be good places to  start. However, several of the suggestions for medium- or high-intensity efforts could begin without  additional preliminary work as soon as resources permit; those are italicized in Exhibit ES.3. The specific  Executive Summary  Mathematica® Inc.  x  selection of where to begin depends, as a practical matter, on how managers within the relevant agencies  find the efforts well-matched with existing work, resources, and program opportunities; but even  implementing a few of the efforts listed in Exhibit ES.3 could help agencies make incremental progress.  Ultimately, the availability of better tools to measure and understand gaps in preparedness against  specific threats could inform federal and state resource allocation and help set priorities to improve  preparedness of public health and healthcare system for the next public health threat. In the hands of  strong leadership, better measurement can also catalyze and enable improvement, resulting in a better- prepared nation. Chapter I. Introduction  Mathematica® Inc.  1  I. Introduction  A wide range of recent domestic disasters have highlighted the  challenge of preparing for large-scale public health emergencies.  Since 2020, the U.S. Department of Health and Human Services  (HHS) has issued 57 declarations of new and continuing public health  emergencies for a range of crises, including infectious diseases such  as COVID-19 and monkeypox; natural disasters such as wildfires,  hurricanes, and severe storms; and the ongoing opioid epidemic.2,II  Inadequate preparation for these disasters has resulted in  preventable loss of life; diminished public trust in federal, state, tribal,  local, and territorial (STLT) governments; and ongoing confusion  about the actions needed to improve public health and health care  preparedness (Exhibit I.1).  Three federal agencies provide critical guidance and funding to help STLT jurisdictions and public  health and health care systems nationwide advance emergency preparedness. The U.S. Centers for  Disease Control and Prevention (CDC)  and the Administration for Strategic  Preparedness and Response (ASPR)  maintain sets of core capabilities that  public health and health care systems  need to achieve preparedness. In  addition, the Federal Emergency  Management Agency (FEMA) maintains  a set of 32 capabilities intended to  guide emergency preparedness  broadly at the community level,  helping to achieve FEMA's National  Preparedness Goal, organized across  five mission areas (Exhibit I.2).4  Collectively, since 2002, federal  agencies, including CDC, ASPR, and  FEMA, have distributed more than $75  billion in funding to help STLT  jurisdictions and public health system  partners prevent, prepare for, and  respond to emergencies.5,6,7     II This count includes both new public health emergency declarations and declarations that have been renewed by the  Secretary of the Department of Health and Human Services for ongoing emergencies, such as the opioid crisis and  COVID-19.   Exhibit I.1. What is public  health and health care  preparedness?  Public health and health care  preparedness is the ability of  public health and health systems,  communities, and individuals to  prevent, protect against,  mitigate, quickly respond to, and  recover from health  emergencies.1  Exhibit I.2. FEMA’s National Preparedness Goal and the five  phases of emergency preparedness  FEMA defines the National Preparedness Goal as “a secure and  resilient Nation with the capabilities required across the whole  community to prevent, protect against, mitigate, respond to, and  recover from the threats and hazards that pose the greatest risk.”    FEMA further describes five categories, or “mission areas,” needed  to support this goal: 3  1. Prevention. Ability to avoid, prevent, or stop imminent threats  Example capability: Intelligence and information sharing  2. Protection. Ability to secure the homeland against acts of  terrorism or disasters  Example capability: Supply chain integrity and security  3. Mitigation. Ability to reduce loss of life and property by  lessening the impact of disasters  Example capability: Community resilience  4. Response. Ability to save lives, protect property and the  environment, and meet basic human needs after an incident  Example capability: Public health, health care, and emergency  medical services  5. Recovery. Ability to help communities recover quickly  Example capability: Housing   Chapter I. Introduction  Mathematica® Inc.  2  Because communities face distinct hazards, public health and health care emergency preparedness  strategies vary from jurisdiction to jurisdiction. For example, a rural community in a low-lying coastal  region would necessarily prioritize different preparedness capabilities than a landlocked city prone to  tornadoes. While guidance from the CDC, ASPR, and FEMA is designed to support emergency  preparedness across a broad range of disaster types (Exhibit 1.3), these agencies also encourage routine  hazard or risk assessments to help communities understand  distinct threats they face and prioritize capabilities based on  local needs. FEMA requires government agencies to work with  stakeholders to conduct a thorough community risk assessment  every three years using the Community Threat and Hazard  Identification Risk Assessment to guide their work.9 Similarly,  CDC's Public Health Emergency Preparedness (PHEP) program  requires funded public health agencies to work with local  jurisdictions and their community partners to conduct a risk  assessment at least once every five years.10 Risk assessments are  also common at the facility (such as hospital or nursing home)  level. For example, health care facilities that participate in  Medicare or Medicaid are required to complete or update a  hazard vulnerability analysis annually to better understand risks  and prioritize activities to mitigate, respond to, and recover from  these risks.11   Preparing for emergencies requires planning and collaboration across a multitude of public and  private sector partners that play distinct roles in public health and health care emergency response.  For example, CDC’s public health preparedness and response capability standards include STLT public  health departments, health clinics, ambulatory care providers, fire departments, law enforcement agencies,  public works, and other partners as contributors to medical surge capabilities; first responders,  epidemiologists, environmental health agencies, clinical laboratories, and other partners as contributors to  laboratory testing capabilities; and social service agencies, schools, community coalitions, mental health  providers, housing programs, and other partners as contributors to community recovery capabilities.12  Similarly, ASPR’s Health Care Preparedness and Response Capabilities are designed for multisector health  care coalitions (HCCs) consisting of public health agencies, hospitals, emergency medical services, and  emergency management organizations located in a defined geographic location.13   The landscape of organizations that make up the public health system and contribute to public  health and health care preparedness is varied and complex. Exhibit I.4 presents the broad network of  partners involved in emergency preparedness and response. Partners range from health clinics and  emergency medical services (EMS) that provide direct health care services, to employers and schools that  play key roles in ensuring safe workplaces and learning environments, such as encouraging testing and  vaccination, as many employers and organizations did during the COVID-19 pandemic.    Exhibit I.3. An all-hazards  approach to preparedness  The capabilities advanced by CDC,  ASPR, and FEMA are designed to be  adaptable across all hazard types:  natural disasters; infectious disease  outbreaks; terrorist attacks;  cybersecurity attacks; and chemical,  biological, radiological, or nuclear  incidents. This all-hazards approach to  preparedness recognizes that “while  hazards vary in source (natural,  technological, societal), they often  challenge health systems in similar  ways and demand a multisectoral  response.”8  Chapter I. Introduction  Mathematica® Inc.  3  Exhibit I.4. Entities that might make up a local public health emergency preparedness system     Image adapted from NACCHO, “Local Assessment Instrument.” National Association of City and County Health  Officials, 2013.  Assessing whether a jurisdiction is prepared for  different emergencies is inherently complex,  and there is a lack of consensus among  practitioners and scholars on how to measure  preparedness. Despite the promise and potential  of preparedness measurement (Exhibit 1.5), tools  introduced in recent decades have numerous  limitations: inconsistently defining preparedness  and its goals, relying on subjective agency  assessments of the standards and capabilities that  contribute to preparedness, and failing to provide  an evidence base for measures.15,16,17 The cross- sectoral, cross-jurisdictional nature of public health  systems adds to the complexity of preparedness  measurement; because of the many agencies and  organizations involved in emergency response, it is  challenging to understand how performance  should be measured and accountability distributed  across these partners.18 The singularity of public  health emergencies is a central challenge. Because    Exhibit I.5. Why measure preparedness?  Measuring preparedness can provide a powerful  decision-making tool to guide strategies to ensure a  community of any size is ready for an emergency.  Stoto and Nelson14 present three core aims of  preparedness measurement:   1. Accountability. Measures can help hold leaders  and public health system partners accountable  for their investments in preparedness by allowing  them to assess preparedness relative to set  standards or benchmarks.  2. Systems improvement. Measures can highlight  where weaknesses and gaps exist across the  public health system, driving quality  improvement efforts.  3. Research and knowledge sharing. Over time, as  measures are tested and refined, they can help  build evidence on “what works” when preparing  for emergencies, which is key to informing the  study of public health preparedness.   Chapter I. Introduction  Mathematica® Inc.  4  each disaster is unique, it is difficult to assess whether key takeaways from one disaster will apply to the  next.19  The COVID-19 pandemic exposed flaws in U.S. emergency response systems, demonstrating the  urgent need for more reliable, evidence-based preparedness measures. Public health and health care  systems faced extraordinary pressures, from staffing a qualified workforce to meeting surging demand for  medical care to addressing the stark health inequities that persisted across communities. Given the  significance of the pandemic and its lasting impact, a close examination of current approaches to  preparedness measurement, including key drivers of preparedness that may have been overlooked, is  essential to inform readiness for infectious disease outbreaks and other potential disasters.  In response to the issues outlined above, HHS’s Office of the Assistant Secretary for Planning and  Evaluation (ASPE) funded this study, designed to draw lessons from the COVID-19 pandemic to  inform efforts to measure preparedness going forward. The study included a targeted environmental  scan of domestic preparedness metrics and a technical expert panel (TEP)—made up of representatives  from federal agencies, public health and health care organizations, and academic institutions—with  diverse experience in preparedness measurement and emergency response.  This report gives a comprehensive overview of the current landscape of preparedness measurement tools  and suggests areas for improvement, exploring why current efforts to measure preparedness have failed  to predict effective responses in real-world settings. In addition, to inform future measurement efforts, it  reveals key criteria that preparedness metrics should meet and highlights strategies that could potentially  advance measurement to meet these criteria and address the gaps found in current metrics of public  health and health care preparedness.   The research questions in Exhibit I.6 guided this work. Questions 1–3 focus on current measures and are  addressed in Chapter II; Questions 4 and 5 look to the future of public health preparedness measurement  and are addressed in Chapter III. In addition, Appendix A describes the study methods, Appendix B  describes existing tools to measure STLT emergency preparedness, Appendix C summarizes literature  assessing how well preparedness indices predicted outcomes during the COVID-19 pandemic, Appendix D  lists the TEP participants, and Appendix E provides the agenda for the TEP.    Exhibit I.6. Study research questions  1. What public health and health care preparedness tools are currently available in the United States? (Chapter II)  2. What are the gaps in existing public health and health care preparedness metrics? (Chapter II)  3. What lessons learned from the COVID-19 pandemic can inform measurement of emergency preparedness and  response at STLT public health agencies in the future? (Chapter II)  4. What attributes should public health and health care preparedness metrics have, and what gaps would these  attributes address? (Chapter III)  5. What strategies should potentially be explored to improve measurement of public health and health care  preparedness? (Chapter III)  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  5  II. The Current State of Public Health and Health Care Preparedness  Metrics in the United States  The nation’s response to the COVID-19 pandemic provides an opportunity to better understand  how to improve public health and medical response to all types of disasters, including infectious  disease outbreaks, cybersecurity threats, and other emergencies. To seize this opportunity, we drew  on findings from the environmental scan and TEP to address the following research questions, which are  the basis of this chapter’s structure:  1. What metrics on public health and health care preparedness are currently available in the United  States?  2. What are the gaps in existing public health and  health care preparedness metrics?  3. What lessons learned from the COVID-19  pandemic can inform measurement of  emergency preparedness and response at STLT  public health agencies in the future?  Unless otherwise noted, this chapter focuses on  tools that quantify preparedness in the United  States across multiple phases of the emergency  management cycle (that is, prevention,  protection, mitigation, response, and recovery).  We define preparedness tools as indices, measure  sets, and other instruments (such as self- administered surveys) that are designed to quantify  how prepared public health and health care  systems are to respond to and recover from  emergencies and disasters across multiple  measures (Exhibit II.1). Exhibit II.2 and Appendix B  summarize the existing STLT preparedness tools  and serve as the foundation for the chapter. Given  the study’s focus on preparedness metrics in the United States, Exhibit II.2 and Appendix B exclude: (1)  global tools used to measure preparedness in other countries or to measure nation-level preparedness  (for example, the Global Health Security Index); (2) tools that focus on a single phase or aspect of the  emergency management cycle (for example, resilience or vulnerability indices); (3) tools that assess but do  not quantify preparedness (for example, FEMA’s Community Threat and Hazard Identification and Risk  Assessment, which describes a process communities can use to understand their risks and capabilities);  and (4) tools that are not publicly accessible because they protected from disclosure under the Protected  Exhibit II.1. Types of preparedness metrics  This chapter covers three types of metrics used to  assess public health and health care emergency  preparedness:  •  Measures quantify specific aspects of emergency  preparedness and response, such as whether a  state has written disaster plans for long-term  care and nursing facilities, or the percentage of  adults receiving a seasonal flu vaccine.   •  Indices create a composite statistic or score by  collecting and aggregating data from multiple  measures, helping audiences easily compare  jurisdictions along various dimensions of  emergency preparedness.   •  Measure sets are lists of measures to help users  quantify preparedness along various dimensions.  Unlike indices, measure sets do not produce a  composite score.  We use the term preparedness tools to describe  indices and measure sets that assess preparedness  across multiple measures.   Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  6  Critical Infrastructure Information Program (for example, Cybersecurity & Infrastructure Security Agency’s  Infrastructure Survey Tool). III  A. What public health and health care preparedness tools are currently available  in the United States?  In this section, we describe the tools that are currently available to assess STLT preparedness across  multiple phases of the emergency management cycle. We focus on the following characteristics:  / The number and types of tools  / The tools’ purpose and intended users  / The jurisdiction levels the tools apply to   / How existing tools conceptualize preparedness  / The types of disasters the tools address  / Sources of data used to quantify preparedness in the tools  1.  Number and types of available tools  There are relatively few tools designed to measure STLT public health and health care preparedness  in the United States. The environmental scan found just nine preparedness tools for use at the STLT level  in the United States, of which three have been used to assess national preparedness (Exhibit II.2; Appendix  B). Of the nine tools, six were indices that produced composite scores and three were measure sets. In  addition, there were three sets of capabilities maintained by federal agencies, which we describe below,  but do not include in the list of tools because they do not quantify preparedness. The literature also  described a variety of tools to measure country-level preparedness; prominent examples are in Exhibit II.3.  Exhibit II.2. List and description of tools to assess STLT public health and healthcare  preparedness  Tool  Description  Tools for internal and external stakeholders  Community Outbreak  Preparedness Index (COPI) 20  Assesses county-level preparedness for infectious disease outbreaks using  publicly available data  Hospital Medical Surge  Preparedness Index21  Evaluates the capacity of hospitals to handle patient surges during mass casualty  events, using over 120 measures from publicly available data sources  Hospital Preparedness Program  (HPP) performance measure  set22  Assesses preparedness of HPP funding recipients based on 22 performance  measures reported by recipients and disseminated broadly  National Health Security  Preparedness Index (NHSPI) 23  Generates a composite preparedness score for states, territories, and the nation  overall based on 130 measures derived from publicly available data sources  Trust for America’s Health  (TFAH) Ready or Not tool24  Evaluates states’ preparedness for public health emergencies using a targeted set  of 10 measures largely derived from the NHSPI    IIIAlthough we excluded these tools from the main analysis of themes and gaps presented in Chapter II, we reviewed  and cite literature related to these tools as it relates to overarching themes and gaps in preparedness metrics.  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  7  Tool  Description  Self-administered tools for internal use by STLT jurisdictions  Assessment for Disaster  Engagement with Partners Tool  (ADEPT) 25  Summarizes the frequency and nature of activities related to disaster  preparedness, response, and recovery that local health departments engage in  with community-based organizations, using a 15-item index for use by local  health departments  Connectivity Measurement  Tool26  Quantifies the level of connectivity of different organizations and systems  involved in public health preparedness across 28 items  Preparedness Capacity  Assessment Survey (PCAS) 27  Creates an aggregate score summarizing preparedness of local health  departments  Rapid Urban Health Security  Assessment (RUHSA) 28  Evaluates local-level health security capacities across 46 measures  The most prominent STLT preparedness tool we  found in the literature is the NHSPI. First  released in 2013 and updated annually using  publicly available data, the NHSPI assesses U.S.,  state, and territorial health preparedness for a wide  range of emergencies and disasters. Scores are  disseminated publicly to inform planning efforts by  internal and external stakeholders. In the literature,  four peer-reviewed articles focused on the NHSPI,  and nearly all articles that described U.S.  preparedness tools mentioned the NHSPI as  relevant background.36,37,38,39 The NHSPI was  developed with input and support from a variety of  funders and partners, initially including the CDC  and the Association of State and Territorial Health  Officials, and beginning in 2016, the Robert Wood  Johnson Foundation. The most recent edition of  the NHSPI uses more than 60 publicly available  data sources across 130 measures to create an  overall preparedness score for each U.S. state on a  scale of 1 to 10.40 The tool also produces a score  for each state across six domains: health security  and surveillance; community planning and  engagement; incident and information  management; health care delivery;  countermeasures management; and environmental  and occupational health.   Existing tools contain a varying number of  measures. Several of these tools are designed to  measure preparedness broadly across 50 or more  Exhibit II.3. Prominent global tools to measure  national public health and health care  preparedness  Although this report focuses on U.S. tools to  measure STLT preparedness, there are a variety of  tools used globally to measure country-level  preparedness. Prominent global preparedness tools  include:  •  Oppenheim et al.’s Epidemic Preparedness  Index. Assesses national-level preparedness for  infectious disease outbreaks.29   •  Global Health Security Index. Assesses and  benchmarks health security and related  capabilities. This tool was originally developed in  partnership among Nuclear Threat Initiative,  Johns Hopkins Center for Health Security, and  Economist Impact, with Brown University  Pandemic Center supporting development of the  most recent edition.30  •  Pan American Health Organization’s  Preparedness Index for Emergencies and  Disasters. Estimates the capacity of national  health care systems to deal with and recover  from emergencies and disasters.31  •  World Health Organization’s Joint External  Evaluation Tool. Measures capacity and  progress toward nine technical areas to assess a  nation’s capacity to prevent, detect, and rapidly  respond to public health threats.32  As detailed in Appendix C, the COVID-19 pandemic  exposed limitations in the predictive validity of many  of these global tools.33, 34, 35  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  8  measures (such as the NHSPI, COPI, the HMSPI, RUHSA). Others have a narrower focus and fewer  measures, such as the ADEPT, which contains 15 items focused specifically on local health departments’  partnerships to prepare for, respond to, and recover from disasters, or the TFAH tool, which assesses  preparedness across a focused set of 10 measures that are largely derived from the NHSPI.   In addition to the tools highlighted in Exhibit II.2, FEMA, CDC, and ASPR maintain lists of  capabilities that are also intended to guide STLT public health preparedness. Although not intended  to quantify and summarize preparedness like indices or measure sets, FEMA, CDC, and ASPR each  maintain sets of capabilities and associated resources and trainings to guide STLT jurisdictions’ efforts to  prepare for and respond to emergencies. For example, FEMA maintains a list of 32 core capabilities that  communities need to advance emergency preparedness.41 CDC and ASPR maintain similar sets of  capabilities and guidance for STLT public health agencies and health care coalitions,IV respectively.42,43  Although these three sets of capabilities are intended for different users, contain different numbers of  capabilities, and use different organizing domains to group the capabilities, all three are designed to be  flexible and adaptable to meet the needs of all STLT jurisdictions, which vary in size, geography, and  governance structures.44,45,46 However, these capabilities sets are largely intended for self-administration  and do not produce composite scores or other data sets that allow for quantitative comparison across  jurisdictions. Exhibit II.4 highlights similarities and differences across these three capability sets.   Exhibit II.4. Characteristics of sets of capabilities from FEMA, CDC, and ASPR  Characteristic  FEMA’s National  Preparedness Goal Core  Capabilities41  CDC’s Public Health  Emergency Preparedness  and Response Capabilities42  ASPR’s Health Care  Preparedness and  Response Capabilities43  Intended user or target  audience  Whole communities  STLT jurisdictions and their  public health agencies  Multisector health care  coalitions, including health  care organizations and  public health agencies  Purpose  To assist everyone who has  a role in preventing,  protecting against,  mitigating, responding to,  and recovering from the  threats and hazards that  pose the greatest risk  To serve as national standards  for STLT public health  Lists the necessary  attributes for the health  care system to save lives  and continue to function in  advance of, during, and  after a response  Initial release year  2011  2011  2012  Most recent release year  2015  2018a  2017  Number of capabilities  32  15a  4b    IV Health care coalitions are defined as multisector groups of health care and response organizations—including public health  agencies—within a geographic area.46    Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  9  Characteristic  FEMA’s National  Preparedness Goal Core  Capabilities41  CDC’s Public Health  Emergency Preparedness  and Response Capabilities42  ASPR’s Health Care  Preparedness and  Response Capabilities43  Domains  • Prevention  • Protection  • Mitigation  • Response  • Recovery  • Community resilience   • Incident management   • Information management   • Countermeasures and  mitigation   • Surge management   • Biosurveillance  • Foundation for health  care and medical  readiness   • Health care and medical  response coordination   • Continuity of health care  service delivery   • Medical surge  a CDC launched the Next Generation of the Public Health Emergency Preparedness Program (PHEP) initiative in 2020, which may  impact capability standards.   b ASPR expects to release an updated set of capabilities in 2024 that will add four new capabilities to the set (for a total of eight).  The COVID-19 pandemic exposed that preparedness tools—such as the NHSPI, TFAH tool, and  other prominent global tools—were not accurate predictors of COVID-19 outcomes.47,48,49,50 For  example, the NHSPI did not successfully predict excess mortality rates at the outset of the COVID-19  pandemic even though the tool was assessed for construct validity during its development and continues  to undergo validity and sensitivity testing on an ongoing basis as new sources of public health emergency  data emerge. 51,52,53 Appendix C summarizes literature on preparedness tools’ accuracy in predicting  COVID-19 outcomes. Even before the COVID-19 pandemic, the evidence-base for preparedness tools was  limited because the relative rarity of public health emergencies limited the use of real-world data to  validate tools.54   2.  Purpose and intended users  Five tools summarize preparedness for broad audiences, including internal and external  stakeholders. The NHSPI, HMSPI, COPI, and TFAH tools all use publicly available data to generate results  that are disseminated broadly and can be easily interpreted by a wide range of internal and external users.  These users may include federal, state, and local officials; public health and health practitioners and  administrators; multisector coalitions; researchers; communications specialists; and the general  public.55,56,57. In addition, data from the HPP measure set are available for use by internal and external  users. Although the HPP measure set is designed to be completed by HPP funding recipients and to  inform federal program monitoring, data from the HPP measure set are publicly available in easy-to-use  visualizations that show and compare how states and health care coalitions performed.58   Four tools are self-administered; they have a narrower focus and more targeted audience. The  ADEPT, Connectivity Measurement Tool, PCAS, and RUHSA are self-guided tools that local jurisdictional  leaders can use to assess preparedness and identify areas for improvement.59,60,61 For example, the ADEPT  tool collects data from local health department staff and their partners to measure the strength of local  health departments’ partnerships with community-based organizations to prepare for, respond to, and  recover from emergencies.62 Unlike the tools described above, results from the self-administered tools are  intended for internal stakeholders only, and are not routinely shared with a broad audience.   Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  10  3.  Jurisdiction levels  Two tools measure state and territorial preparedness, although additional tools produce results  that can be aggregated at the state level. The NHSPI is a comprehensive tool to measure and  summarize preparedness at the state and territorial levels. 63, Similarly, the TFAH tool, which consist of nine  measures from the NHSPI and an additional measure of state public health spending trends, provides  another way for states to compare themselves to others and assess areas for improvement.64 Further,  some of the other tools that collect data within states, such as the HPP performance measure set, can be  rolled up to the state level to understand preparedness across the state.   Seven tools assess preparedness at a local level—such as at the county, local health department, or  hospital level—but they have noteworthy limitations that could be addressed through additional  research and new or innovative sources of local data. For example, the COPI creates a composite  outbreak preparedness score at the county level and assesses a wide range of measures across the  emergency management cycle, but is a relatively new tool and consequently, has not been widely used or  validated across settings.65 Similarly, the HMSPI assesses preparedness at the hospital level, but the index  has not been widely validated against hospital performance during actual disasters.66. The HPP measure  set includes measures of preparedness for health care coalitions, but the size and composition of health  care coalitions varies across localities, making the data difficult to compare. A few tools, such as the  ADEPT, Connectivity Measurement Tool, PCAS, and RUHSA, measure the emergency preparedness of local  health departments, but are self-assessment tools intended to be completed by staff at the public health  departments and are not publicly reported (which would allow for comparison across local health  departments). New or untapped sources of local data could support development of new tools that could  facilitate comparison of local jurisdictions’ preparedness and inform federal and STLT planning efforts.   Three of the STLT tools can assess preparedness at the national level. Results from two tools—the  NHSPI and HPP measure set—are routinely aggregated at the national level to present a snapshot of  national preparedness (an example from NHSPI is shown in Exhibit II.5). In addition, the TFAH tool groups  states into tiers based on scores for each measure, which can be used to assess national preparedness, for  example by assessing the number or percentage of states in the highest performing tier for each measure  to assess relative strengths and weaknesses across the United States.  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  11  Exhibit II.5. Example of national preparedness data available on the NHSPI website    Note: The dashboard is available on the NHSPI website (https://nhspi.org/#by-state) and includes preparedness scores for the nation  and by state, including the overall preparedness level and preparedness scores by domain.  4.  How existing tools conceptualize preparedness  A few tools take a comprehensive approach to defining and measuring preparedness and include  measures that address all five phases of emergency management. Comprehensive indices like the  NHSPI and COPI include measures aligned with all five phases of the emergency management cycle  (prevention, protection, mitigation, response, and recovery).67,68 Tools with fewer measures and a  narrower focus, such as the ADEPT or the HPP measure set, tend to focus on measuring jurisdictions’  efforts to develop and implement emergency response plans related to prevention, mitigation, and  response. Exhibit II.6 highlights examples of measures from select tools across the five phases of  emergency management.   STLT preparedness tools include a mix of proactive and reactive measures. Proactive measures for  disaster preparedness identify potential risks and establish best practices to mitigate their impact— aligning with the prevention, protection, and mitigation phases of emergency management—whereas  reactive measures focus on post-event response and recovery.69 The tools we found generally contain a  mix of both types of measures. Examples of common proactive measures of preparedness include  accreditation of public health and health care facilities, measures quantifying the size of vulnerable  populations, such as children, adults ages 65 and older, or people eligible for Medicaid, and measures of  social capital, such as housing affordability or voter turnout. The tools also contained numerous reactive  measures focused on the ability to respond to threats, such as the number of burn care beds or  emergency response teams, access to volunteers (measured as the number of registered Medical Reserve   Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  12  Corp volunteers or the number of partnerships with volunteer entities), and availability of personal  protective equipment, among others.  Exhibit II.6. Examples of measures that assess various aspects of the five phases of emergency  management from select preparedness tools, including the NHSPI, COPI, HPP measure set, and  ADEPT   Emergency management phasea  Examples of measures from select tools  Prevention. Ability to avoid, prevent,  or stop imminent threats  • Number of epidemiologists per 100,000 population in the state, by  quintile (NHSPI)  • Population coverage for wastewater surveillance testing (COPI)  • Percentage of health care coalitions engaged in their recipient’s (state or  large local health department’s) jurisdiction risk assessment (HPP measure  set)  • State health department participates in a broad prevention collaborative  addressing health care–associated infections (NHSPI)  Protection. Ability to secure the  homeland against acts of terrorism  and disasters  • Percentage of bridges that are in good or fair condition (transportation  structural integrity) (NHSPI)  • Number of infrastructure companies (e.g., utility and communications  companies) and local public safety agencies (e.g., law enforcement)  participating in the health care coalition (HPP measure set)  Mitigation. Ability to reduce loss of  life and property by lessening the  impact of disasters  • Number of obstetricians and gynecologists per 100,000 female population  in the state (NHSPI)  • Pediatric vaccination rate (defined as proportion of county’s children with  all required immunizations for school enrollment) (COPI)  • Whether programs have conducted community outreach side-by-side  with community-based organization staff to reach vulnerable and hard-to- reach populations (ADEPT)  • Percentage of HCCs that access the de-identified emPOWER data map at  least once every six months to identify the number of individuals with  electricity-dependent medical and assistive equipment for planning  purposes (HPP Measure Set)  Response. Ability to save lives,  protect property and the  environment, and meet basic humans  needs after an incident  • State public health laboratory has a plan for a six-to-eight-week surge in  testing capacity to respond to an outbreak or other public health event,  with enough staffing capacity to work five 12-hour days for six to eight  weeks in response to an infectious disease outbreak (NHSPI)  • Number of community emergency response team (CERT) programs in a  county per capita (COPI)  • Program has coordinated the use of a community-based organization  facility during a disaster (ADEPT)  • Percentage of HCCs that have a complete and approved response plan  annex addressing the specialty surge requirement (HPP measure set)  Recovery. Ability to help  communities recover effectively  • Percentage of employed population in the state engaging in some work  from home by telecommuting (NHPSI)  • Quality of unemployment (UE) benefits (defined as ratio of state maximum  weekly UE benefits divided by county’s average supplemental poverty  measure threshold) (COPI)  aEmergency management phases and definitions are from the FEMA National Preparedness Goal.44   ADEPT = Assessment for Disaster Engagement with Partners Tool; COPI = Community Outbreak Preparedness Index; HCC = health  care coalition; NHSPI = National Health Security Preparedness Index; HPP = Hospital Preparedness Program.   Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  13  Numerous tools measure specific aspects  of preparedness, such as vulnerability or  resiliency. Social vulnerability and community  resilience play an important role in  preparedness and are pertinent to health  outcomes after emergencies and disasters.78  To strengthen individual and community  resilience in the U.S., HHS recently developed  the Federal Plan for Equitable Long-Term  Recovery and Resilience, which lays out an  approach for federal agencies to cooperatively  strengthen the vital conditions for health and  well-being.79 Although it is widely accepted  that social vulnerability and resilience affect preparedness, there are competing views on the extent that  these measures and other contextual factors should be included in preparedness tools. Some researchers  believe these factors affect preparedness and so should be incorporated in indices,80 but others suggest  that preparedness indices should only measure factors within the immediate control of jurisdictions.81  These tensions contribute to overarching challenges defining and conceptualizing preparedness.  Although we do not focus on these tools in this chapter, Exhibit II.7 highlights several examples of tools  that measure resilience and vulnerability.  5.  Types of disasters addressed  Many tools take an “all-hazards” approach that measures preparedness for a range of disasters  rather than for a specific type of disaster or emergency.82 Eight of the nine tools take an all-hazards  approach to measuring preparedness across emergency situations, including natural disasters;  communicable disease outbreaks; cyberattacks; acts of terrorism; and risks related to chemical, biological,  radiological, nuclear, and explosive incidents. Consequently, most measures within these tools are relevant  to a range of disasters—for example, NHSPI’s “percentage of local health departments in the state with an  emergency preparedness coordinator” or TFAH’s “change in state public health spending” measure— rather than targeting skills and resources needed by emergency type, such as whether a state has an  evacuation route in place if a hurricane occurs.   Exhibit II.7. Examples of tools to measure health  and social vulnerability or resilience78  •  Baseline Resilience Indicators for Communities70   •  Community Disaster Resilience Index71  •  Community Resilience Estimates72   •  Community Resilience Index73   •  COVID-19 Community Vulnerability Index74  •  COVID-19 Vulnerability Index75  •  COVID-19 Pandemic Vulnerability Index76   •  Social Vulnerability Index (SVI)77  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  14  The COPI is the only one of the nine tools we reviewed that focuses on preparedness for infectious  disease outbreaks.V Before the COVID-19 pandemic, there were a few global tools—like the Epidemic  Preparedness Index, Global Health Security Index, and Infectious Disease Vulnerability Index87— that  assessed countries’ preparedness to respond to infectious disease emergencies. A March 2023 study cited  the need for additional measures to quantify preparedness and response capabilities for pandemics and  infectious disease outbreaks specifically in the United States.88 In response to the lack of local-level tools  and COVID-19, a team at a California-based nonprofit developed the COPI to assess county-level  preparedness across the five phases of emergency management for an outbreak of an infectious disease.  The index measures strengths and gaps in areas such hospital surge capacity, nursing home staffing,  insurance coverage and access to primary care,  using over 30 data sources. It includes new data  sources developed in response to COVID-19, such  as the Centers for Medicare and Medicaid Services’  Nursing Home COVID-19 Vaccination Data.   There are some efforts to measure components  of STLT preparedness for natural disasters.  Exhibit II.8 highlights examples of disaster-specific  tools related to wildfire smoke exposure,  hurricanes, tsunamis, and extreme heat. Although  many of the factors assessed in these tools overlap  with the all-hazards tools described above—such  as measures of community socioeconomic status  and unemployment— they also include factors that  are specific to types of disasters. For example, the  TsunamiReady guidelines include a measure of  whether the community has produced tsunami  evacuation maps, and the ReadyMapper data  visualization tool, which has been used during  wildfires and hurricanes, includes variables of  population-level movement to show where people  are evacuating from and where they are going.89  6.  Data sources  Four tools leverage data from public sources,  including national surveys, government  agencies, and associations. The NHSPI, COPI,  HMSPI, and TFAH tool all rely on publicly available  data to inform measurement (Exhibit II.9). The NHSPI uses publicly available data from 64 sources to  calculate states’ preparedness scores,90 and the COPI uses data from more than 30 sources.91 A limitation    V As noted in Exhibit II.6, several tools specifically assess state and local vulnerability and resilience to COVID-19—and  not other infectious diseases—but do not focus on preparedness.  Exhibit II.8. Examples of tools that measure  components of STLT preparedness for specific  disasters   Examples of tools that measure components of STLT  preparedness for specific disasters include:  •  Community Health Vulnerability Index.  Measures county-level vulnerability to wildfire  smoke exposure. Health officials can use the tool  in combination with air quality models to focus  public health strategies on areas where air  quality is impaired.83   •  ReadyMapper. Tracks and measures response— including population mobility, infrastructure  damage, and health system response capacity—  during natural disasters. ReadyMapper was used  during the wildfires in California and in the  Hurricane Ida response in Louisiana.84  •  National Weather Service’s TsunamiReady  program and associated guidelines. Establishes  16 guidelines for communities to work towards  to mitigate, prepare for, and respond to  tsunamis.85  •  Heat Vulnerability Index. Assesses factors  associated with adverse health effects during  extreme heat to identify communities at the  greatest risk and inform mitigation efforts, such  as setting up cooling centers in vulnerable areas  where many people do not have access to air  conditioning.86  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  15  to using publicly available data is that there are often lags in availability. For example, the latest NHSPI  report, released in 2021, relies on data from 2020 and earlier, and the COPI report released in 2023 draws  on some data sources dating back to 2014.  Five preparedness tools rely on self-reported data. The ADEPT, Connectivity Measurement Tool, PCAS,  and RUHSA are self-assessment tools, meaning the data are collected and used by the jurisdiction  only.92,93,94,95,96,97 Although this approach expands the types of measures that can be assessed because  tool developers are not limited by data availability, self-assessment tools do not allow for comparison  across jurisdictions. Similarly, self-reported data are prone to response bias.98 The HPP measure set is also  self-reported by HPP funding recipients but results are disseminated publicly for external stakeholders.  Exhibit II.9. Examples of data sources used to measure preparedness in existing tools, by level  (state and/or local) that the data are available    State  Local  Survey data  American Hospital Association Annual Survey      Association of Public Health Laboratories All-Hazards Laboratory Preparedness Survey      Association of Public Health Laboratories Comprehensive Laboratory Services      Association of State and Territorial Health Officials Profile Survey      Centers for Disease Control and Prevention’s Behavioral Risk Factor Surveillance System      Centers for Disease Control and Prevention’s Youth Risk Behavior Survey      Robert Wood Johnson Foundation’s National Longitudinal Survey of Public Health  Systemsa       National Association of City and County Health Officials Profile of Local Health  Departments      U.S. Census Bureau’s American Community Survey      U.S. Census Bureau’s Current Population Survey      Publicly available data from government agencies  Agency for Healthcare Research and Quality Pediatric Quality Indicators      Agency for Toxic Substance and Disease Registry Environmental Justice Index      Administration for Strategic Preparedness and Response Hospital Preparedness Program  measure data      Administration for Strategic Preparedness and Response Medical Reserve Corp data       Bureau of Labor Statistics Occupational Employment Statistics      Centers for Disease Control and Prevention’s National Snapshot of Public Health  Preparedness      Centers for Disease Control and Prevention’s National Health care Safety Network  Prevention Status Reports      Centers for Disease Control and Prevention’s Funding Recipient lists      CDC’s National Vital Statistics System data       Centers for Medicare & Medicaid Services Hospital Compare      Centers for Medicare & Medicaid Skilled Nursing Facility Quality Reporting Program data      Federal Emergency Management Association Community Rating System      Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  16    State  Local  Health Resources and Services Administration data on health care shortage areas      National Plan & Provider Enumeration System National Provider Identifier registry       Data from associations and other organizations  Association of Public Health Laboratories member list      Leapfrog group hospital safety score      NACCHO Project Public Health Ready participation      National Emergency Management Association data      Penn State University Social Capital Index composite score of civic engagement      Public Health Accreditation Board member list      United States Election Project General Election Turnout Rates      Note:  Mathematica compiled the data sources in this table by reviewing the source lists for the NHSPI and HMSPI. The list focuses  on publicly available data sources and is not meant to be exhaustive. We define local level data as any data available within  states (such as data from a county, hospital system, health care coalition, or hospital). Survey data available at the local level  may only be available for a sample of local jurisdictions.   a The survey was originally funded by the Centers for Disease Control and Prevention before the Robert Wood Johnson Foundation  became the primary funder.  B. What are the gaps in existing public health and health care preparedness  metrics?   In this section, we describe gaps in preparedness  metrics, including gaps in:  / Factors that are measured in existing indices and  measure sets  / Jurisdiction levels for which the preparedness  tools are designed   / Types of disasters addressed  / Available data  / Other areas (including limitations)  1.  Gaps in factors that are measured  Existing preparedness tools do not fully capture  several important predictors of preparedness  and response capacity. Next, we describe the  predictors of preparedness that the literature and  TEP cited as missing or insufficiently captured in  preparedness measurement; they are also  summarized in Exhibit II.10.  Current preparedness tools do not adequately  assess partnerships and cross-sector collaboration. It is well documented that cross-sector  collaboration between public health, health systems, community-based organizations, laboratories, and  Exhibit II.10. Spotlight on the technical expert  panel: Factors that are inadequately captured in  existing metrics  Technical expert panel (TEP) members highlighted  several important factors that affect preparedness in  the U.S., but are inadequately captured in existing  preparedness indices and tools:  •  Partnerships and cross-sector collaboration  (mentioned 11 times by TEP members)  •  Individual training and preparedness (mentioned  11 times by TEP members)  •  Administrative capacity to hire and scale up  operations (mentioned seven times by TEP  members)  •  Political factors (mentioned six times by TEP  members)  •  Social vulnerability (mentioned six times by TEP  members)  •  Public trust (mentioned five times by TEP  members)  Source: Mathematica’s analysis of TEP data.  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  17  other organizations is critical for promoting resource sharing and engaging communities and volunteers  in preparedness, response, and recovery.99,100,101,102 However, both the literature and the TEP emphasized  the need for better measures of cross-sector collaboration; in fact, this was one of the most prominent  TEP themes. 103 A TEP member noted that it can be resource intensive to collect the survey data needed to  assess relational coordination.   We can see these weaknesses when we examine  specific tools. For example, the NHSPI has a sub- domain dedicated to “cross-sector/community  collaboration” that includes six measures, but these  measures (which rely on publicly available data) do  not capture the extent to which a state and its  localities have plans and systems in place to  collaborate during emergencies.VI Likewise, a  known weakness of the HMSPI is that it includes publicly available measures of individual hospitals’  preparedness for mass casualty events, but it does not measure the synergies between hospitals that  would improve collective response.104 On the other hand, the HPP performance measure set is specifically  designed to collect and summarize information about cross-sector health care coalitions and the extent to  which their member organizations partner with each other.105 However, HPP funding recipients cite  challenges with burdensome reporting requirements.106   Existing metrics fail to consider whether the individuals working in public health and health  organizations are adequately trained to perform their duties. TEP members highlighted that  numerous tools measure the number and types of staff in an organization, but do not assess how they  have been trained and whether that training gives them the ability to effectively respond to an  emergency. TEP members pointed out that ill-prepared leadership across sectors and a lack of real-world  experience among epidemiologists and other experts are both key limitations of existing metrics. The  CDC’s guidance document summarizing public health capabilities includes detailed suggestions on  individual skills and training needs to meet the required capabilities,107 which implies that STLT public  health agencies may have this information to guide their planning, but it is not currently assessed in  indices and measure sets.  TEP members highlighted the need for additional  measures to assess STLT health departments’  administrative capacity to hire staff and scale up  operations during emergencies. TEP members noted  that a variety of metrics are designed to measure  epidemiological or public health laboratory capacity  (for example, number of epidemiologists per 100,000),  but fewer measures examine functions like human  resources and procurement (for example, staff who    VI Examples of measures in this NHSPI domain include “state health department accredited by the Public Health  Accreditation Board” and “percent of hospitals in the state that participate in health care preparedness coalitions  through the Hospital Preparedness Program.” See NHSPI 2020 Measure Set.    “Being able to measure relational coordination  and connectivity is something we did learn  from the COVID-19 pandemic. That is a  measure that is really important [for  preparedness].”   — TEP member   ‘What we’re finding now is that [key  questions are,] “Can you move people,  money, stuff, and data around quickly? Do  you have the systems and authorities in  place to allow you to do that? Can you hire  quickly? Can you buy things quickly?”’   — TEP member  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  18  support recruitment, training, payroll and benefit administration, and purchasing). The COVID-19  pandemic exposed how critical such measures are, as many STLT public health departments struggled to  quickly hire staff and source computers and other basic supplies for contact tracing operations and  COVID-19 testing sites.108  Preparedness indices do not capture the importance of political factors in public health  preparedness and response outcomes. Existing tools do  not measure the presence of effective political  leadership109,110 or how political leanings might affect public  health operations and outcomes.111 Several TEP members  expanded on this, noting that measuring the presence of a  public health emergency response plan, as many existing  indices do, is not sufficient when political leaders have the  power to prevent these plans from being executed.   Existing metrics do not adequately measure STLT  jurisdictions’ preparedness to mitigate, respond to, and recover from emergencies affecting  socially and medically vulnerable populations. Most of the comprehensive preparedness indices that  we focused on for this report attempt to assess risks for some vulnerable populations, but also exclude  important subgroups. For example, the NHSPI has a sub-domain on “at-risk populations.” The four  measures in the domain focus on children and people who are pregnant, but do not measure other  important groups that may be at elevated risk of experiencing adverse effects from disasters, such as  people living with disabilities or people who are uninsured. On the other hand, indices focused exclusively  on vulnerability (such as those in Exhibit II.7) may help to identify communities that may need support  before, during, or after disasters based on socioeconomic factors, household characteristics, racial and  ethnic composition, and housing types and transportation, but they miss other critical factors related to  preparedness, such as emergency planning and surge capacity. Several TEP members reinforced this  finding, emphasizing the importance of identifying communities that are most vulnerable to disasters and  the extent that these communities are receiving the support they need to prevent, mitigate, respond to,  and recover from emergencies. The literature highlighted opportunities to combine social vulnerability  data from tools like CDC’s Social Vulnerability Index with results from preparedness tools to improve the  predictive capability of existing preparedness tools for underserved communities.112,113  TEP members highlighted a need for measures of public trust in government. Public trust in the  government had important implications for population-level health behaviors and outcomes during the  COVID-19 pandemic.114 TEP members noted that this factor is not adequately addressed in existing  preparedness measures and should be explored. A starting point could be examining existing metrics that  quantify public trust in government (Exhibit II.11).   2.  Gaps by jurisdiction type  Currently, there is no comprehensive all-hazards index to measure and guide local jurisdictions’  emergency preparedness efforts. Although there are a handful of tools to measure preparedness across  localities within states (for example, at the county or local health department level), none of these tools  comprehensively quantify local public health preparedness across disaster types. Existing tools at the local    “You can have capacity, you can have the  capability, and people could be robustly  prepared ... but if the political will isn’t  there, [it’s not] going to happen.”   — TEP member Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  19  level either have a somewhat narrow focus, such as the ADEPT tool’s focus on partnerships for  preparedness and response, or HMSPI’s focus on hospital surge plans and capacity, or are intended for  self-administration rather than broader dissemination, such as the PCAS and RUHSA. Further, although  the recently developed COPI takes a comprehensive approach to assessing preparedness for infectious  disease outbreak at the county level, its focus on infectious disease and lack of widespread validation may  limit its usefulness on a broad scale.   This is a critical gap that makes it challenging for  federal and state officials to understand which  communities and local public health departments  are less prepared and need additional resources  and support to address emergencies. The TEP  mentioned this gap, noting that the lack of a local  all-hazards index makes it challenging for similarly  sized communities to compare themselves to each  other and prioritize areas for improvement.  However, there are trade-offs to consider. One TEP  member cautioned that all-hazards indices at the  local level may be challenging for localities to use  and interpret, given the notable variation in size,  geographic characteristics, and governance  structure across local jurisdictions. This TEP  member suggested that hazard-specific tools may  be easier for local jurisdictions to use. Another TEP  member cautioned that STLT jurisdictions may fear  political consequences and the stigma of “being at  the bottom of the list” if they have a low  preparedness score. Because local organizations  such as public health agencies, hospitals, and other  health care and social service providers are on the  front lines of emergency responses, it is important  to consider options for developing additional tools  to understand community-level preparedness.119  Tribal communities need tailored metrics to  help them assess preparedness and response. FEMA’s pre-disaster recovery guide for tribal  governments highlights the need for special considerations for tribal communities when planning for  emergencies.120 For example, the guide encourages tribal communities to consider the presence of sacred  or historic land when taking inventory of assets during emergency preparedness exercises, and  emphasizes the criticality of cross-jurisdictional coordination (including intergovernmental agreements  with state, local, and other tribal governments) given the sovereignty of federally recognized tribes. Yet  the existing preparedness tools and the related literature do not discuss implications or use of  measurement tools with or by tribal organizations, suggesting that potential gaps may exist in measuring  preparedness within tribal nations and communities. More recently, FEMA released the 2022–2026  Exhibit II.11. Examples of metrics that quantify  public trust in government  Several indices quantify public trust in government  across multiple dimensions. For example:  •  The Citizen Trust in Government  Organizations scale includes nine items  measuring citizens’ perception of government  agencies’ competence, benevolence, and  integrity.115   •  The Trust in Government Measure quantifies  public trust as it relates to population health  interventions and public health messaging. The  tool includes 17 items related to perceptions of  the government’s capability, effectiveness,  judgement, beneficence, and integrity.116  In addition, a few U.S. organizations collect and track  standalone measures of citizens’ self-reported trust  in government. For example:  •  Pew Research Center estimates the percentage  of people in the U.S. who trust the government  to do what is right “just about always”, “most of  the time”, “only some of the time” or “never”  based on polling data.117,  •  A survey conducted by the Harvard T. H. Chan  School of Public Health asks respondents to  rate their level of trust in federal, state, and local  public health agencies using a four-point Likert  scale. 118    Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  20  National Tribal Strategy, which expands on the FEMA national preparedness goal by addressing its  responsibilities to tribal nations.121 One aim of the National Tribal Strategy is to develop tribal-specific  technical assistance resources and case studies to help Tribal Nations reach goals related to preparedness,  protection, mitigation, response, and recovery. Although metrics could help set and track progress toward  these goals, it will be critical that any tools to assess the preparedness of tribal groups are developed in  close consultation with tribes and do not have any unexpected consequences.   3.  Gaps by disaster type  Most preparedness tools discussed here were designed to assess “all hazards” preparedness, which  may not reliably predict outcomes for all types of emergencies. The literature and TEP members  highlighted that all-hazards indices do not reliably predict outcomes for all types of emergencies and may  be challenging for jurisdictional users to use to improve preparedness.122 This was evidenced during the  COVID-19 pandemic, when numerous studies found that all-hazards indices were poor predictors of  COVID-19 mortality rates. (See Appendix C.)123,124,125 TEP members highlighted unique challenges in  measuring preparedness for infectious disease outbreaks—borne out by the experience of the COVID-19  pandemic—such as the need to plan for a sustained response over many months or years, reduction in  routine program functions, changing priorities and policies, and the critical role of public health leaders  relative to other common types of responders, such as fire and rescue. These challenges also apply to  measuring preparedness for other emergency situations, such as concurrent disasters (e.g., a hurricane  occurring during an infectious disease outbreak) or cascading hazards (e.g., a tsunami triggering electrical  grid failure that ultimately results in a nuclear power plant incident).   There are potential gaps in preparedness  metrics for emergent threats, such as  cybersecurity threats, natural disasters, and  other hazards. Infectious disease outbreaks are  not the only type of hazard that require specialized  plans. Cybersecurity threats require extensive  planning that may not be captured in all-hazards  indices. For example, the NHSPI assesses the  existence of data systems to coordinate emergency  response, but does not measure plans if these data  systems are breached or inoperable due to  cybersecurity attacks.127 A 2023 report released by  the White House highlighted the need for a  national cybersecurity strategy, including  approaches to measure preparedness for  cybersecurity threats.128 Similarly, natural disasters  have unique preparedness needs. For example,  planning for storms and floods involves  determining evacuation routes, planning to protect  food and water from contamination, and developing communication plans to update the public during  power outages.129 With the growing threat of cyber-related service disruptions and natural disasters and  Exhibit II.12. Local health departments’  perception of preparedness by threat  The 2022 NACCHO Survey of Local Health  Department Preparedness asks respondents to  assess their own preparedness and concern for 23  different threats/hazards.126 Among the 23 hazards,  there are seven hazards on which their concern was  substantially higher than they thought their  preparedness was (defined as a difference of 20  percentage points [ppts] or more):  •  Opioid abuse and overdose (58 ppt difference)  •  Medical supply chain disruptions (38 ppt  difference)  •  Cyber-related threats (35 ppt difference)  •  Active shooter (24 ppt difference)  •  Critical infrastructure protection (24 ppt  difference)  •  Storms/ flooding (23 ppt difference)  •  Vaccine-preventable diseases (23 ppt difference)  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  21  rising concerns about the hazards listed in Exhibit II.12, it is important to ensure there are ways to  adequately measure STLT preparedness for these types of events.130,131  4.  Gaps in available data  There are gaps in data sources used to measure preparedness at the local level. Many data sources  are only available for public use at the state or national level (Exhibit II.9), and only about half of these  data sources are available at the sub-state level. The literature attributes the lack of comprehensive  indices at the local level, in part, to the lack of standardized data collection among local public health  agencies,132,133 as well as challenges related to interoperability (for example, incompatible software and  systems, diverse data sources, STLT-level regulations around data sharing, and concerns about data  security and privacy) and the lack of a public health IT infrastructure network.134   Data are not always timely. Several TEP members mentioned that static measures of preparedness can  be misleading because they only capture a single point in  time and are not regularly updated. This is especially true  of assessing measures of response capacity such as  vacant hospital beds or availability of personal protective  equipment. Many data sources like national surveys tend  to be collected sporadically and may have lags between  the times when data are collected and when they are  publicly available. There is a growing need to identify  data sources that are frequently updated and rapidly available for public use so more timely measures of  preparedness are available.  5.  Other gaps and limitations   The scan highlighted a few additional limitations in STLT jurisdictions’ ability to use existing  preparedness tools, such as challenges adapting scores from indices to their local contexts and  using the tools for goal setting. This suggests that there is a need for greater STLT engagement in  developing and refining metrics, and in informing development of technical assistance tools to  support use and interpretation of metrics. A few TEP members said it can be difficult for STLT  jurisdictions to interpret and adapt preparedness scores from existing indices to their unique context to  identify the greatest threats and risks their own community faces. Furthermore, for both state and local  jurisdictions, preparedness tools lack benchmarks to help guide goal setting and improvement. Although  indices like the NHSPI contain helpful information to summarize preparedness, the literature cites  challenges at the STLT level in using this information to set goals and guide improvement.135 Moving  forward, those responsible for developing and improving metrics should consider ways to continuously  engage STLT jurisdictions and their partners in developing, testing, and refining tools. This would increase  awareness of tools and ensure they are actionable for end users.136,137,138    “Static measures [of preparedness] give us a lot  of misleading information about what capacity  we do or do not have.”   — TEP member Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  22  C. What lessons learned from the COVID-19 pandemic can inform measurement  of emergency preparedness and response at STLT public health agencies in the  future?   Experiences from the COVID-19 pandemic offer critical takeaways that can inform the development of  more robust and effective measures in the future. Below, we highlight some of these lessons learned.  The COVID-19 pandemic exposed preparedness tools’ lack of predictive validity. As noted in section  II.1 and described in detail in Appendix C, numerous studies found that preparedness tools—including the  NHSPI, TFAH tool, and prominent all-hazards global tools—were poor predictors of COVID-19 mortality  rates.139,140,141 This underscores the need to explore ways to improve measurement within existing tools  and consider whether all-hazards tools are the best way to assess preparedness for the wide range of  unique emergencies that the country is likely to face. It also highlights the need to test and refine existing  tools on an ongoing basis as new sources of public health emergency data become available.   During the pandemic, researchers uncovered a variety of factors associated with public health  outcomes that should be considered and measured when assessing emergency preparedness. As  highlighted in Section II.B, factors such as strength of partnerships, political will, public trust, and social  vulnerability were shown to be associated with key outcomes during the pandemic, and TEP members  underscored them as noteworthy gaps in existing metrics. We can improve our understanding of  preparedness by finding ways to measure these factors. Relatedly, a few TEP members noted that there is  potential to use artificial intelligence to expedite review of after-action reports and qualitative data on  preparedness to discover additional factors to measure.   The COVID-19 pandemic underscored the need to embed equity into emergency preparedness and  response systems, and thus, into how we measure communities’ preparedness and assess their  vulnerabilities. It is well documented that the COVID-19 pandemic disproportionately impacted  historically marginalized communities.142,143 For example, one study showed that communities with higher  rates of social and health vulnerability had significantly lower health security levels as measured by the  NHSPI.144 This underscores the need for emergency preparedness systems to adopt approaches that  promote equitable crisis response processes and outcomes. Examples of equitable approaches to crisis  response include creating diverse crisis response teams and partnering with community organizations  known to local communities, who can help build trust and support communication with groups at  elevated risk of poor health outcomes due to structural and systemic barriers. The CDC recently  implemented the Public Health Response Readiness Framework, which includes health equity as one 10  program priorities. The 2024-2028 PHEP notice of funding opportunity (NOFO) embeds health equity  requirements in the NOFO’s three overarching strategies.145However, there remains a need to develop  equity-focused preparedness metrics that align with the changes to the CDC framework and incorporate  them in existing indices and tools to quantify inequities in preparedness. This need is further articulated in  a 2023 report from the American Medical Association, which calls for improved collection of demographic  and social needs data (such as race and ethnicity, language, disability status, and gender identity) to  reliably detect, measure, and evaluate inequities in crisis preparedness and response.146  Investments in data infrastructure could improve measurement of preparedness, especially at the  local level where measurement is limited by a lack of standardized data sources. The COVID-19  Chapter II. The Current State of Public Health and Health Care Preparedness Metrics in the United States  Mathematica® Inc.  23  pandemic exposed many weaknesses in the public health data and surveillance infrastructure in the  United States, including limitations in reporting and tracking lab test results, lack of interoperability across  health care and public health reporting systems, and gaps in the types of data that are collected and  tracked, among others.147,148 Improved data infrastructure, sharing, and collection was mentioned nearly  20 times by members of the expert panel. This is especially true at the local level, where lack of timely,  standardized data (along with other factors such as limited resources, local priorities, and so on) makes it  challenging to create composite measures of preparedness. CDC’s multibillion dollar Data Modernization  Initiative, which began in 2020, aims to improve data infrastructure to make it easier for STLT public health  agencies to report data (including data related to preparedness) and for state and federal officials to use  these data to inform decision making. TEP members noted that better data infrastructure—including  systems that are updated with data in real time—could help the nation transition from point-in-time  measurement of preparedness to continuous assessment, which would be a significant advance. To  support data infrastructure improvements, many TEP members highlighted that federal and STLT public  health agencies need additional funding and other resources so they can invest in new data systems and  support ongoing changes to the ones they have.   Although existing metrics have been updated and improved substantially in the last decade, the  evidence base for public health and health care preparedness metrics remains weak, and  substantial work remains at all levels—national, state, local, tribal territorial, and the private  sector—to ensure the United States is prepared to respond to federal public health and health care  emergencies.149 Although the development of tools like the NHSPI, TFAH tool, COPI, and others listed in  Appendix B has advanced STLT preparedness measurement, noteworthy gaps remain in public health  preparedness metrics and in STLT emergency preparedness more broadly. For example, a 2023  Government Accountability Office (GAO) report stated that substantial deficits in the federal government’s  preparedness for emergencies remain—noting that GAO had made 155 recommendations to HHS for  improvements in the prior ten years, and only 64 had been implemented. In the wake of the COVID-19  pandemic, the literature also highlights the need for improvements to other aspects of the U.S. public  health infrastructure—such as building the public health workforce, advancing the collection and use of  public health data, and enhancing communication from public health agencies to the communities they  serve—to enhance preparedness for future emergencies.150 Improving tools to measure public health  preparedness is key to tracking the nation’s progress towards emergency preparedness goals, but the lack  of consensus on how preparedness should be conceptualized and defined in preparedness frameworks,  capabilities lists, and tools complicates the path to success in this area. Looking forward, it will be  important to know which strategies can address the gaps in preparedness metrics so the federal  government and STLT public health agencies can find the weaknesses and better allocate resources to  improve emergency preparedness nationwide.  Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  24  III. Strategies to Improve Measurement of Public Health and Health  Care Preparedness  Gaps and challenges in current preparedness metrics can inform refinement of domestic  preparedness metrics going forward. Considering takeaways from Chapter II, this chapter focuses on  ways to improve preparedness measurement by addressing the following two research questions:  1. What key attributes should public health and health care preparedness measures and indices have,  and what gaps from Chapter II do these attributes address?  2. What strategies should potentially be explored to improve measurement of public health and health  care preparedness?   A. What key attributes should new public health and health care preparedness  measures have, and what gaps would they address?   To identify strategies for improving preparedness measurement, researchers and policymakers  should begin by (1) examining the ideal attributes of measures and indices, and (2) considering  how they could fill the gaps revealed here. Defining key attributes or necessary characteristics of  metrics provides a set of criteria to assess measures and indices against. Exhibit III.1 lists 10 key attributes  to consider for preparedness metrics, along with the gaps they address. This list is based on the attributes  put forth by Lichiello and Turnock in their Guidebook for Performance Measurement.151 The research team  then drew on existing literature on public health measurement to consider modifications to the attributes.  These 10 attributes were ultimately organized into three categories:  / Research-dependent attributes—including importance, validity, and reliability—require quantitative  analysis to understand whether the measure or index is relevant, accurate, and repeatable.   / Data-source dependent attributes—including availability, responsiveness, and completeness—concern  access to timely data sources that underlie measures and indices.   / Ready for real-world use attributes—including understandability, actionability, credibility, and  flexibility/adaptability—support a measure or index’s ability to be used in practice.   Eight of the attributes highlighted in Exhibit III.1 apply to both the measures themselves and to indices;  two of the attributes—completeness and flexibility and adaptability—apply solely to indices. Lichiello and  Turnock noted that there is a risk to complete and accurate reporting if there will be negative  consequences for staff or organizations that report low scores. Lichiello and Turnock described this as a  need for “abuse-proof” measurement,152 while the TEP noted a need to avoid political leaders fearing  retribution over low scores.  Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  25  Exhibit III.1. Key attributes of preparedness measures and the gaps they addressa  No. Attribute  Description and risk if not met  Current gaps or weaknesses  addressed  Research-dependent  1  Importance  Reflects a structure, process, or outcome with a large  impact on health; demonstrates substantial variation  reflecting meaningful underlying differences;153 is  directly related to objectives.  Risk if not met: The measure or index may not focus on  activities that preserve life or improve health or other  key outcomes following an emergency.  Metrics reflect a wide range of  competing capabilities and  frameworks set forth by federal  agencies and researchers, with  little evidence base for selection  of their components.  2  Validity   Captures the essence of what it purports to measure,  instead of correlated characteristics.   Risk if not met: The measure or index may inaccurately  indicate a jurisdiction is prepared, when in reality it is  not.  High performance on  preparedness metrics did not  predict COVID-19 outcomes for  many indices; there is a lack of  evidence supporting the validity  of measures in practice.  3  Reliability  Has a high likelihood of yielding the same results in  repeated trials, so there are low levels of random error  in measurement.  Risk if not met: The measure or index may accurately  predict preparedness in one scenario, but not others.  There is a lack of evidence  supporting the reliability of  measures and indices in practice.  Data source-dependent  4  Availability  Readily available with means on hand; accessible,  ongoing sources of data.154   Risk if not met: The measure or index will require time- intensive data collection and reporting, displacing focus  that could be spent on preparedness activities.  Data limitations have hindered  development of comprehensive  local indices. Lack of data makes  it difficult to measure some  aspects of preparedness, such as  the strength of partnerships.  Challenges with interoperability  impede timely data sharing, a  critical component of  preparedness.   5  Responsiveness  Able to detect change; properly calibrated and sensitive  enough to pick up important changes.155   Risk if not met: The measure or index is not up to date  when an incident occurs; that is, a capacity that  appeared adequate from older data is not there.  Gaps in timely data sources and  lack of dynamic measures of  preparedness limit existing  indices’ ability to register critical  changes in preparedness.  6  Completenessb  An emergency preparedness index should ideally cover  all important aspects of emergency preparedness that  affect outcomes.  Risk if not met: The index will fail to predict the quality  of response and outcomes in an emergency. For  example, if the index captures a wide range of factors  that affect preparedness but does not include measures  of surge capacity, a jurisdiction may be ill-equipped to  meet demand for health services, leading to increased  preventable mortality following a disaster.  Existing metrics fail to consider  the wide range of factors that  affect preparedness, such as the  strength of partnerships,  individual training, and  administrative capabilities.  There is a lack of equity- focused preparedness metrics  to identify potential inequities in  crisis preparedness and response.  Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  26  No. Attribute  Description and risk if not met  Current gaps or weaknesses  addressed  Ready for real-world use  7  Understandability Easily understood by all, with minimal explanation.   Risk if not met: The measure or index is unlikely to be  used in practice or could be used incorrectly.  All-hazards indices and tools  that compound preparedness  scores across a range of  measures may be difficult to  interpret, especially at the local  level, and there are few hazard- specific preparedness tools  available at the STLT level.  8  Actionability  Process or condition within the organization’s control.156  Risk if not met: The measure or index is unlikely to  improve readiness or be used in practice.  Many indices lack benchmarks  to help jurisdictions set goals,  track progress, and take action to  improve preparedness.  The lack of regularly updated  data sources makes it  challenging to measure factors  that change quickly, limiting the  actionability of preparedness  measures that rely on outdated  data.  9  Credibility   Supported by stakeholders.  Risk if not met: The measure or index is unlikely to be  used in practice.  STLT jurisdictions have described  challenges using existing tools,  which may be related to lack of  engagement of STLT  jurisdictions in the development  and refinement of metrics.  10  Flexibility and  adaptabilityb  An index should be adaptable across jurisdiction types.  For example, an index may need to have required and  optional components that could be tailored to a  jurisdiction and its specific hazards.157,158  Risk if not met: The index will not accurately capture  preparedness across communities with different risks  and/or may not be feasibly used across communities  with different public health structures.  There is a lack of metrics that  can be adapted to the unique  needs and risks of STLT  jurisdictions in a way that informs  their efforts to improve  preparedness.  aAttributes 1–5, 7, and 9 are adapted from Lichiello and Turnock’s Guidebook for Performance Measurement;146 additional references  are as noted.  bAttributes 6 and 10 apply to indices only.  The COVID-19 experience suggests that federal, state, local, and nongovernmental organizations  could establish priorities for preparedness measurement—including assessing tradeoffs between  measure attributes and the feasibility of various measurement strategies—to make incremental  progress as resources become available. One of the TEP members lamented the lack of progress in the  past decade on improving availability of preparedness measurement metrics. A variety of factors have  limited progress, including constrained resources and the challenge of prioritizing how to invest them.  More recently, fading memories of just how ill-prepared the U.S. was during the COVID-19 public health  emergency could also be a factor. To help ASPE and others capitalize on ASPE’s investment in this project,  Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  27  in the next section we outline a series of strategies, along with their likely resource intensity, for  consideration to help build forward momentum on preparedness measurement activities.   B. What strategies could potentially be explored to improve measurement of  public health and health care preparedness?  Given the challenges discussed, there is an  opportunity to pursue development of improved  measures and indices that have the attributes  described above in Exhibit III.1 and that address the  gaps in metrics described in Chapter II. This section  describes four strategies to advance preparedness  measurement (Exhibit III.2).   For each of the four proposed strategies, we  present tables that outline potential action steps  that could support achievement of these strategies  (Exhibits III.5, III.6, III.9, and III.10). For each action  step, we also estimate its resource intensity—low,  medium, or high—to shed light on strategy  feasibility. Low-intensity strategies are those likely  to require one to three staff working for less than a  year; high-intensity strategies are those that involve  large-scale data collections or system changes; and  medium-intensity strategies capture efforts likely to  fall between the low- and high-intensity ranges.  Some of the medium- and high-intensity strategies would benefit from building on the results of listed  low-intensity strategies, while others—those we have italicized—could begin as soon as resources are  available.   Strategy #1: Address gaps in existing metrics by developing or refining important measures of  preparedness and supplementing preparedness metrics with contextual data.  Developing new measures for critical aspects of preparedness would help fill identified gaps in  current measures and improve current measurement tools. As shown in Exhibit III.1, a key attribute of  a desirable measure is a proven link between the activity and outcomes. Further, indices should  incorporate all important aspects of emergency preparedness to achieve completeness. As discussed  during the TEP, several factors that contribute to preparedness are missing from current measures and  indices. Important preparedness factors that could be considered in future measure development work  include individual preparedness, cross-sector partnerships, and administrative response capabilities, each  discussed further below, with examples for potential follow up actions listed in Exhibit III.5. We also  discuss the importance of pairing supplemental data on social, political, economic, and environmental  factors with preparedness metrics, acknowledging that data sources to measure these factors are not  Exhibit III.2. Four strategies that could  potentially advance preparedness  measurement  1. Address gaps in existing metrics by developing  or refining important measures of preparedness  and supplementing preparedness metrics with  contextual data.   2. Improve how health equity is addressed in  preparedness metrics by engaging underserved  communities in continuous efforts to advance  measurement and considering social vulnerability  data together with preparedness measures.   3. Improve source data and use additional  analysis to enhance the availability,  responsiveness, and salience of preparedness  metrics.  4. Enhance actionability and understandability  of metrics by developing and disseminating  information on exemplars.  Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  28  currently available on an ongoing basis, meaning the cost of collecting these data would need to be  weighed against the importance of these measures.  /  Individual readiness of the public health and health care workforce. As highlighted in Chapter II,  TEP members noted the importance of measuring individual readiness—that is, whether public health  and health care workers have the right training to respond to public health emergencies—in addition to  organizational preparedness. In practice, this may be easier said than done. For example, during the  COVID-19 public health emergency, guidance for managing the response was sometimes emerging  locally and disparately, due to the novel situation, and there were limited and sometimes lagged or  incomplete data to inform that guidance. While neither individual nor organizational preparedness  training plans can account for all possible emergency scenarios with challenging conditions like the  COVID-19 pandemic, a better-trained workforce might mitigate some future challenges. To the extent  that policymakers need to issue local guidance as with COVID-19, they should also consider what  supplemental training or mentorship may be needed for health care workers to effectively implement  their guidance.  Beyond individual training, some degree of cross-training might also improve preparedness, since  during the COVID-19 public health emergency, many public health and health care professionals had to  assume roles for which they were not trained.162 Although there are many options for training public  health and health care workers and leaders on emergency readiness,163,164 the literature did not describe  any metrics that report the percentage of the public health or health care workforce that have recently  completed relevant, evidence-based training. Learning management systems, which are widely available  online, can easily collect and report the kinds of data that could be useful, such as the specific roles of  those trained (leader, health care worker, and so on), when they had their last training, percentage of  employees trained, and so on.165 In conjunction  with measuring individual readiness, there may  be an opportunity to improve or expand existing  preparedness programs and clarify a minimum  training expectation to ensure that all the types  of professionals necessary for an effective  response are trained.   / Cross-sector partnerships. As noted in Chapter  II, partnerships between public health, health  care, and other sectors such as community-based  organizations are critical to emergency response.  To better understand the strength of a  jurisdiction’s partnerships, a standardized survey  approach that preserves anonymity of  respondents could objectively capture this  information. Some survey tools exist to measure  partnerships (Exhibit III.3). Going forward, work  could focus on reviewing existing tools and  considering whether they could be adapted for  Exhibit III.3. Survey instruments to measure  the strength of partnerships among STLT public  health departments and their partners  •  The ADEPT index is intended for use by local  health departments to assess engagement with  cross-sector partners; however, it does not  capture other partners’ perspectives on the  relationships.159  •  A survey tool to measure post-disaster  resilience was developed and administered to  369 community-based organizations in New  York, as well as the New York State Department  of Health and Hygiene to measure partnership  activity and resilience after Hurricane Sandy, but  was not intended to assess preparedness  broadly.160   •  The Connectivity Measurement Tool includes a  survey of multiple partners in public health  emergency response about their perceived  connectivity, but there is little research on the  validity of the tool in peer-reviewed research.161   Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  29  widespread use, and on exploring the feasibility of incorporating a survey-based measure of partnership  strength into routine local preparedness measurement.   / Administrative response capability. To improve measurement of administrative capabilities like hiring  or procurement at STLT health departments, there may be an opportunity to build on existing data  collection efforts. For example, NACCHO’s 2022 Preparedness Profile survey asked a sample of local  health departments whether the following set of administrative capabilities were in place: (1) ability to  receive and use emergency funding, (2) ability to reduce time to contract for or procure necessary  goods and services, (3) ability to allocate or reallocate financial resources to pay for staff during an  emergency, and (4) ability to reduce time required to hire staff or reassign existing staff.166 Between 23  and 37 percent of local health departments reported these capabilities were either not in place, or they  were unsure if they were in place. The NACCHO tool is a valuable resource; further development of  administrative capacity measures using NACCHO’s tool or something similar could enhance  understanding of administrative capability trends across communities and over time.VII   / Contextual factors affecting response and  outcomes. Numerous social, political, economic,  and environmental factors affect emergency  response and outcomes but are outside the  control of the public health and health care  sector (Exhibit III.4). Incorporating measurement  of these factors into public health preparedness  indices would fail to consider the key measure  attribute of “actionability.” Instead, routinely  measuring and analyzing these contextual  factors, and presenting them alongside  preparedness measures within the control of the public health and health care sector, could help  policymakers and the public understand areas for investment.VIII   Exhibit III.5. Potential approaches to address strategy #1, and likely resource intensity of each  Examples of potential follow-up approaches  Likely  resource  intensity  • Advance individual training and measurement of training through professional associations.  Conduct key informant interviews with key public health and health professional associations about  interest in encouraging members to take a standardized emergency preparedness training and any  outstanding needs or barriers to doing that; identify how progress could be made and measured  (could lead to need for a Medium or High resource intensity follow-up to support the associations).   Low    VII If a survey were expanded and implemented annually, there could be an opportunity to improve actionability: the  same NACCHO survey identified barriers to administrative preparedness. Connecting health departments with  resources to improve readiness based on barriers identified during the survey process would help make the measures  actionable.   VIII A TEP member especially advocated for shining a light on government laws, policies, and regulations, providing an  example of how a hiring freeze had prevented jurisdictions from hiring during COVID-19 despite the availability of  federal funding to do so.  Exhibit III.4. Contextual factors affecting public  health preparedness and response outcomes  •  Social and health vulnerability167   •  Political will168  •  Public trust169  •  Policies/laws/regulations170,171  •  Supply chain172   •  Funding for public health173   •  Built and natural environment context174,175,176   Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  30  Examples of potential follow-up approaches  Likely  resource  intensity  • Evaluate existing online preparedness curricula to begin setting a foundation for measurement  of individual preparedness. Evaluate existing online curricula and obtain target audience input to  suggest improvements to help optimize them.  • Explore degree program accreditation as a tool to improve readiness of future public health  professionals and set a foundation for a national measure of individual preparedness. Meet  with the Council on Education for Public Health, which accredits schools’ public health degree  programs, to explore potential and any initiatives underway to increase “on-the-ground” training to  100 percent of students, and any initiatives promoting cross-training.   • Develop new trainings to fill gaps to support improvement on future measurement of  individual preparedness. Conduct a gap analysis and fill gaps in existing emergency preparedness  curricula by developing new modules or trainings.   • Advance measurement of strength of essential partnerships. Review existing tools for measuring  strength of partnerships as they relate to emergency preparedness, obtain input from the field,  suggest possible adaptations needed for widespread use, and explore the feasibility of incorporating  a survey-based measure into routine local preparedness measurement efforts.  • Investigate contextual factors critical to response and outcomes. Develop a method to quantify  or assess contextual factors affecting emergency response and outcomes based on publicly available  data sources; develop options for presenting these along with preparedness indices and gather  target audience feedback on best way to consider these alongside preparedness.  Medium  • Improve measurement of administrative response capability and provide support to help STLT  jurisdictions overcome barriers. Expand NACCHO or similar survey effort to capture administrative  response capability across all local health departments, and survey them annually until they are all  consistently reporting these capabilities. This effort should be paired with resources, including  technical assistance, to overcome barriers to improving these capabilities.  • Develop a national-level measure or measures corresponding to administrative response  capability. Track progress toward having 100 percent of new graduates in key fields enter the  professional workforce with appropriate preparedness training.  High  Notes:  Low-intensity=likely to require one to three staff working for less than a year; high-intensity=those that involve large-scale  data collections or system changes; medium-intensity=efforts likely to fall between the low- and high-intensity ranges.     Low-intensity and italicized efforts could begin when resources are available. Italicized efforts are not dependent on low- intensity efforts to be completed first. Medium- and high-intensity efforts not italicized would best be structured using  results from the low-intensity efforts listed.  Strategy #2: Improve how health equity is addressed in preparedness metrics by engaging  underserved communities in continuous efforts to advance measurement and considering social  vulnerability data together with preparedness measures.   Incorporating health equity into preparedness measurement could be considered high priority and  would strengthen the credibility and importance of existing metrics. The TEP emphasized the  importance of health equity, citing literature on the disproportionate burden of COVID-19 disease and  death on socially and medically vulnerable populations.177 In addition, addressing health equity in  preparedness measurement aligns with new priorities identified by the COVID-19 Health Equity Task  ForceIX and with CDC’s efforts to incorporate health equity into the Public Health Response Readiness    IX The Task Force was created through E.O. 13995 to make recommendations to the president for mitigating health  inequities caused or exacerbated by the pandemic, and for preventing them in the future (Office of Minority Health.    Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  31  Framework.178 PHEP requires its funding recipients to report on functional exercises involving critical  workforce groups and disproportionately impacted populations. However, there are no current measures  or indices that quantify preparedness to serve socially and medically vulnerable populations, such as those  disproportionately impacted by COVID-19.   One way to approach the challenge of incorporating health equity into preparedness measurement  is for researchers to engage frontline response staff at public health and health care organizations  in identifying local needs for metrics and ways to capture the required data. Historically, measure  development has relied on input from academic experts, as opposed to feedback from real-world,  frontline response workers embedded in the communities where emergencies occur. Future measure  development could prioritize community input and buy-in to ensure that measures are (1) feasible—that  is, they will work given local circumstances; (2) credible, because they are informed by users themselves;  and (3) important, because they are connected to desired outcomes based on end-user experiences.X  Local public health workers could be engaged along with first responders and emergency management  personnel in developing hazard-specific metrics tailored to their communities, which were identified in the  literature as missing from the ""all-hazards approach” that most national frameworks and STLT tools take.  Exhibit III.6 highlights an example of health equity in local preparedness metrics.  Explore optimal ways to communicate  preparedness metrics alongside measures of  social vulnerability to guide equitable allocation  of resources. A community’s social and health  vulnerability is highly correlated with emergency  response and recovery outcomes regardless of the  level of preparedness.179 Pairing social and health  vulnerability information together with  preparedness measures or indices could increase  awareness of communities with low preparedness  and particularly high vulnerability so policymakers  could allocate resources effectively. This approach  would require developing and testing a preferred  format for communicating social vulnerability and preparedness measures together.   Examples of potential follow-up approaches are summarized in Exhibit III.7.    “COVID-19 Health Equity Task Force – Charter.” U.S. Department of Health and Human Services, January 2021.  https://www.minorityhealth.hhs.gov/omh/browse.aspx?lvl=3&lvlid=118. OMH, 2022).  X The NHSPI included a stakeholder engagement and communications workgroup in 2017; however, the workgroup  does not appear active today.  Exhibit III.6. Example of health equity in local  hazard-specific preparedness metrics:   A jurisdiction may have a neighborhood with low  average household income and a high percentage of  people of color, and that neighborhood may also be  physically low-lying or in a floodplain and as such,  more at-risk for severe flooding. Because this  neighborhood has specific needs that the rest of the  jurisdiction does not, input from local responders,  local public health workers, and community  members could be convened to identify or create a  preparedness metric to address their needs.  Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  32  Exhibit III.7. Potential approaches to address strategy #2 and likely resource intensity of each  Examples of potential follow-up approaches  Likely  resource  intensity  • Develop recommendations for an effective approach to present social and health  vulnerability indicators with or within preparedness indices. Mockup multiple options for  displaying social and health vulnerability indicators together with preparedness indices and  obtain feedback from target audiences to iterate to an effective approach or visualization  method. Promote the visualization tool in conjunction with the release of indices.  Low  • Identify locally appropriate metrics focused on health equity to advance equity-focused  preparedness measurement in communities. Select communities based on social vulnerability  that are part of larger, less vulnerable cities or counties that are typically the unit for  measurement. Engage front-line response staff to explore what metrics, including hazard-specific  metrics, would capture emergency preparedness and how these metrics could be included for  visibility as the larger area is assessed.   Medium to  high,  depending on  number of  communities  included  Notes:  Low-intensity=likely to require one to three staff working for less than a year; high-intensity=those that involve large-scale  data collections or system changes; medium-intensity=efforts likely to fall between the low- and high-intensity ranges.    Low-intensity and italicized efforts could begin when resources are available. Italicized efforts are not dependent on low- intensity efforts being completed first. Medium- and high-intensity efforts not italicized would best be structured using  results from the low-intensity efforts listed.  Strategy #3: Improve source data and use additional analysis to enhance the availability,  responsiveness, and salience of preparedness metrics.  Underused data sources—such as after-action reports and non-public data—as well as  unconventional data sources, such as cell phone data, could help close gaps in data availability,  particularly at the local level. Improving use of underused data is an appealing approach as it avoids  burden from new data collection, but it presents other challenges that vary by data source. Below we  highlight specific considerations for each potential data source; Exhibit III.10 provides a set of potential  follow-up approaches to consider.  / After-action reports (AARs). AARs are a promising data  source for identifying factors that affect preparedness  (Exhibit III.8)181,182 However, communities are not required  to complete them after disasters, and there is no  standardized format.183,184 While a standardized format  would better facilitate cross-AAR analysis, an analysis  identified strong AARs using varied methodologies and  following different outlines,185 suggesting that imposing a  single format may sacrifice utility for the localities that  need the results. Another possibility is requiring a  standardized metadata template, including categories  summarizing frequent types of challenges and  recommended improvements, as a way to facilitate cross- AAR analysis and learning without sacrificing flexibility.  A concern raised by the TEP members is that AARs may be seen as perfunctory requirements and not as  tools to inform future preparedness. One TEP member suggested future research could analyze AARs  Exhibit III.8. What is an after-action  report?  As defined by FEMA, “An after-action  report is developed after exercises and  real-world incidents to summarize key  information and continuous improvement- related analytical findings, including  observations and recommended actions. It  is a detailed and comprehensive  document that describes what went well  and what did not go well, considers why,  and provides recommended actions.”180   Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  33  from different communities and disaster types, comparing findings to existing metrics and revising  metrics to address challenges. Small-scale qualitative analysis of AARs has been conducted before186  and could be scaled up by using artificial intelligence to find patterns in reported processes and  challenges. The availability of AARs would need to be explored; one data source could be the Homeland  Security Digital Library, which houses an archive of AARs from jurisdictions across the United States,  some of which are available publicly. The variable quality of the AARs will affect the usefulness of these  data; improvements such as peer review of draft AARs could increase the value of AARs for both  learning and improvement.187  / Non-public data sources. Non-public data  sources, such as health system data, could build  an evidence base for the importance of  measures. The environmental scan and TEP  identified several non-public data sources that  could help measure factors related to  preparedness or link preparedness metrics to  outcomes (Exhibit III.9). These data sources are  restricted, so efforts to link measures and  outcomes using this data would require data  sharing agreements, de-identification, and other  data security protocols.   / Unconventional data collection and sources.  The COVID-19 pandemic required creative  approaches to obtaining urgently needed data.  For example, surge periods during the pandemic required first responders to access hospital capacity  data such as beds and key equipment available on a near real-time basis. To address this gap, the CDC’s  National Healthcare Safety NetworkXI (NHSN) provided a repository for hospitals (or state  intermediaries) to frequently input important facility-level data. In addition, some states implemented  their own statewide systems to share these data, such as New York’s publicly accessible Hospital Bed  Capacity Dashboard.XII Beyond facility-level preparedness data, a TEP member noted that many  jurisdictions turned to nontraditional data such as cell phone data to track cases and social media data  to track masking trends. Despite the critical information this type of infrastructure can convey, collecting  these data can be burdensome and may be considered intrusive, so future efforts may need to explore  making these data available solely during emergencies, rather than making them continuously available  (as in the case of cell phone data).     XI The CDC’s NHSN is the nation’s most widely used health care-associated infection tracking system, with most  hospitals in the U.S. contributing data through a secure, web-based application, traditionally on a monthly or  quarterly basis. https://www.cdc.gov/nhsn/index.html.  XII Bed occupancy data were required to input data on hospital and ICU beds available Monday through Friday on the  State’s Electronic Response Data System; see https://coronavirus.health.ny.gov/hospital-bed-capacity.  Exhibit III.9. Examples of non-public data  sources that could be leveraged to improve  measurement of preparedness   •  Non-public HPP data, such as data from the  Real-World Incident Reporting and Evaluation  Tool, and performance measure data.188  •  Emergency management data maintained by  vendors, cited by a TEP member as a central  source of granular data from large numbers of  hospitals on operations, capacity, and incidents.  •  Data from large hospital systems; for example, a  TEP member shared that one system’s readiness  project includes 140 preparedness-related data  points for more than 100 hospitals.  Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  34  Exhibit III.10. Potential approaches to address strategy #3, and likely resource-intensity of each   Examples of potential follow-up approaches  Likely  resource  intensity  • Explore feasibility of use of AI with AARs. Test feasibility of using AI methods, including  machine learning, to efficiently identify patterns in the challenges and learnings reported in AARs  that could have implications for preparedness metrics.   • Explore stakeholder receptiveness to implementing a metadata template for AARs, and  develop one if they are receptive, along with options for housing and accessing metadata. Could  lead to a medium resource intensity project to build and encourage use of a new system.  • Explore feasibility and benefits of using non-public data sources such as those in Exhibit III.8  to advance the evidence base for preparedness metrics.  • Capture lessons learned from use of cell phone data and social media tracking during  COVID-19. Explore how similar or improved use of these sources can be ready for future  emergencies.   Low  • Analyze AARs on a large scale, to identify themes in response experience; reflect and report on  the themes as they relate to current measurement and related needs; conduct follow up  interviews to verify themes and identify any additional reflections.   • Facilitate improvement of AARs’ quality and availability, through peer review and support to  ensure AARs are created and shared following all disasters.  • Undertake research using non-public data sources such as those in Exhibit III.8 to advance the  evidence base for preparedness metrics, once feasibility and a strong plan have been established.  Medium  • Identify and develop automated data solutions that would reduce reporting burden, such as  helping hospitals establish interfaces to automate NHSN submissions to obtain real-time, local  data for key capacity measures.   High  Notes:  Low-intensity=likely to require one to three staff working for less than a year; high-intensity=those that involve large-scale  data collections or system changes; medium-intensity=efforts likely to fall between the low- and high-intensity ranges.  Low-intensity and italicized efforts could begin when resources are available. Italicized efforts are not dependent on low-intensity  efforts to be completed first. Medium- and high-intensity efforts not italicized would best be structured using results from the low- intensity efforts listed.  Strategy #4: Enhance actionability and understandability of metrics by developing and  disseminating information on exemplars.  All-hazards preparedness indices may seem overwhelming to public health leaders given the  extensive capabilities they measure, the distinct local contexts and risks to consider, and the  complexity of cross-sector partnerships that are required. Presenting leaders with real-life examples of  exemplary emergency response can highlight the feasibility of “getting it right.” For example, one TEP  member noted, and another agreed, that it is important to showcase examples from hospitals or health  systems that performed relatively well—across both health outcomes and financially—during the COVID- 19 pandemic to encourage health systems to invest in preparedness. ASPR’s Healthcare Emergency  Preparedness Information Gateway (known as ASPR TRACIE) provides examples of strong community- level responses, but more can be done to improve access to and use of exemplar cases. Exhibit III.11  presents examples of potential action items for consideration.  Chapter III. Strategies to Improve Measurement of Public Health and Health Care Preparedness  Mathematica® Inc.  35  Exhibit III.11. Potential approaches to address strategy #4, and likely resource-intensity of each  Examples of potential follow-up approaches  Likely  resource  intensity  • Conduct a needs assessment to identify jurisdiction types, organizations, and disaster types  most in need of exemplar models, and a landscape assessment to identify existing strong  examples and find important gaps. Explore the key audience need through a small set of  interviews to ensure subsequent case studies (a medium resource intensity approach) are  designed to meet the needs.  Low  • Develop case studies to fill identified needs for exemplar models and disseminate them to  relevant audiences.   Medium  Notes:  Low-intensity=likely to require one to three staff working for less than a year; high-intensity=those that involve large-scale  data collections or system changes; medium-intensity=efforts likely to fall between the low- and high-intensity ranges.  C. Discussion  The four strategies shared in this section offer potential directions for the future to address clear  measurement gaps discussed in Chapter II. Ultimately, the availability of better tools to measure and  understand gaps in preparedness against specific threats could inform federal and state resource  allocation and help set priorities to improve preparedness of public health and healthcare system for the  next public health threat. In the hands of dedicated leadership, better measurement can also catalyze and  enable improvement, leading to a better-prepared nation.   Progress will depend on the interest and resources from government and nongovernment organizations  leading the way in preparedness at all jurisdiction levels. Each involved organization—at the federal level  to include ASPR, CDC, and FEMA—has its own preexisting priorities, and preparedness measurement  improvement resources will inevitably compete with program support. The breadth of the suggested  improvements should not discourage incremental enhancements. Incremental enhancements, such as any  handful of the low- and medium-intensity efforts described above, especially if coordinated across  organizations, could translate to a markedly better understanding of the status of preparedness among  public health leaders, policymakers, and the general public, thanks to better measurement. The specific  approaches that should be undertaken first depend, as a practical matter, on how managers within the  relevant agencies find the efforts well-matched with existing work, resources, and program opportunities. References  Mathematica® Inc.  36  References    1 Nelson, C., N. Lurie, J. Wasserman, and S. Zakowski. “Conceptualizing and Defining Public Health Emergency  Preparedness.” American Journal of Public Health, vol. 97, April 1, 2007, pp. S9–S11.  https://doi.org/10.2105/AJPH.2007.114496.   2 Administration for Strategic Preparedness and Response. “ASPR TRACIE Evaluation of Hazard Vulnerability  Assessment Tools.” January 2024. https://files.asprtracie.hhs.gov/documents/aspr-tracie-evaluation-of-hva-tools- 3-10-17.pdf. Accessed May 6, 2024.  3 Federal Emergency Management Agency. “Mission Areas and Core Capabilities.” July 2020.  https://www.fema.gov/emergency-managers/national-preparedness/mission-core-capabilities. Accessed June 17,  2024.  4 Federal Emergency Management Agency. “National Preparedness Goal.” March 2023.  https://www.fema.gov/emergency-managers/national-preparedness/goal. Accessed May 6, 2024.  5 Assistant Secretary for Preparedness and Response. “Hospital Preparedness Program.” n.d.  https://aspr.hhs.gov/HealthCareReadiness/HPP/Documents/HPP%20Fact%20Sheet%20April%202021.pdf.  Accessed May 6, 2024.  6 Centers for Disease Control and Prevention. “2023 Readiness Report.” 2023.  https://www.cdc.gov/orr/media/pdfs/ORR_2023_Readiness_Report_508.pdf. Accessed June 17, 2024.  7 Federal Emergency Management Agency. “Preparedness Grants.” February 2024.  https://www.fema.gov/grants/preparedness. Accessed June 17, 2024.  8 World Health Organization. “Key Approaches to Strengthening Emergency Preparedness and Response.” n.d.  https://www.who.int/europe/emergencies/our-work-in-emergencies/key-approaches. Accessed June 17, 2024.  9 Federal Emergency Management Agency. “2019 National Threat and Hazard Identification and Risk Assessment  (THIRA): Overview and Methodology.” U.S. Department of Homeland Security, July 2019.  https://www.fema.gov/sites/default/files/2020-06/fema_national-thira-overview-methodology_2019_0.pdf.  Accessed May 6, 2024.  10 Centers for Disease Control and Prevention. “Division of State and Local Readiness Friday Update.” June 2024.  https://www.cdc.gov/readiness/media/pdfs/2024/06/2024-06-07_DSLR-Friday-Update_508C.pdf.   11 Centers for Medicare & Medicaid Services. “Core EP Rule Elements.” September 2023.  https://www.cms.gov/medicare/health-safety-standards/quality-safety-oversight-emergency-preparedness/core- ep-rule-elements. Accessed June 13, 2024.  12 Centers for Disease Control and Prevention. “Public Health Emergency Preparedness and Response Capabilities:  National Standards for State, Local, Tribal, and Territorial Public Health.” October 2018.  https://www.cdc.gov/readiness/media/pdfs/CDC_PreparednesResponseCapabilities_October2018_Final_508.pdf.   13 Administration for Strategic Preparedness and Response. “General Overview of Healthcare Coalitions.” n.d.  https://files.asprtracie.hhs.gov/documents/aspr-tracie-general-overview-hccs.pdf.   14 Stoto, M., and C. Nelson. “Measuring and Assessing Public Health Emergency Preparedness: A Methodological  Primer.” SSRN, September 2012. https://doi.org/10.2139/ssrn.2886349.  15 Asch, S.M., M. Stoto, M. Mendes, R. Burciaga Valdez, M.E. Gallagher, P. Halverson, and N. Lurie. “A Review of  Instruments Assessing Public Health Preparedness.” Public Health Reports, vol. 120, no. 5, October 2005, pp. 532– 542. https://doi.org/10.1177/003335490512000508.  16 Nelson, C., N. Lurie, J. Wasserman, and S. Zakowski. “Conceptualizing and Defining Public Health Emergency  Preparedness.” American Journal of Public Health, vol. 97, April 1, 2007, pp. S9–S11.  https://doi.org/10.2105/AJPH.2007.114496.   17 Stoto, M., and C. Nelson. “Measuring and Assessing Public Health Emergency Preparedness: A Methodological  Primer.” SSRN, August 2023. http://dx.doi.org/10.2139/ssrn.4538548.   18 Ibid.    References  Mathematica® Inc.  37    19 Piltch-Loeb, R.N., C. Nelson, J. Kraemer, E. Savoia, and M. Stoto. “A Peer Assessment Approach for Learning from  Public Health Emergencies.” Public Health Reports, vol. 129, no. 6, November 2014, pp. 28–34.  https://doi.org/10.1177/00333549141296S405.  20 Ghosh, J.K., B. Martinez, C. Beck, and C. Rogers. “Community Outbreak Preparedness Index (COPI).” Heluna Health,  October 2023. COPI_Technical_Report_October_2023.pdf.  21 Marcozzi, D.E., R. Pietrobon, J.V. Lawler, M.T. French, C. Mecher, J. Peffer, N.E. Baehr, et al. “Development of a  Hospital Medical Surge Preparedness Index Using a National Hospital Survey.” Health Service and Outcomes  Research Methodology, vol. 20, no. 1, 2020, pp. 60–83.   22 Administration for Strategic Preparedness and Response. “2019–2023 Hospital Preparedness Program: Performance  Measures Implementation Guidance.” 2022.  https://aspr.hhs.gov/HealthCareReadiness/guidance/Documents/2019-2023-HPP-Performance-Measures- Implementation-Guidance-8Nov22.pdf.   23 Lumpkin, J.R., Y.K. Miller, T. Inglesby, J.M. Links, A.T. Schwartz, C.C. Slemp, R.L. Burhans, J. Blumenstock, and A.S.  Khan. “The Importance of Establishing a National Health Security Preparedness Index.” Biosecurity and  Bioterrorism: Biodefense Strategy, Practice, and Science, vol. 11, no. 1, 2013, pp. 81–87.  24 McKillop, M., R. Faberman, and D. Alpert Lieberman. “Ready or Not 2024: Protecting the Public’s Health from  Diseases, Disasters, and Bioterrorism.” Trust for America’s Health, 2024. https://www.tfah.org/report-details/ready- or-not-2024/.  25 Glik, D.C., D.P. Eisenman, I. Donatello, A. Afifi, M. Stajura, M.L. Prelip, J. Sammartinova, et al. “Reliability and Validity  of the Assessment for Disaster Engagement with Partners Tool (ADEPT) for Local Health Departments.” Public  Health Reports, vol. 129, no. 6, suppl. 4, 2014, pp. 77–86. https://doi.org/10.1177/00333549141296S411.  26 Dorn, B.C., E. Savoia, M.A. Testa, M.A. Stoto, and L.J. Marcus. “Development of a Survey Instrument to Measure  Connectivity to Evaluate National Public Health Preparedness and Response Performance.” Public Health Reports,  vol. 122, no. 3, 2007, pp. 329-38. doi: 10.1177/003335490712200306. PMID: 17518304; PMCID: PMC1847495.  Development of a Survey Instrument to Measure Connectivity to Evaluate National Public Health Preparedness  and Response Performance - PMC (nih.gov).  27 Davis, Mary, Glen Mays, James Bellamy, Christine Bevc, and Cammie Marti. “Improving Public Health Preparedness  Capacity Measurement: Development of the Local Health Department Preparedness Capacities Assessment  Survey.” Disaster Medicine and Public Health Preparedness, vol 7, 2013, pp. 1–7. 10.1017/dmp.2013.108.  28 Boyce, M.R., and R. Katz. “Rapid Urban Health Security Assessment Tool: A New Resource for Evaluating Local-Level  Public Health Preparedness.” BMJ Global Health, vol. 5, no. 6, 2020, article e002606.  29 Oppenheim, B., M. Gallivan, N.K. Madhav, N. Brown, V. Serhiyenko, N.D. Wolfe, and P. Ayscue. “Assessing Global  Preparedness for the Next Pandemic: Development and Application of an Epidemic Preparedness Index.” BMJ  Global Health, vol. 4, no. 1, 2019, article e001157.  30 Global Health Security Index. “What is the GHS Index?” n.d.  https://ghsindex.org/about/#:~:text=The%20Global%20Health%20Security%20%28GHS%29%20Index%20is%20an ,prevent%2C%20detect%2C%20and%20respond%20to%20epidemics%20and%20pandemics.  31 Pan American Health Organization. “Preparedness Index for Health Emergencies and Disasters.” 2019.  https://www.paho.org/en/documents/preparedness-index-health-emergencies-and-disasters-0.  32 Gupta, V., J.D. Kraemer, R. Katz, A.K. Jha, V.B. Kerry, J. Sane, J. Ollgren, et al. “Analysis of Results from the Joint  External Evaluation: Examining Its Strength and Assessing for Trends Among Participating Countries.” Journal of  Global Health, vol. 8, no. 2, 2018.  33 Stribling, J., A. Clifton, G. McGill, and K. de Vries. “Examining the UK Covid-19 Mortality Paradox: Pandemic  Preparedness, Healthcare Expenditure, and the Nursing Workforce.”  Journal of Advanced Nursing, vol. 76, no. 12,  2020, pp. 3218–3227. https://doi.org/10.1111/jan.14562.   34 Kaiser, M.A., T.Y. Chen, and P. Gluckman. “Should Policy Makers Trust Composite Indices? A Commentary on the  Pitfalls of Inappropriate Indices for Policy Formation.” Health Research Policy and Systems, vol. 19, no.1, March  2021. https://doi.org10.1186/s12961-021-00702-4.     References  Mathematica® Inc.  38    35 Haider, N., A. Yavlinsky, Y.M. Chang, M.N. Hasan, C. Benfield, A.Y. Osman, M.J. Uddin, et al. “The Global Health  Security Index and Joint External Evaluation Score for Health Preparedness Are Not Correlated with Countries'  COVID-19 Detection Response Time and Mortality Outcome.” Epidemiology and Infection, vol. 148, 2020, article  e210. https://doi.org/10.1017/S0950268820002046.   36 Lumpkin, J.R., Y.K. Miller, T. Inglesby, J.M. Links, A.T. Schwartz, C.C. Slemp, R.L. Burhans, J. Blumenstock, and A.S.  Khan. “The Importance of Establishing a National Health Security Preparedness Index.” Biosecurity and  Bioterrorism: Biodefense Strategy, Practice, and Science, vol. 11, no. 1, 2013, pp. 81–87.  37 Keim, M.E., and A.P. Lovallo. “Validity of the National Health Security Preparedness Index as a Predictor of Excess  COVID-19 Mortality.” Prehospital and Disaster Medicine, vol. 36, no. 2, 2021, pp. 141–144.  https://doi.org/10.1017/S1049023X20001521.   38 Moulton, A.D. “A COVID-19 Lesson: Better Health Emergency Preparedness Standards Are Needed.” Health  Security, vol. 20, no. 6, 2022, pp. 457–466. https://doi.org/10.1089/hs.2022.0037.   39 Mays, G., and M. Childress. “2021 Release of National Health Security Preparedness Index.” University of Colorado,  Colorado School of Public Health, June 2021.   40 National Health Security Preparedness Index. “Measures List: National Health Security Preparedness Index, 2021  Release.” https://nhspi.org/wp-content/uploads/2021/06/2021-NHSPI-Measure-List.pdf.  41 Federal Emergency Management Agency. “National Preparedness Goal.” U.S. Department of Homeland Security,  September 2015. https://www.fema.gov/sites/default/files/2020-06/national_preparedness_goal_2nd_edition.pdf.  42 Centers for Disease Control and Prevention. “Public Health Emergency Preparedness and Response Capabilities:  National Standards for State, Local, Tribal, and Territorial Public Health.” October 2018.  https://www.cdc.gov/orr/readiness/00_docs/CDC_PreparednesResponseCapabilities_October2018_Final_508.pdf.   43 Assistant Secretary for Preparedness and Response. “2017–2022 Health Care Preparedness and Response  Capabilities.” November 2016. https://www.phe.gov/Preparedness/planning/hpp/reports/Documents/2017-2022- healthcare-pr-capablities.pdf.  44 Federal Emergency Management Agency. “National Preparedness Goal.” U.S. Department of Homeland Security,  September 2015. https://www.fema.gov/sites/default/files/2020-06/national_preparedness_goal_2nd_edition.pdf.  45 Centers for Disease Control and Prevention. “Public Health Emergency Preparedness and Response Capabilities:  National Standards for State, Local, Tribal, and Territorial Public Health.” October 2018.  https://www.cdc.gov/orr/readiness/00_docs/CDC_PreparednesResponseCapabilities_October2018_Final_508.pdf.  46 Assistant Secretary for Preparedness and Response. “2017–2022 Health Care Preparedness and Response  Capabilities.” November 2016. https://www.phe.gov/Preparedness/planning/hpp/reports/Documents/2017-2022- healthcare-pr-capablities.pdf.  47 Stribling, J., A. Clifton, G. McGill, and K. de Vries. “Examining the UK Covid-19 Mortality Paradox: Pandemic  Preparedness, Healthcare Expenditure, and the Nursing Workforce.”  Journal of Advanced Nursing, vol. 76, no. 12,  2020, pp. 3218–3227. https://doi.org/10.1111/jan.14562.   48 Kaiser, M.A., T.Y. Chen, and P. Gluckman. “Should Policy Makers Trust Composite Indices? A Commentary on the  Pitfalls of Inappropriate Indices for Policy Formation.” Health Research Policy and Systems, vol. 19, no.1, March  2021. https://doi.org10.1186/s12961-021-00702-4.   49 Keim, M.E., and A.P. Lovallo. “Validity of the National Health Security Preparedness Index as a Predictor of Excess  COVID-19 Mortality.” Prehospital and Disaster Medicine, vol. 36, no. 2, 2021, pp. 141–144.  https://doi.org/10.1017/S1049023X20001521.   50 Haider, N., A. Yavlinsky, Y.M. Chang, M.N. Hasan, C. Benfield, A.Y. Osman, M.J. Uddin, et al. “The Global Health  Security Index and Joint External Evaluation Score for Health Preparedness Are Not Correlated with Countries'  COVID-19 Detection Response Time and Mortality Outcome.” Epidemiology and Infection, vol. 148, 2020, article  e210. https://doi.org/10.1017/S0950268820002046.   51 Keim, M.E., and A.P. Lovallo. “Validity of the National Health Security Preparedness Index as a Predictor of Excess  COVID-19 Mortality.” Prehospital and Disaster Medicine, vol. 36, no. 2, 2021, pp. 141–144.  https://doi.org/10.1017/S1049023X20001521.    References  Mathematica® Inc.  39    52 Moulton, A.D. “A COVID-19 Lesson: Better Health Emergency Preparedness Standards Are Needed.” Health  Security, vol. 20, no. 6, 2022, pp. 457–466. https://doi.org/10.1089/hs.2022.0037.   53 Mays, G., and M. Childress. “2021 Release of National Health Security Preparedness Index.” University of Colorado,  Colorado School of Public Health, June 2021.   54 Nelson, C., N. Lurie, J. Wasserman. “Assessing public health emergency preparedness: concepts, tools, and  challenges.” Annual Rev. Public Health, vol. 28, 2007, pp. 1- 18. https://doi.org/10.1146/annurev.publhealth.  28.021406.144054.  55 National Health Security Preparedness Index. “Measures List: National Health Security Preparedness Index, 2021  Release” 2021. https://nhspi.org/wp-content/uploads/2021/06/2021-NHSPI-Measure-List.pdf.  56 McKillop, M., R. Faberman, and D. Alpert Lieberman. “Ready or Not 2024: Protecting the Public’s Health from  Diseases, Disasters, and Bioterrorism.” Trust for America’s Health, 2024. https://www.tfah.org/report-details/ready- or-not-2024/.  57 Ghosh, J.K., B. Martinez, C. Beck, and C. Rogers. “Community Outbreak Preparedness Index (COPI).” Heluna Health,  October 2023. COPI_Technical_Report_October_2023.pdf.  58 U.S. Department of Health and Human Services. “HPP Performance Metrics – FY 2021 Dashboard.” 2021.  https://dhhs.maps.arcgis.com/apps/dashboards/71b5f53173cb4f4e9248b85244158151.  59 Dorn, B.C., E. Savoia, M.A. Testa, M.A. Stoto, and L.J. Marcus. “Development of a Survey Instrument to Measure  Connectivity to Evaluate National Public Health Preparedness and Response Performance.” Public Health Reports,  vol. 122, no. 3, 2007, pp. 329-38. doi: 10.1177/003335490712200306. PMID: 17518304; PMCID: PMC1847495.  Development of a Survey Instrument to Measure Connectivity to Evaluate National Public Health Preparedness  and Response Performance - PMC (nih.gov).  60 Davis, Mary, Glen Mays, James Bellamy, Christine Bevc, and Cammie Marti. “Improving Public Health Preparedness  Capacity Measurement: Development of the Local Health Department Preparedness Capacities Assessment  Survey.” Disaster Medicine and Public Health Preparedness, vol 7, 2013, pp. 1–7. 10.1017/dmp.2013.108.  61 Boyce, M.R., and R. Katz. “Rapid Urban Health Security Assessment Tool: A New Resource for Evaluating Local-Level  Public Health Preparedness.” BMJ Global Health, vol. 5, no. 6, 2020, article e002606.  62 Glik, D.C., D.P. Eisenman, I. Donatello, A. Afifi, M. Stajura, M.L. Prelip, J. Sammartinova, et al. “Reliability and Validity  of the Assessment for Disaster Engagement with Partners Tool (ADEPT) for Local Health Departments.” Public  Health Reports, vol. 129, no. 6, suppl. 4, 2014, pp. 77–86. https://doi.org/10.1177/00333549141296S411.  63 National Health Security Preparedness Index. “Methodology for the 2021 Release of the National Health Security  Preparedness Index.” https://nhspi.org/wp-content/uploads/2021/NHSPI_2021_Methodology.pdf.  64 McKillop, M., R. Faberman, and D. Alpert Lieberman. “Ready or Not 2024: Protecting the Public’s Health from  Diseases, Disasters, and Bioterrorism.” Trust for America’s Health, 2024. https://www.tfah.org/report-details/ready- or-not-2024/.  65 Ghosh, J.K., B. Martinez, C. Beck, and C. Rogers. “Community Outbreak Preparedness Index (COPI).” Heluna Health,  October 2023. COPI_Technical_Report_October_2023.pdf.  66 Marcozzi, D.E., R. Pietrobon, J.V. Lawler, M.T. French, C. Mecher, J. Peffer, N.E. Baehr, et al. “Development of a  Hospital Medical Surge Preparedness Index Using a National Hospital Survey.” Health Service and Outcomes  Research Methodology, vol. 20, no. 1, 2020, pp. 60–83.   67 National Health Security Preparedness Index. “Measures List: National Health Security Preparedness Index, 2021  Release.” https://nhspi.org/wp-content/uploads/2021/06/2021-NHSPI-Measure-List.pdf.  68 Ghosh, J.K., B. Martinez, C. Beck, and C. Rogers. “Community Outbreak Preparedness Index (COPI).” Heluna Health,  October 2023. COPI_Technical_Report_October_2023.pdf.  69 Gray, B., J. Eaton, J. Christy, J. Duncan, F. Hanna, and S. Kasi. “A Proactive Approach: Examples for Integrating  Disaster Risk Reduction and Mental Health and Psychosocial Support Programming.” International Journal of  Disaster Risk Reduction, vol. 15, no. 54, 2021. https://doi.org/10.1016/j.ijdrr.2021.102051.    References  Mathematica® Inc.  40    70 Cutter, S., C. Burton, and C.T. Emrich. ""Disaster Resilience Indicators for Benchmarking Baseline Conditions.” Journal  of Homeland Security and Emergency Management, vol. 7: no. 1, August 2010. https://doi.org/10.2202/1547- 7355.1732.  71 Peacock, W.G., S.D. Brody, W.A. Seitz, W.J. Merrell, A. Vedlitz, S. Zahran, R.C. Harriss, et al. “Advancing Resilience of  Coastal Localities: Developing, Implementing, and Sustaining the Use of Coastal Resilience Indicators: A Final  Report.” Hazard Reduction & Recovery Center, 2010, pp. 1–148.  72 United States Census Bureau. 2020. Community Resilience Estimates: Quick Guide. Also available  at https://www2.census.gov/data/experimental-data-products/community-resilience-estimates/2020/technical- document.pdf.  73 Sherrieb, K., F.H. Norris, and S. Galea. “Measuring Capacities for Community Resilience.” Social Indicators Research,  vol. 99, no. 2, 2010, pp. 227–247.  74 Surgo Ventures. “COVID-19 Community Vulnerability Index (CCVI) Methodology.” 2020. https://covid-static- assets.s3.amazonaws.com/US-CCVI/COVID-19+Community+Vulnerability+Index+(CCVI)+Methodology.pdf.  75 Tiwari, A., A.V. Dadhania, V.A.B. Ragunathrao, and E.R.A Oliveira. “Using Machine Learning to Develop a Novel  COVID-19 Vulnerability Index (C19VI).” Science of the Total Environment, vol. 773, 2021, article 14565.   76 Marvel, S.W., J.S. House, M. Wheeler, K. Song, Y.H. Zhou, F.A. Wright, W.A. Chiu, et al. “The COVID-19 Pandemic  Vulnerability Index (PVI) Dashboard: Monitoring County-Level Vulnerability Using Visualization, Statistical  Modeling, and Machine Learning.” Environmental Health Perspectives, vol. 129, no. 1, 2021.   77 Flanagan, B.E., E.W. Gregory, E.J. Hallisey, J.L. Heitgerd, and B. Lewis. “A Social Vulnerability Index for Disaster  Management.” Journal of Homeland Security and Emergency Management, vol. 8, no. 1, 2011.   78 Rogers, C.J., B. Cutler, K. Bhamidipati, and J.K. Ghosh. “Preparing for the Next Outbreak: A Review of Indices  Measuring Outbreak Preparedness, Vulnerability, and Resilience.” Preventive Medicine Reports, vol. 35, 2023, article  102282. https://doi.org/10.1016/j.pmedr.2023.102282.   79 Office of Disease Prevention and Health Promotion, U.S. Department of Health and Human Services. “Federal Plan  for Equitable Long-Term Recovery and Resilience for Social, Behavioral, and Community Health.” January 2022.  Available at: https://health.gov/our-work/national-health-initiatives/equitable-long-term-recovery-and-resilience  80 Boyce, M. “State-level public health preparedness indices as predictors of COVID-19 mortality outcomes: results  from the United States of America in 2020.” Frontiers in Epidemiology, December 2023. DOI:  10.3389/fepid.2023.1229718.  81 Stoto, M., and C. Nelson. “Measuring and Assessing Public Health Emergency Preparedness: A Methodological  Primer.” SSRN, August 2023. http://dx.doi.org/10.2139/ssrn.4538548.   82 Ghosh, J.K., B. Martinez, C. Beck, and C. Rogers. “Community Outbreak Preparedness Index (COPI).” Heluna Health,  October 2023. COPI_Technical_Report_October_2023.pdf.  83 Environmental Protection Agency (EPA). “Community Health Vulnerability Index.”. July 2017.  https://www.epa.gov/sites/default/files/2017-07/documents/community_health_vulnerability_index.pdf.  84 Nallen, Joe. “ReadyMapper: A Digital Solution to Optimize Health System Resilience and Response During  Wildfires.” Crisis Ready, 2022. https://www.crisisready.io/optimizing-health-response-during-wild-fires-with-real- time-integrated-data/.  85National Weather Service. “TsunamiReady Guidelines.”n.d. https://www.weather.gov/tsunamiready/guidelines.  86 Hammer, J., D.G. Ruggieri, C. Thomas, and J. Caum. “Local Extreme Heat Planning: An Interactive Tool to Examine a  Heat Vulnerability Index for Philadelphia, Pennsylvania.” Journal of Urban Health, vol. 97, no. 4, 2020, pp. 519–528.  https://doi.org/10.1007/s11524-020-00443-9.   87 Moore, M., B. Gelfeld, and C.P. Adeyemi Okunogbe. “Identifying Future Disease Hot Spots: Infectious Disease  Vulnerability Index.” Rand Health Quarterly, vol. 6, no. 3, 2017.   88 Lee, J.M., R. Jansen, K.E. Sanderson, F. Guerra, S. Keller-Olaman, M. Murti, T.L. O'Sullivan, et al. “Public Health  Emergency Preparedness for Infectious Disease Emergencies: A Scoping Review of Recent Evidence.” BMC Public  Health, vol. 23, no. 1, 2023, pp. 420. https://doi.org/10.1186/s12889-023-15313-7.    References  Mathematica® Inc.  41    89 Nallen, Joe. “ReadyMapper: A Digital Solution to Optimize Health System Resilience and Response During  Wildfires.” Crisis Ready, 2022. https://www.crisisready.io/optimizing-health-response-during-wild-fires-with-real- time-integrated-data/.  90 National Health Security Preparedness Index. “Measures List: National Health Security Preparedness Index, 2021  Release.” https://nhspi.org/wp-content/uploads/2021/06/2021-NHSPI-Measure-List.pdf.  91 Ghosh, J.K., B. Martinez, C. Beck, and C. Rogers. “Community Outbreak Preparedness Index (COPI).” Heluna Health,  October 2023. COPI_Technical_Report_October_2023.pdf.  92 Glik, D.C., D.P. Eisenman, I. Donatello, A. Afifi, M. Stajura, M.L. Prelip, J. Sammartinova, et al. “Reliability and Validity  of the Assessment for Disaster Engagement with Partners Tool (ADEPT) for Local Health Departments.” Public  Health Reports, vol. 129, no. 6, suppl. 4, 2014, pp. 77–86. https://doi.org/10.1177/00333549141296S411.  93 Dorn, B.C., E. Savoia, M.A. Testa, M.A. Stoto, and L.J. Marcus. “Development of a Survey Instrument to Measure  Connectivity to Evaluate National Public Health Preparedness and Response Performance.” Public Health Reports,  vol. 122, no. 3, 2007, pp. 329-38. doi: 10.1177/003335490712200306. PMID: 17518304; PMCID: PMC1847495.  Development of a Survey Instrument to Measure Connectivity to Evaluate National Public Health Preparedness  and Response Performance - PMC (nih.gov).  94 Boyce, M.R., and R. Katz. “Rapid Urban Health Security Assessment Tool: A New Resource for Evaluating Local-Level  Public Health Preparedness.” BMJ Global Health, vol. 5, no. 6, 2020, article e002606.  95 Centers for Disease Control and Prevention. “Public Health Emergency Preparedness and Response Capabilities:  National Standards for State, Local, Tribal, and Territorial Public Health.” October 2018.  https://www.cdc.gov/orr/readiness/00_docs/CDC_PreparednesResponseCapabilities_October2018_Final_508.pdf.  96 Assistant Secretary for Preparedness and Response. “2017–2022 Health Care Preparedness and Response  Capabilities.” November 2016. https://www.phe.gov/Preparedness/planning/hpp/reports/Documents/2017-2022- healthcare-pr-capablities.pdf.  97 Federal Emergency Management Agency. “National Preparedness Goal.” U.S. Department of Homeland Security,  September 2015. https://www.fema.gov/sites/default/files/2020-06/national_preparedness_goal_2nd_edition.pdf.  98 Dorn, B.C., E. Savoia, M.A. Testa, M.A. Stoto, and L.J. Marcus. “Development of a Survey Instrument to Measure  Connectivity to Evaluate National Public Health Preparedness and Response Performance.” Public Health Reports,  vol. 122, no. 3, 2007, pp. 329-38. doi: 10.1177/003335490712200306. PMID: 17518304; PMCID: PMC1847495.  Development of a Survey Instrument to Measure Connectivity to Evaluate National Public Health Preparedness  and Response Performance - PMC (nih.gov).  99 Ghosh, J.K., B. Martinez, C. Beck, and C. Rogers. “Community Outbreak Preparedness Index (COPI).” Heluna Health,  October 2023. COPI_Technical_Report_October_2023.pdf.  100 Acosta, J.D., L. Burgette, A. Chandra, D.P. Eisenman, I. Gonzalez, D. Varda, and L. Xenakis. “How Community and  Public Health Partnerships Contribute to Disaster Recovery and Resilience.” Disaster Medicine and Public Health  Preparedness, vol. 12, no. 5, 2018, pp. 635–643. https://doi.org/10.1017/dmp.2017.130.   101 Adams, R.M., M.L. Prelip, D.C. Glik, I. Donatello, and D.P. Eisenman. “Facilitating Partnerships with Community- and  Faith-Based Organizations for Disaster Preparedness and Response: Results of a National Survey of Public Health  Departments.” Disaster Medicine and Public Health Preparedness, vol. 12, no. 1, 2018, pp. 57–66.  https://doi.org/10.1017/dmp.2017.3.   102 Brewer, L.C., G. Asiedu, C. Jones, M. Richard, J. Erickson, J. Weis, A. Abbenyi, et al. “Emergency Preparedness and  Risk Communication Among African American Churches: Leveraging a Community-Based Participatory Research  Partnership COVID-19 Initiative.” Preventing Chronic Disease, vol. 17, 2020. https://doi.org/10.5888/pcd17.200408.   103 Glik, D.C., D.P. Eisenman, I. Donatello, A. Afifi, M. Stajura, M.L. Prelip, J. Sammartinova, et al. “Reliability and Validity  of the Assessment for Disaster Engagement with Partners Tool (ADEPT) for Local Health Departments.” Public  Health Reports, vol. 129, no. 6, suppl. 4, 2014, pp. 77–86. https://doi.org/10.1177/00333549141296S411.  104 Marcozzi, D.E., R. Pietrobon, J.V. Lawler, M.T. French, C. Mecher, J. Peffer, N.E. Baehr, et al. “Development of a  Hospital Medical Surge Preparedness Index Using a National Hospital Survey.” Health Service and Outcomes  Research Methodology, vol. 20, no. 1, 2020, pp. 60–83.    References  Mathematica® Inc.  42    105 Administration for Strategic Preparedness and Response. “2019–2023 Hospital Preparedness Program:  Performance Measures Implementation Guidance.” 2022.  https://aspr.hhs.gov/HealthCareReadiness/guidance/Documents/2019-2023-HPP-Performance-Measures- Implementation-Guidance-8Nov22.pdf.   106 National Association of County and City Health Officials. “Administrative Preparedness: Emergency Reporting  Practices for Health Departments.” September 2013. https://www.naccho.org/uploads/header-images/public- health-preparedness/Admin-Prep_Reporting.pdf.  107 Centers for Disease Control and Prevention. “Public Health Emergency Preparedness and Response Capabilities:  National Standards for State, Local, Tribal, and Territorial Public Health.” October 2018.  https://www.cdc.gov/orr/readiness/00_docs/CDC_PreparednesResponseCapabilities_October2018_Final_508.pdf.  108 Gupta, N., S.A. Balcom, A. Gulliver, and R.L. Witherspoon. “Health Workforce Surge Capacity During the COVID-19  Pandemic and Other Global Respiratory Disease Outbreaks: A Systematic Review of Health System Requirements  and Responses.” The International Journal of Health Planning and Management, vol. 36, no. 1, 2021, pp. 26–41.  https://doi.org/10.1002/hpm.3137.   109 Stoto, M., and C. Nelson. “Measuring and Assessing Public Health Emergency Preparedness: A Methodological  Primer.” SSRN, August 2023. http://dx.doi.org/10.2139/ssrn.4538548.  110 Moulton, A.D. “A COVID-19 Lesson: Better Health Emergency Preparedness Standards Are Needed.” Health  Security, vol. 20, no. 6, 2022, pp. 457–466. https://doi.org/10.1089/hs.2022.0037.  111 Krieger, Nancy, Christian Testa, Jarvis T. Chen, William P. Hanage, and Alecia J. McGregor. “Relationship of Political  Ideology of U.S. Federal and State Elected Officials and Key COVID Pandemic Outcomes Following Vaccine Rollout  to Adults: April 2021–March 2022.” The Lancet Regional Health - Americas, vol. 16, 2022, article 100384.  https://doi.org/10.1016/j.lana.2022.100384.  112 Pence, J., I. Miller, T. Sakurahara, J. Whitacre, S. Reihani, E. Kee, and Z. Mohaghegh. “GIS-Based Integration of Social  Vulnerability and Level 3 Probabilistic Risk Assessment to Advance Emergency Preparedness, Planning, and  Response for Severe Nuclear Power Plant Accidents.” Risk Analysis, vol. 39, no. 6, 2019, pp. 1262–1280.  https://doi.org/10.1111/risa.13241.  113 Boyce, M. “State-level public health preparedness indices as predictors of COVID-19 mortality outcomes: results  from the United States of America in 2020.” Frontiers in Epidemiology, December 2023. DOI:  10.3389/fepid.2023.1229718.  114 Stoto, M., and C. Nelson. “Measuring and Assessing Public Health Emergency Preparedness: A Methodological  Primer.” SSRN, August 2023. http://dx.doi.org/10.2139/ssrn.4538548.   115 Grimmelikhuijsen, S & E. Knies. “Validating a scale for citizen trust in government organizations.” International  Review of Administrative Sciences, vol 83, no. 3, 2017, pp. 583 – 601. https://doi.org/10.1177/0020852315585950  116 Burns, K, P. Brown, M. Calnan, P.R. Ward, J. Little et al. “Development and validation of the Trust in Government  measure (TGM)”. BMC Public Health, 2023. https://doi.org/10.1186/s12889-023-16974-0  117 Pew Research Center. “Public Trust in Government: 1958 – 2024”. June  2024.https://www.pewresearch.org/politics/2024/06/24/public-trust-in-government-1958-2024/  118 SteelFisher, G., M. G. Finding, H.C. Caporello, K.M. Lubell, et al. “Trust in U.S. Federal, State, and Local Public Health  Agencies During COVID-19: Responses and Policy Implications.” Health Affairs, vol 42, no. 3, March 2023, pp. 328 –  337. https://doi.org/10.1377/hlthaff.2022.01204  119 Ghosh, J.K., B. Martinez, C. Beck, and C. Rogers. “Community Outbreak Preparedness Index (COPI).” Heluna Health,  October 2023. COPI_Technical_Report_October_2023.pdf.  120 Federal Emergency Management Agency. “Pre-Disaster Recovery Planning Guide for Tribal Governments (FP 104- 008-02).” U.S. Department of Homeland Security, September 2019. https://www.fema.gov/sites/default/files/2020- 07/pre-disaster-recovery-planning-guide-for-tribal-government.pdf.  121 Federal Emergency Management Agency. “2022–2026 FEMA National Tribal Strategy.” U.S. Department of  Homeland Security, 2022. https://www.fema.gov/sites/default/files/documents/fema_national-tribal- strategy_08182022.pdf.    References  Mathematica® Inc.  43    122 Lorenzoni, Nina, Stephanie Kainrath, Maria Unterholzner, and Harold Stummer. ""Instruments for Disaster  Preparedness Evaluation: A Scoping Review."" Australian Journal of Emergency Management, vol. 37, no. 3, 2022,  pp. 56.  123 Moulton, A.D. “A COVID-19 Lesson: Better Health Emergency Preparedness Standards Are Needed.” Health  Security, vol. 20, no. 6, 2022, pp. 457–466. https://doi.org/10.1089/hs.2022.0037.   124 Keim, M.E., and A.P. Lovallo. “Validity of the National Health Security Preparedness Index as a Predictor of Excess  COVID-19 Mortality.” Prehospital and Disaster Medicine, vol. 36, no. 2, 2021, pp. 141–144.  https://doi.org/10.1017/S1049023X20001521.  125 Kachali, Hlekiwe, Ira Haavisto, Riikka-Leena Leskelä, Auri Väljä, and Mikko Nuutinen. “Are Preparedness Indices  Reflective of Pandemic Preparedness? A COVID-19 Reality Check.” International Journal of Disaster Risk Reduction,  vol. 77, 2022, pp. 2212–4209. https://doi.org/10.1016/j.ijdrr.2022.103074.  126 NACCHOO, “2022 Preparedness Profile Study.” 2023. https://www.naccho.org/uploads/downloadable- resources/2022PrepProfile-PREVIEWREPORT.pdf.   127 National Health Security Preparedness Index. “Measures List: National Health Security Preparedness Index, 2021  Release.” 2021. https://nhspi.org/wp-content/uploads/2021/06/2021-NHSPI-Measure-List.pdf.  128 The White House. “National Cybersecurity Strategy.” March 2023. https://www.whitehouse.gov/wp- content/uploads/2023/03/National-Cybersecurity-Strategy-2023.pdf.  129 National Association of County and City Health Officials. “Hurricane and Flooding Preparedness Resources.”2015.  https://www.naccho.org/blog/articles/hurricane-and-flooding-preparedness-resources.  130 National Association of County and City Health Officials. “2022 Preparedness Profile Study.” 2023.  https://www.naccho.org/uploads/downloadable-resources/2022PrepProfile-PREVIEWREPORT.pdf.  131 National Health Security Preparedness Index. “Measures List: National Health Security Preparedness Index, 2021  Release.” 2021. https://nhspi.org/wp-content/uploads/2021/06/2021-NHSPI-Measure-List.pdf.  132 Qari, S.H., H.R. Yusuf, S.L. Groseclose, M.R. Leinhos, and E.G. Carbone. “Public Health Emergency Preparedness  System Evaluation Criteria and Performance Metrics: A Review of Contributions of the CDC-Funded Preparedness  and Emergency Response Research Centers.” Disaster Medicine and Public Health Preparedness, vol. 13, no. 3,  2019, pp. 626–638. https://doi.org/10.1017/dmp.2018.110.  133 Government Accountability Office. “COVID-19: Pandemic Lessons Highlight the Need for Public health Situational  Network.” 2022. https://www.gao.gov/products/gao-22-104600.  134 Ibid.  135 Lorenzoni, Nina, Stephanie Kainrath, Maria Unterholzner, and Harold Stummer. ""Instruments for Disaster  Preparedness Evaluation: A Scoping Review."" Australian Journal of Emergency Management, vol. 37, no. 3, 2022,  pp. 56.  136 Ibid.  137 Qari, S.H., H.R. Yusuf, S.L. Groseclose, M.R. Leinhos, and E.G. Carbone. “Public Health Emergency Preparedness  System Evaluation Criteria and Performance Metrics: A Review of Contributions of the CDC-Funded Preparedness  and Emergency Response Research Centers.” Disaster Medicine and Public Health Preparedness, vol. 13, no. 3,  2019, pp. 626–638. https://doi.org/10.1017/dmp.2018.110.  138 Institute of Medicine Committee on Post-Disaster Recovery of a Community's Public Health, Medical, and Social  Services. “Healthy, resilient, and sustainable communities after disasters: strategies, opportunities, and planning  for recovery.” Washington (DC): National Academies Press (US); 2015 Sep 10. Available from:  https://www.ncbi.nlm.nih.gov/books/NBK316532/ doi: 10.17226/18996.  139 Stribling, J., A. Clifton, G. McGill, and K. de Vries. “Examining the UK Covid-19 Mortality Paradox: Pandemic  Preparedness, Healthcare Expenditure, and the Nursing Workforce.”  Journal of Advanced Nursing, vol. 76, no. 12,  2020, pp. 3218–3227. https://doi.org/10.1111/jan.14562.     References  Mathematica® Inc.  44    140 Kaiser, M.A., T.Y. Chen, and P. Gluckman. “Should Policy Makers Trust Composite Indices? A Commentary on the  Pitfalls of Inappropriate Indices for Policy Formation.” Health Research Policy and Systems, vol. 19, no.1, March  2021. https://doi.org10.1186/s12961-021-00702-4.  141 Keim, M.E., and A.P. Lovallo. “Validity of the National Health Security Preparedness Index as a Predictor of Excess  COVID-19 Mortality.” Prehospital and Disaster Medicine, vol. 36, no. 2, 2021, pp. 141–144.  https://doi.org/10.1017/S1049023X20001521.  142 Choo, E.K. “COVID-19 Fault Lines.” Lancet, vol. 395, no. 10233, 2020, pp. 1333. https://doi.org/10.1016/S0140- 6736(20)30812-6.  143 Liao, T.F., and F. De Maio. “Association of Social and Economic Inequality with Coronavirus Disease 2019 Incidence  and Mortality Across US Counties.” JAMA Network Open, vol. 4, no. 1, 2021, article e2034578.  https://doi.org/10.1001/jamanetworkopen.2020.34578.  144 Mays, G., and M. Childress. “2021 Release of National Health Security Preparedness Index.” University of Colorado,  Colorado School of Public Health, June 2021.   145 Centers for Disease Control and Prevention. “Public Health Response Readiness Framework: 2024-–2028 Program  Priorities – Defines Excellence in Response Operations.” 2024.  https://www.cdc.gov/orr/readiness/phep/00_docs/CDC-PHEP-Response-Readiness-Framework_January- 2024_508c.pdf.  146 American Medical Association, Planned Parenthood Federation of America, American Association of Public Health  Physicians, National Birth Equity Collaborative, American College of Preventive Medicine, American Public Health  Association, For the Culture Consulting, LLC, and New York City Pandemic Response Institute. “Embedding Equity  in Crisis Preparedness and Response.” 2023. https://www.ama-assn.org/about/ama-center-health- equity/embedding-equity-crisis-preparedness-and-response-health-systems. Accessed May 10, 2024.  147 Congressional Research Service. “Tracking COVID-19: U.S. Public Health Surveillance and Data.” 2020.  https://crsreports.congress.gov/product/pdf/R/R46588.  148 Centers for Disease Control and Prevention. “Data Modernization Initiative website.” n.d.  https://www.cdc.gov/surveillance/data-modernization/index.html. Accessed May 2, 2024.  149 National Academies of Sciences, Engineering, and Medicine. “Evidence Based Practice for Public Health Emergency  Preparedness and Response.” 2020.  150 Samet, J. and R.C. Brownson. “Reimagining Public Health: Mapping a Path Forward”. Health Affairs, vol. 43, no. 6,  2024, pp. 750 – 758. https://doi.org/10.1377/hlthaff.2024.00007.   151 Lichiello, P., and B. Turnock. Guidebook to Performance Management. 1999.  https://www.phf.org/resourcestools/documents/pmcguidebook.pdf.   152 Ibid.  153 Roper, W., and B. Mays. “Performance Measurement in Public Health: Conceptual and Methodological Issues in  Building the Science Base.” Journal of Public Health, September 2000.  154 McDavid, J. C., and L.R.L. Hawthorn, L. R. L. Program Evaluation and Performance Measurement: An Introduction to  Practice. Sage Publications, Inc, 2006.  155 Roper, W., and B. Mays. “Performance Measurement in Public Health: Conceptual and Methodological Issues in  Building the Science Base.” Journal of Public Health, September 2000.  156 Ibid.  157 Cox, R., and M. Hamlen. “Community Disaster Resilience and the Rural Resilience Index.” American Behavioral  Scientist, vol. 59, no. 2, February 2015.  158 Lorenzoni, Nina, Stephanie Kainrath, Maria Unterholzner, and Harold Stummer. ""Instruments for Disaster  Preparedness Evaluation: A Scoping Review."" Australian Journal of Emergency Management, vol. 37, no. 3, 2022,  pp. 56.  159 Glik, D.C., D.P. Eisenman, I. Donatello, A. Afifi, M. Stajura, M.L. Prelip, J. Sammartinova, et al. “Reliability and Validity  of the Assessment for Disaster Engagement with Partners Tool (ADEPT) for Local Health Departments.” Public  Health Reports, vol. 129, no. 6, suppl. 4, 2014, pp. 77–86. https://doi.org/10.1177/00333549141296S411.     References  Mathematica® Inc.  45    160 Acosta, J.D., L. Burgette, A. Chandra, D.P. Eisenman, I. Gonzalez, D. Varda, and L. Xenakis. “How Community and  Public Health Partnerships Contribute to Disaster Recovery and Resilience.” Disaster Medicine and Public Health  Preparedness, vol. 12, no. 5, 2018, pp. 635–643. https://doi.org/10.1017/dmp.2017.130.   161 Dorn, B.C., E. Savoia, M.A. Testa, M.A. Stoto, and L.J. Marcus. “Development of a Survey Instrument to Measure  Connectivity to Evaluate National Public Health Preparedness and Response Performance.” Public Health Reports,  vol. 122, no. 3, 2007, pp. 329-38. doi: 10.1177/003335490712200306. PMID: 17518304; PMCID: PMC1847495.  Development of a Survey Instrument to Measure Connectivity to Evaluate National Public Health Preparedness  and Response Performance - PMC (nih.gov).  162 Ibid.  163 Technical Resources, Assistance Center, and Information Exchange. ""ASPR TRACIE Emergency Preparedness  Information Modules for Nurses in Acute Care Settings,"" February 2022.  https://files.asprtracie.hhs.gov/documents/aspr-tracie-emergency-preparedness-information-modules-for-nurses- and-economic-framework.pdf. Accessed May 10, 2024.  164 Society for Healthcare Epidemiology of America. ""Outbreak Response Training Program."" https://learningce.shea- online.org/content/sheacdc-outbreak-response-training-program-ortp#group-tabs-node-course-default1.  Accessed May 10, 2024.  165 Haan, Katherine. ""Best Learning Management Systems (LMS) of 2024."" Forbes Advisor, April 8, 2024.  https://www.forbes.com/advisor/business/best-learning-management- systems/#:~:text=Learning%20management%20systems%2C%20or%20LMS%2C%20are%20software%20platform s,as%20to%20provide%20compliance%20training%20or%20customer%20education. Accessed May 10, 2024.  166 National Association of County and City Health Officials. “2022 Preparedness Profile Study.” 2023.  https://www.naccho.org/uploads/downloadable-resources/2022PrepProfile-PREVIEWREPORT.pdf.  167 Choo, E.K. “COVID-19 Fault Lines.” Lancet, vol. 395, no. 10233, 2020, pp. 1333. https://doi.org/10.1016/S0140- 6736(20)30812-6; Liao, T.F., and F. De Maio. “Association of Social and Economic Inequality with Coronavirus  Disease 2019 Incidence and Mortality Across US Counties.” JAMA Network Open, vol. 4, no. 1, 2021, article  e2034578. https://doi.org/10.1001/jamanetworkopen.2020.34578.  168 Krieger, Nancy, Christian Testa, Jarvis T. Chen, William P. Hanage, and Alecia J. McGregor. “Relationship of Political  Ideology of U.S. Federal and State Elected Officials and Key COVID Pandemic Outcomes Following Vaccine Rollout  to Adults: April 2021–March 2022.” The Lancet Regional Health - Americas, vol. 16, 2022, article 100384.  https://doi.org/10.1016/j.lana.2022.100384; Technical Expert Panel.  169 Stoto, M., and C. Nelson. “Measuring and Assessing Public Health Emergency Preparedness: A Methodological  Primer.” SSRN, August 2023. http://dx.doi.org/10.2139/ssrn.4538548; Technical Expert Panel.  170 Brown, Lisa M., Kathryn Hyer, and LuMarie Polivka-West, ""A comparative study of laws, rules, codes and other  influences on nursing homes' disaster preparedness in the Gulf Coast states."" Behavioral Sciences & the Law, vol.  25, no. 5, September 2007, pp. 655-675. https://doi.org/10.1002/bsl.785.   171 Botoseneanu, A., H. Wu, J. Wasserman, P.D. Jacobson, ""Achieving public health legal preparedness: how dissonant  views on public health law threaten emergency preparedness and response."" Journal of Public Health, vol. 33, no.  3, September 2011, pp. 361-368. https://doi.org/10.1093/pubmed/fdq092.   172 Chowdhury, P., S. Kumar Paul, S. Kaisar, and Md. A. Moktadir. ""COVID-19 Pandemic Related Supply Chain Studies:  A Systematic Review. Trnsp Res E Logist Transp Rev. April 2021, 148: 102271. doi: 10.1016/j.tre.2021.102271.  173 DeSalvo, K., B. Hughes, M. Bassett, G. Benjamin, M. Fraser, S. Galea, N. Garcia, and J. Howard. 2021. “Public Health  COVID-19 Impact Assessment: Lessons Learned and Compelling Needs.” NAM Perspectives. Discussion Paper,  National Academy of Medicine, Washington, DC. https://doi.org/10.31478/202104c.   174 Burke, Marshall, Anne Driscoli, Sam Huft-Neal, Jiani Xue, Jennifer Burney, and Michael Wara, ""The changing risk  and burden of wildfire in the United States."" PNAS vol. 118, no. 2, 2021. https://doi.org/10.1073/pnas.2011048118.   175 Mannucci, Simona, Federica Rosso, Alessandro D'Amico, Gabriele Bernardini, and Michele Morganti, ""Flood  resilience and adaptation in the built environment: how far along are we?"" Sustainability, vol. 14 no. 7, March  2022, 4096; https://doi.org/10.3390/su14074096.    References  Mathematica® Inc.  46    176 Brown, Lisa M., Kathryn Hyer, and LuMarie Polivka-West, ""A comparative study of laws, rules, codes and other  influences on nursing homes' disaster preparedness in the Gulf Coast states."" Behavioral Sciences & the Law, vol.  25, no. 5, September 2007, pp. 655-675. https://doi.org/10.1002/bsl.785.  177 Choo, E.K. “COVID-19 Fault Lines.” Lancet, vol. 395, no. 10233, 2020, pp. 1333. https://doi.org/10.1016/S0140- 6736(20)30812-6; Liao, T.F., and F. De Maio. “Association of Social and Economic Inequality with Coronavirus  Disease 2019 Incidence and Mortality Across US Counties.” JAMA Network Open, vol. 4, no. 1, 2021, article  e2034578. https://doi.org/10.1001/jamanetworkopen.2020.34578.  178 Centers for Disease Control and Prevention. “Public Health Response Readiness Framework: 2024-2028 Program  Priorities – Defines Excellence in Response Operations.” CDC.  https://www.cdc.gov/orr/readiness/phep/00_docs/CDC-PHEP-Response-Readiness-Framework_January- 2024_508c.pdf.  179 Mays, G., and M. Childress. “2021 Release of National Health Security Preparedness Index.” University of Colorado,  Colorado School of Public Health, June 2021.  180 Federal Emergency Management Agency. “After-Action Review User Guide.” November 2023.  https://preptoolkit.fema.gov/documents/d/cip- citap/after_action_review_user_guide_november_2023_f?download=true.   181 Federal Emergency Management Agency. “National Preparedness Goal.” March 2023.  https://www.fema.gov/emergency-managers/national-preparedness/goal. Accessed May 6, 2024  182 National Academies of Sciences, Engineering, and Medicine. “Evidence Based Practice for Public Health Emergency  Preparedness and Response.” 2020.  183 Knox, C. C. “Systematic Analysis of After-Action Reports: A Plan Evaluation Methodological Approach.” Nat.  Hazards Rev., vol. 22, no. 4, 2021. https://ascelibrary.org/doi/10.1061/%28ASCE%29NH.1527-6996.0000519.   184 FEMA. “Homeland Security Exercise Evaluation Program.” Undated. https://preptoolkit.fema.gov/web/hseep- resources.   185 TRACIE. “COVID After-Action Report Resources and Examples.” January 19, 2023.  https://files.asprtracie.hhs.gov/documents/aspr-tracie-covid-19-after-action-reports---7-21-2021-508.pdf.   186 Naik, R., N. Maxwell, T. Jones, and S. A. Dopson. ""Public Health Emergency Preparedness: Qualitative Analysis of  After-Action Reports. Disaster Medicine and Public Health Preparedness, vol. 17, 2023, e523. doi:  https://doi.org/10.1017/dmp.2023.201.   187 Piltch-Loeb, R.N., C. Nelson, J. Kraemer, E. Savoia, and M. Stoto. “s.” Public Health Reports, vol. 129, no. 6,  November 2014, pp. 28–34. https://doi.org/10.1177/00333549141296S405.  188 Administration for Strategic Preparedness and Response. “2019–2023 Hospital Preparedness Program:  Performance Measures Implementation Guidance.” 2022.  https://aspr.hhs.gov/HealthCareReadiness/guidance/Documents/2019-2023-HPP-Performance-Measures- Implementation-Guidance-8Nov22.pdf.     Appendix A. Methods  Mathematica® Inc.  A.1  Appendix A. Methods  The study included an environmental scan, a technical expert panel (TEP), a synthesis analysis of themes  and gaps in current metrics, and a synthesis analysis of strategies to advance public health preparedness  metrics. In this appendix, we describe our study methods.  A. Environmental scan  With support from our partner, MDB, Inc., we conducted an environmental scan. The primary goal of the  scan was to learn about existing domestic and international public health and health-care preparedness  measures, indices, and inventories (which we collectively describe as “tools”). Although the focus of this  project is on domestic preparedness, the inclusion of articles focusing on global tools that assess nation- level preparedness provided valuable insights and perspectives that enhanced and broadened  understanding of this topic.   We identified peer-reviewed and gray literature for the environmental scan by systematically searching  PubMed and Web of Science databases and select government agency and preparedness-tool websites.  We supplemented these searches with targeted Google searches. We applied the following exclusion  criteria to literature returned through the searches that:  / Focused on individual or household emergency preparedness (for example, checklists to assess  emergency preparedness at the household level)  / Focused on studies or tools that measure preparedness at the subnational level in countries outside the  United States (for example, comparing disaster preparedness in French pediatric hospitals)  / Did not focus on any phase of the emergency preparedness cycle (for example, articles describing post- disaster outcomes that did not also assess elements of prevention, protection, mitigation, response, and  recovery)  / Did not describe efforts to measure preparedness (for examples, articles describing emergency training  curricula for health and public health workers)  / Summarized after-action reports and lessons learned from individual public health agencies  Below, we describe our approaches to identifying relevant literature.  1. Searched PubMed and Web of Science for peer-reviewed journal articles: From October through  December 2023, we conducted searches of PubMed and Web of Science records using search terms  related to public health (and variations such as community health and population health); medical  systems (including variations such as health infrastructure and hospitals); emergencies, disasters, and  hazards; preparedness, readiness, resilience, and vulnerability; and measurement (including variations  such as measures, scores, index, and scorecard).13 We restricted the PubMed and Web of Science  searches to articles published after 2012 to focus on recent tools and research, and applied the  exclusion criteria listed above to the returned articles.    13 This list of search terms is not exhaustive.  Appendix A. Methods  Mathematica® Inc.  A.2  2. Searched government agency and preparedness-tool websites: In April 2024, we conducted focused  searches on the websites of key government agencies and prominent preparedness-tool websites to  identify relevant publications on public health or health preparedness. We then applied the exclusion  criteria described above to the returned literature. We searched webpages for the following  organizations and agencies: the Centers for Disease Control and Prevention, Administration for  Strategic Preparedness and Response, the Federal Emergency Management Administration, National  Association of County and City Health Officials (NACCHO), the Association of State and Territorial  Health Officials (ASTHO), Pan American Health Organization, National Health Security Preparedness  Index (NHSPI), and Trust for America’s Health.  3. Conducted targeted Google searches: The peer-reviewed and gray literature included scoping reviews  that mentioned a few U.S.-based tools that were not associated with articles returned from the  searches noted above. To learn more about these tools, we conducted additional targeted Google  searches using the tool name as the search term.   Initial PubMed and Web of Science searches for peer-reviewed literature returned 1,878 articles. After  applying exclusion criteria, we identified 74 articles to include in the full-text review. Our searches for gray  literature on federal agency websites, preparedness-tool websites, and Google yielded an additional 30  documents.  In total, we closely reviewed 104 pieces of peer-reviewed and gray literature (Exhibit A.1). From each of  these articles, we extracted information identifying the tool being discussed (if any), challenges in  measuring preparedness, and key findings related to measuring public health and health-care  preparedness. We highlight key findings from the environmental scan in Chapter II, although we cite  relevant sources throughout the report.  Exhibit A.1. Identification of literature via databases and supplemental searches     Appendix A. Methods  Mathematica® Inc.  A.3  B. Technical expert panel  Our partner, MDB, Inc., convened a technical expert panel (TEP) in January 2024, composed of 20 experts  from federal agencies, public health and health-care organizations, and academia (a complete list of TEP  members and their affiliations is in Appendix D). We identified TEP members with a background in public  health or health preparedness, an understanding of metrics, and broad knowledge of current disaster  threats. We attempted to identify a subset of TEP members who represent the perspectives of populations  that are socially vulnerable, given the challenges in reaching these populations and disparities in post- disaster outcomes.  MDB conducted the two-hour TEP meeting virtually, facilitating a combination of breakout group and  main group discussions. The breakout group portion of the meeting consisted of three small group  discussions facilitated by MDB and Mathematica staff. Throughout the meeting, TEP members discussed  (1) the strengths and weaknesses of current measures, drawing on findings from the scan and their own  experiences; (2) key takeaways from the COVID-19 pandemic; and (3) considerations for future  preparedness measurement efforts, including new data sources that could be captured in measures.  Appendix E presents the full TEP meeting agenda.   A notetaker captured comments from TEP members, which was subsequently used for thematic analysis.  We coded the findings to understand the relative frequency of different themes and the prevalence of  these themes across breakout groups. Unless otherwise noted, all TEP findings and quotations included in  this report were mentioned by at least three TEP members across two or more breakout groups. TEP  findings are interweaved with literature findings throughout the report.  C. Analysis of themes and gaps in current preparedness metrics  Using data from the environmental scan and the TEP, we analyzed themes and gaps in current U.S.  preparedness metrics related to eight characteristics (Exhibit A.2), which we summarize in Chapter II. Our  approach to assessing themes and gaps varied by characteristic, as some characteristics are relevant at the  tool level (for example, the types and intended users of indices) whereas others are relevant at the  measure level (for example, the data source for a measure of preparedness).  Exhibit A.2. Characteristics of preparedness metrics assessed in the analysis of themes and gaps  Characteristic  Level of analysis  Types of available tools  Tool level  Purpose  Tool level  Intended users  Tool level  How metrics conceptualize preparedness  Tool and measure levels  Types of disasters addressed  Tool level  Jurisdiction levels   Tool level  Data sources  Measure level  Preparedness factors being measured  Measure level  For tool-level characteristics, we used the literature from the environmental scan to develop a de- duplicated summary table of U.S.-based preparedness tools and their characteristics (Appendix B). We  Appendix A. Methods  Mathematica® Inc.  A.4  defined “preparedness tools” as tools that assessed national, state, or local capacity in two or more phases  of the emergency preparedness cycle (that is, prevention, protection, mitigation, response, and recovery).  We did not include tools that (1) exclusively measure nation-level preparedness, (2) measure subnation  level preparedness outside the United States, or (3) measure a single phase of preparedness (for example,  vulnerability and resilience indices). From the 104 documents that we reviewed as part of the  environmental scan, we identified 12 existing preparedness tools at the STLT (state, tribal, local, or  territorial) level. Each article did not present a unique tool; many of these articles focused on the same  tools and metrics (most commonly, the NHSPI). We used the summary table to describe the frequencies  of tool-level characteristics across existing tools and to identify gaps in these characteristics. We  synthesized these findings with others from the literature and the TEP and highlighted in the report any  inconsistencies across data sources.   For measure-level characteristics, we relied on the synthesis of TEP and literature findings to identify  themes and gaps. We approached the measure-level analysis this way because it was not feasible to  develop a comprehensive, de-duplicated list of existing measures, given the vast number of measures  across existing tools. We summarize the findings from the themes and gap analysis in Chapter II.  D. Analysis of strategies to advance preparedness measurement  To inform the analysis of strategies to advance preparedness measurement (Chapter III), we first reviewed  literature on public health performance measurement to understand important attributes of metrics and  the extent to which these attributes are missing from existing preparedness metrics, as identified in the  gaps analysis. We then reviewed TEP findings to identify strategies the TEP members had suggested to  address the gaps in existing metrics. We synthesized TEP findings with themes from the literature to  develop the list of proposed strategies in Chapter III. Appendix B. Tools to Measure State, Local, Tribal, and Territorial Public Health and Health Care Preparedness in the United States  Mathematica® Inc.  B.1  Appendix B. Tools to Measure State, Local, Tribal, and Territorial Public Health and Health Care  Preparedness in the United States   Tool   Type of tool  Hazard   Domain(s)  Jurisdiction  level(s)  Data source  Intended  users  Strengths  Weaknesses  ASPR’s Hospital  Preparedness Program  performance measure set  (ASPR 2022)  Measure set  AH  • Foundation for  Health Care and  Medical Readiness  • Health Care and  Medical Response  Coordination  • Continuity of  Health Care Service  Delivery  • Medical Surge   National  HPP funding  recipient (state,  territorial, large  local)   HCC (local/  substate) level   Data self- reported by  funding  recipients; ASPR  shares data  publicly  Federal  officials  State and local  officials  HCCs and local  multi-sector  coalitions  General public  Some measures are  at the HCC (local)  level so may be  relevant and  actionable for  communities to use  for planning   Most data are reported  at the level of the  funding recipient  (generally state and  territorial).  HCCs are not consistent  sizes and are not always  comparable.   Earlier assessments of  the HPP measures in a  2013 GAO report noted  the need for annual  benchmarks to support  progress over time.a  Assessment for Disaster  Engagement with  Partners Tool (ADEPT)  (Glik et al. 2014)    Index  AH  • Communication  outreach and  coordination  • Resource  mobilization  • Organizational  capacity building  • Partnership  development and  maintenance  Local  Data self- reported/ self- administered by  local health  departments  Local health  departments   Captures valuable  information on the  linkages between  LHDs and  CBOs/FBOs for  disaster information,  resources, shelter,  and other assistance  Evidence of the tool’s  validity and predictive  capacity in real-world  emergencies is limited.  Appendix B. Tools to Measure State, Local, Tribal, and Territorial Public Health and Health Care Preparedness in the United States  Mathematica® Inc.  B.2  Tool   Type of tool  Hazard   Domain(s)  Jurisdiction  level(s)  Data source  Intended  users  Strengths  Weaknesses  Community Outbreak  Preparedness Index  (Ghosh et al. 2023)  Index  ID  • Health-care system  preparedness  • Public health  system  preparedness  • Access to health  insurance and  social safety net  services  • Community factors  Local  NACHHO data;  FEMA data;  American  Hospital  Association  Survey; National  Provider  Identifier; other  sources  STLT public  health officials  and partners;  policy makers  Fills a gap in indices  specific to local  agencies  The new tool is not  widely validated or  researched in different  local contexts.   Connectivity  Measurement Tool   (Dorn et al. 2007)  Index  AH  • System  • Coworker  • Organization  • Individual   Local  (individual,  organization  and/or system)  Self-administered  questionnaire   STLT  emergency  preparedness  agencies and  organizations  Authors believe that  aggregated scores  collected from  specific  organizations or  systems provide data  that can be used for  comparative  purposes   Self-reported data are  prone to response bias.   The tool does not  include an assessment  of performance in  relation to connectivity.   Hospital Medical Surge  Preparedness Index  (Marcozzi et al. 2020)  Index  AH  • Staff  • Supplies  • Space  • System  Local  (facility/hospital)    American  Hospital  Association  annual survey  data, U.S. Census  Bureau, and the  Dartmouth Atlas  project  Hospital  administrators   Regional and  state  emergency  planners  Hospital-level data  provide granular  information that can  be aggregated to  inform state and  regional emergency  planners   The index fails to  measure synergies  between hospitals to  improve collective  response.  The index has not been  validated in relation to  hospital performance in  the face of actual  disasters.  Appendix B. Tools to Measure State, Local, Tribal, and Territorial Public Health and Health Care Preparedness in the United States  Mathematica® Inc.  B.3  Tool   Type of tool  Hazard   Domain(s)  Jurisdiction  level(s)  Data source  Intended  users  Strengths  Weaknesses  National Health Security  Preparedness Index    Index  AH  • Health security and  surveillance  • Community  planning and  engagement  • Incident and  information  management  • Health care  delivery  • Countermeasures  management  • Environmental and  occupational  health  National  State  64 data sources,  including surveys  (i.e., BRFSS,  ASTHO profiles,  Comprehensive  Laboratory  Services Survey,  All-Hazards  Laboratory  Preparedness  Survey, and  others), safety  inspection  results, and  federal  administrative  records  Federal, state,  and local  officials  Multisector  coalitions  Researchers  General Public    ASTHO coordinated  input from a wide  range of  stakeholders to  develop the tool in  2013  Served as one of the  first tools to assess  all-hazards  preparedness at the  sub-national level in  the United States  There is some  evidence that high  preparedness scores  were associated with  lower death rates  during the COVID-19  pandemica,b  There is evidence that  the NHSPI is not a valid  predictor of excess  COVID-19 mortality  rates for 50 U.S. states  and Puerto Rico during  the first six months of  the pandemic.c    Appendix B. Tools to Measure State, Local, Tribal, and Territorial Public Health and Health Care Preparedness in the United States  Mathematica® Inc.  B.4  Tool   Type of tool  Hazard   Domain(s)  Jurisdiction  level(s)  Data source  Intended  users  Strengths  Weaknesses  Preparedness Capacity  Assessment Survey   (Davis et al. 2013)  .   Index   AH  • Surveillance and  investigation  • Plans and  protocols  • Workforce and  volunteers  • Communication  and information  dissemination  • Incident command  • Legal infrastructure  and preparedness  • Emergency events  and exercises  • Corrective action  activities  Local   Survey  completed by  local health  departments  State and local  health  departments  The domains reflect  the essential and  vital capacities for  local and state  health departments  to effectively build  and maintain their  preparedness  capabilities   Data are self-reported  and may contain  potential response bias.  The tool is not designed  for comparison across  jurisdictions.   Rapid Urban Health  Security Assessment  (RUHSA) (Boyce and Katz  2020)  Measure set  AH  • Prevent public  health emergencies  • Detect public  health emergencies  • Respond to public  health emergencies  • Other  considerations  Local  Internal data  used for self- assessment  Local  government  leaders and  policymakers   Assesses immediate  capacity to respond  to disease and  health threats at the  local level    RUHSA is a self- assessment tool, so  does not allow for  comparison or  benchmarking against  other similar  jurisdictions.  Designed specifically for  urban jurisdictions, the  tool is not applicable to  rural jurisdictions.  Appendix B. Tools to Measure State, Local, Tribal, and Territorial Public Health and Health Care Preparedness in the United States  Mathematica® Inc.  B.5  Tool   Type of tool  Hazard   Domain(s)  Jurisdiction  level(s)  Data source  Intended  users  Strengths  Weaknesses  Trust for America’s  Health Ready or Not tool  (McKillop 2024)  Measure set  AH  • Incident  management  • Institutional quality  • Water security  • Workforce  resiliency and  infection control  • Countermeasure  utilization  • Patient safety  • Health security  surveillance  • Public health  system  comprehensiveness  (not included in  2024 report)   National  (summarizes  number of  states in high,  medium, and  low tiers)  State  NHSPI data  sources and state  public health  expenditure data  collected and  analyzed by  TFAH   Federal, state,  and local  officials;  general public;  researchers;   Includes a narrower  set of measures than  NHSPI, allowing for  focused attention to  guide stakeholders  in improvement  efforts.  High TFAH  preparedness scores  were generally, but  not uniformly,  associated with  lower death rates.a  The tool’s narrow set of  goals does not consider  the full range of risks  that a jurisdiction may  face.     Note:  Strengths and weaknesses were cited within the source for each tool, unless otherwise noted. AH = all hazards; AHA = American Hospital Association; ASPR = Administration  for Strategic Preparedness and Response; ASTHO = Association of State and Territorial Health Officials; BRFSS = Behavioral Risk Factor Surveillance System; CBO =  community-based organization; FBO = faith based organization; FEMA = Federal Emergency Management Agency; HCC = health care coalition; ID = infectious disease; LHD =  local health department; NHSPI = National Health Security Preparedness Index; STLT = state, tribal, local, or territorial; TFAH = Trust for American Health index.  a Moulton, A.D. “A COVID-19 Lesson: Better Health Emergency Preparedness Standards Are Needed.” Health Security, vol. 20, no. 6, 2022, pp. 457–466.  https://doi.org/10.1089/hs.2022.0037.   b Mays, G., and M. Childress. “2021 Release of National Health Security Preparedness Index.” University of Colorado, Colorado School of Public Health, June 2021.   c Keim, M.E., and A.P. Lovallo. “Validity of the National Health Security Preparedness Index as a Predictor of Excess COVID-19 Mortality.” Prehospital and Disaster Medicine, vol. 36, no. 2,  2021, pp. 141–144. https://doi.org/10.1017/S1049023X20001521.  Appendix C. Summary of Literature Assessing the Extent to Which Preparedness Indices Predicted Outcomes During the COVID-19 Pandemic  Mathematica® Inc.  C.1  Appendix C. Summary of Literature Assessing the Extent to Which Preparedness Indices   Predicted Outcomes During the COVID-19 Pandemic  Title  Author (date)  Tool discussed  Key findings  Examining the UK Covid-19 mortality paradox:  pandemic preparedness, health-care  expenditure, and the nursing workforce   Stribling et al.  (2020)  Global Health Security (GHS)  Index  Country-level mortality rates do not appear to be related to the GHS Index in a  manner that would be expected. The top 3 scoring countries on the GHS Index  (United States, United Kingdom, and the Netherlands) all have relatively high  mortality rates, while Canada (ranked 5th on the GHS Index) has a moderate  mortality rate, but Australia and Thailand (ranked 4th and 6th, respectively)  have a very low mortality rate.  Validity of the National Health Security  Preparedness Index as a predictor of excess  COVID-19 mortality  Keim and Lovallo  (2021)  National Health Security  Preparedness Index (NHSPI)  The NHSPI tool did not appear to be a valid predictor of excess COVID-19  mortality rates for the 50 U.S. states and Puerto Rico during the first 6 months  of the pandemic (March–September 2020). Researchers found a high degree of  variance and poor correlation between excess COVID-19 mortality rates  compared to the overall score and to the 6 individual domains in the NHSPI.   Should policy makers trust composite indices? A  commentary on the pitfalls of inappropriate  indices for policy formation  Kaiser et al. (2021) GHS Index  Composite preparedness indices like the GHS have several weaknesses, which  may account for the inverted relationship between predicted vs. actual  performance. Weaknesses identified include an inconsistent scoring system,  arbitrary weighting of indicators, inclusion of indicators with questionable  validity, inability to compare scores across countries, and inability to capture  political bias.   The Global Health Security index and Joint  External Evaluation score for health preparedness  are not correlated with countries' COVID-19  detection response time and mortality outcome  Haider et al.  (2020)  GHS Index  Joint External Evaluation  (JEE) for health preparedness  The GHS index and JEE were found to be strongly correlated, but both indices  had a poor correlation with countries’ COVID-19 related mortality outcomes  and had low predictive value for detection response time from March 11–July 1,  2020.   Does it matter that standard preparedness  indices did not predict COVID-19 outcomes?  Stoto and Nelson  (2023)  GHS Index  JEE for health preparedness  A country’s success in dealing with a pandemic is highly multidimensional and  may be too complex to represent with a single number, as provided by the GHS  and JEE. Methodological issues identified include the comparability of mortality  data due to highly variable completeness and representativeness and the  inability to capture variations in the presence of effective political leadership.   The Global Health Security Index is not predictive  of coronavirus pandemic responses among  Organization for Economic Cooperation and  Development countries  Abbey et al. (2020) GHS Index  A rank-based analysis measuring total cases, total deaths, recovery rate, and  total tests performed found a discrepancy between the GHS Index rating and  the actual performance of Organization for Economic Cooperation and  Development countries during the COVID-19 pandemic.  Appendix C. Summary of Literature Assessing the Extent to Which Preparedness Indices Predicted Outcomes During the COVID-19 Pandemic  Mathematica® Inc.  C.2  Title  Author (date)  Tool discussed  Key findings  Strengthening national capacities for pandemic  preparedness: A cross-country analysis of  COVID-19 cases and deaths  Duong et al.  (2022)  IHR-SPAR  GHS index  Universal Health Coverage  Service Coverage Index  World Bank Worldwide  Governance Indicator  Countries with higher GHS and IHR-SPAR scores experienced fewer reported  COVID-19 cases and deaths but only for the first 8 weeks after the country’s  first case (for GHS, the association was limited to countries with populations  below 69.4 million). The country-level rankings from the Universal Health  Coverage Service Coverage Index and Worldwide Governance Indicator were  not associated with COVID-19 outcomes.   A COVID-19 lesson: Better health emergency  preparedness standards are needed  Moulton (2022)  NHSPI  TFAH  High NHSPI and TFAH preparedness scores were generally, but not uniformly,  associated with lower COVID-19 death rates. The measure of effectiveness of  the pandemic response was measured by states’ cumulative COVID-19 deaths  per 100,000 population from January 1, 2020–January 20, 2022.   Are preparedness indices reflective of pandemic  preparedness? A COVID-19 reality check  Kachali et al.  (2022)  IHR  GHS index  States’ reported cumulative mortality rates during the first wave of the COVID- 19 pandemic (spring 2020) were primarily negatively correlated with the  expected preparedness rank, according to IHR and GHS.  Comparison of COVID-19 Resilience Index and its  associated factors across 29 countries during the  Delta and Omicron variant periods  Huy et al. (2022)  Pandemic resilience index  Across 29 countries, the percentage of the population fully vaccinated and high  government indices scores were significantly associated with a better resilience  index score in both the COVID-19 Delta and Omicron periods. The pandemic  resilience index combines country-level mortality, hospital occupancy, and  intensive care unit occupancy rates.   Global health security preparedness and  response: An analysis of the relationship between  Joint External Evaluation scores and COVID-19  response performance  Nguyen et al.  (2021)   JEE for health preparedness  Emergency Response  Capacity Tool (ERCT)  There is low agreement between JEE scores and COVID-19 response  performance, with JEE scores often trending higher. The JEE indicator  “Emergency Operations Center (EOC) operating procedures and plans” had the  highest agreement and predicted probability with ERCT (62 percent), and the  “capacity to activate emergency operations” had the lowest predicted  probability (16 percent).   The National Health Security Preparedness Index  (2021 release)     Note: Not peer reviewed.  Mays et al. (2021)  NHSPI  COVID-19 deaths were significantly lower in communities with higher levels of  health security as measured in the index when controlling for county population  size, population density, percent aged 65 years or older, percent Black, percent  Hispanic, percent below poverty level, percent under age 65 without health  insurance, number of nursing home residents per capita, and social vulnerability  rates measured in the Community Resiliency Index, and adjusting for clustering  of counties within states.  GHS = Global Health Security; IHR = International Health Regulations; NHSPI = National Health Security Preparedness Index; SPAR = States Parties Self-Assessment Annual Report;  TFAH = Trust for American Health index. Appendix D. Technical Expert Panel Participants  Mathematica® Inc.  D.1  Appendix D. Technical Expert Panel Participants  Joseph A. Barbera, M.D., The George Washington University   Georges Benjamin, M.D., American Public Health Association   Paul Biddinger, M.D., Mass General Brigham   Laura Biesiadecki, National Association of County and City Health Officials   Jason Brown, Public Health Management Corporation   Jack Herrmann, M.S.Ed., American Red Cross   Richard Hunt, M.D., U.S. Department of Health and Human Services   Mark Keim, M.D., MBA, Fairfax County Health Department   Christine Kosmos, R.N., M.S., U.S. Centers for Disease Control and Prevention   James Lawler, M.D., M.P.H., University of Nebraska Medical Center   Beth Maldin Morgenthau, M.P.H., NYC Department of Health and Mental Hygiene   Jenna Mandel-Ricci, M.P.A., M.P.H., NYC Department of Health and Mental Hygiene   Glen Mays, Ph.D., Colorado School of Public Health   Ali Mokdad, Ph.D., University of Washington   Jennifer Nuzzo, Dr.PH., Brown University   Lisa J. Peterson, Association of State and Territorial Health Officials   Michael A. Stoto, Ph.D., Georgetown University   Eric Toner, M.D., Johns Hopkins Center for Health Security   Craig Vanderwagen, M.D., East West Protection, LLC   Michael Wargo, HCA Healthcare   Appendix E. Technical Expert Panel Agenda  Mathematica® Inc.  E.1  Appendix E. Technical Expert Panel Agenda  A. Welcome and Stage Setting  B. Breakout Session 1: Current Measures  1. What is your experience with currently available measures and their strengths and weaknesses?   2. What did COVID-19 teach us about measures?   3. What important aspects of the literature on public health and health-care preparedness measures  were not addressed in the environmental scan?   C. Breakout Session 2: Future Measures   1. What impact should measures have?   2. How should public health and health-care preparedness be measured?   3. What are common data sources that could be used for future measures?  D. Group Discussion: Framework Development   1. What current and future measures can inform the development of a framework?    2. Should the COVID-19 experience drive the development of public health and health-care  preparedness measures?   3. Should the framework be based on threats, hazards, capabilities, or capacities?  E. Summary Remarks and Conclusion         Mathematica Inc.  Our employee-owners work nationwide and around the world.  Find us at mathematica.org and edi-global.com.  Mathematica, Progress Together, and the “spotlight M” logo are registered trademarks of Mathematica Inc."|0|0|0|0|0
M-24-10-Advancing-Governance-Innovation-and-Risk-Management-for-Agency-Use-of-Artificial-Intelligence.txt|EXECUTIVE OFFICE OF THE PRESIDENT  OFFICE OF MANAGEMENT AND BUDGET   WASHINGTON, D.C. 20503       THE DI RECT O R     March 28, 2024  M-24-10  MEMORANDUM FOR THE HEADS OF EXECUTIVE DEPARTMENTS AND AGENCIES  FROM:  Shalanda D. Young  SUBJECT:  Advancing Governance, Innovation, and Risk Management for Agency Use of  Artificial Intelligence     Artificial intelligence (AI) is one of the most powerful technologies of our time, and the  President has been clear that we must seize the opportunities AI presents while managing its  risks. Consistent with the AI in Government Act of 2020,1 the Advancing American AI Act,2 and  Executive Order 14110 on the Safe, Secure, and Trustworthy Development and Use of Artificial  Intelligence, this memorandum directs agencies to advance AI governance and innovation while  managing risks from the use of AI in the Federal Government, particularly those affecting the  rights and safety of the public.3     1.   OVERVIEW    While AI is improving operations and service delivery across the Federal Government,  agencies must effectively manage its use. As such, this memorandum establishes new agency  requirements and guidance for AI governance, innovation, and risk management, including  through specific minimum risk management practices for uses of AI that impact the rights and  safety of the public.     Strengthening AI Governance. Managing AI risk and promoting AI innovation requires  effective AI governance. As required by Executive Order 14110, each agency must designate a  Chief AI Officer (CAIO) within 60 days of the date of the issuance of this memorandum. This  memorandum describes the roles, responsibilities, seniority, position, and reporting structures for  agency CAIOs, including expanded reporting through agency AI use case inventories. Because  AI is deeply interconnected with other technical and policy areas including data, information  technology (IT), security, privacy, civil rights and civil liberties, customer experience, and    1 Pub. L. No. 116-260, div. U, title 1, § 104 (codified at 40 U.S.C. § 11301 note),  https://www.congress.gov/116/plaws/publ260/PLAW-116publ260.pdf.  2 Pub. L. No. 117-263, div. G, title LXXII, subtitle B, §§ 7224(a), 7224(d)(1)(B), and 7225 (codified at 40 U.S.C.  11301 note), https://www.congress.gov/117/plaws/publ263/PLAW-117publ263.pdf.   3 This memorandum accounts for public comments that OMB received following its publication of a draft version of  this memorandum on November 1, 2023. OMB has separately published an explanation and response to public  comments, available at https://www.regulations.gov/document/OMB-2023-0020-0001.  2    workforce management, CAIOs must work in close coordination with existing responsible  officials and organizations within their agencies.     Advancing Responsible AI Innovation. With appropriate safeguards in place, AI can be  a helpful tool for modernizing agency operations and improving Federal Government service to  the public. To that end, agencies must increase their capacity to responsibly adopt AI, including  generative AI, and take steps to enable sharing and reuse of AI models, code, and data. This  memorandum requires each agency identified in the Chief Financial Officers Act (CFO Act)4 to  develop an enterprise strategy for how they will advance the responsible use of AI. This  memorandum also provides recommendations for how agencies should reduce barriers to the  responsible use of AI, including barriers related to IT infrastructure, data, cybersecurity,  workforce, and the particular challenges of generative AI.     Managing Risks from the Use of AI. While agencies will realize significant benefits  from AI, they must also manage a range of risks from the use of AI. Agencies are subject to  existing risk management requirements relevant to AI, and this memorandum does not replace or  supersede these requirements. Instead, it establishes new requirements and recommendations  that, both independently and collectively, address the specific risks from relying on AI to inform  or carry out agency decisions and actions, particularly when such reliance impacts the rights and  safety of the public. To address these risks, this memorandum requires agencies to follow  minimum practices when using safety-impacting AI and rights-impacting AI, and enumerates  specific categories of AI that are presumed to impact rights and safety. Finally, this  memorandum also establishes a series of recommendations for managing AI risks in the context  of Federal procurement.5    2.   SCOPE     Agency adoption of AI poses many challenges, some novel and specific to AI and some  well-known. While agencies must give due attention to all aspects of AI, this memorandum is  more narrowly scoped to address a subset of AI risks, as well as governance and innovation  issues that are directly tied to agencies’ use of AI. The risks addressed in this memorandum  result from any reliance on AI outputs to inform, influence, decide, or execute agency decisions  or actions, which could undermine the efficacy, safety, equitableness, fairness, transparency,  accountability, appropriateness, or lawfulness of such decisions or actions.6     4 31 U.S.C. § 901(b).  5 Consistent with provisions of the AI in Government Act of 2020, the Advancing American AI Act, and Executive  Order 14110 directing the publication of this memorandum, this memorandum sets forth multiple independent  requirements and recommendations for agencies, and OMB intends that these requirements and recommendations be  treated as severable. For example, the memorandum’s provisions regarding the strengthening of AI governance in  Section 2 are capable of operating independently, and serve an independent purpose, from the required risk  management practices set forth in Section 5. Likewise, each of Section 5’s individual risk management practices  serves an independent purpose and can function independently from the other risk management practices.  Accordingly, while this memorandum governs only agencies’ own use of AI and does not create rights or  obligations for the public, in the event that a court were to stay or enjoin application of a particular provision of this  memorandum, or its application to a particular factual circumstance, OMB would intend that the remainder of the  memorandum remain operative.   6 The subset of AI risks addressed in this memorandum is generally referred to in this document as “risks from the  use of AI”, and a full definition for this term is provided in Section 6.  3    This memorandum does not address issues that are present regardless of whether AI is  used versus any other software, such as issues with respect to Federal information and  information systems in general. In addition, this memorandum does not supersede other, more  general Federal policies that apply to AI but are not focused specifically on AI, such as policies  that relate to enterprise risk management, information resources management, privacy,  accessibility, Federal statistical activities, IT, or cybersecurity.    Agencies must continue to comply with applicable OMB policies in other domains  relevant to AI, and to coordinate compliance across the agency with all appropriate officials. All  agency responsible officials retain their existing authorities and responsibilities established in  other laws and policies.    a. Covered Agencies. Except as specifically noted, this memorandum applies to all agencies  defined in 44 U.S.C. § 3502(1).7 As noted in the relevant sections, some requirements in this  memorandum apply only to Chief Financial Officers Act (CFO Act) agencies as identified in 31  U.S.C. § 901(b), and other requirements do not apply to elements of the Intelligence Community,  as defined in 50 U.S.C. § 3003.      b. Covered AI. This memorandum provides requirements and recommendations that, as  described in more detail below, apply to new and existing AI that is developed, used, or procured  by or on behalf of covered agencies. This memorandum does not, by contrast, govern:   i.  agencies’ regulatory actions designed to prescribe law or policy regarding non-agency  uses of AI;   ii.  agencies’ evaluations of particular AI applications because the AI provider is the target or  potential target of a regulatory enforcement, law enforcement, or national security  action;8  iii.  agencies’ development of metrics, methods, and standards to test and measure AI, where  such metrics, methods, and standards are for use by the general public or the government  as a whole, rather than to test AI for a particular agency application9; or  iv.  agencies’ use of AI to carry out basic research or applied research, except where the  purpose of such research is to develop particular AI applications within the agency.    7 The term “agency,” as used in both the AI in Government Act of 2020 and the Advancing American AI Act, is  defined as “any executive department, military department, Government corporation, Government controlled  corporation, or other establishment in the executive branch of the Government (including the Executive Office of the  President), or any independent regulatory agency,” but does not include the Government Accountability Office; the  Federal Election Commission; the governments of the District of Columbia and of the territories and possessions of  the United States, and their various subdivisions; or Government-owned contractor-operated facilities, including  laboratories engaged in national defense research and production activities. 44 U.S.C. § 3502(1); see AI in  Government Act of 2020 § 102(2) (defining “agency” by reference to § 3502); Advancing American AI Act §  7223(1) (same). As a result, independent regulatory agencies as defined in 44 U.S.C. § 3502(5), which were not  included in the definitions of “agency” in Executive Order 13960 and Executive Order 14110, are covered by this  memorandum.  8 AI is not in scope when it is the target or potential target of such an action, but it is in scope when the AI is used to  carry out an enforcement or national security action. For example, when evaluating an AI tool to determine whether  it violates the law, the AI would not be in scope; if agencies were using that same tool to assess a different target,  then the AI would be in scope.   9 Examples include agency actions to develop, for general use, standards or testing methodologies for evaluating or  red-teaming AI capabilities.  4    The requirements and recommendations of this memorandum apply to system functionality that  implements or is reliant on AI, rather than to the entirety of an information system that  incorporates AI. As noted in the relevant sections, some requirements in this memorandum apply  only in specific circumstances in which agencies use AI, such as when the AI may impact rights  or safety.     c. Applicability to National Security Systems. This memorandum does not cover AI when it is  being used as a component of a National Security System.10     3.    STRENGTHENING ARTIFICIAL INTELLIGENCE GOVERNANCE    The head of each covered agency is responsible for pursuing AI innovation and ensuring  that their agency complies with AI requirements in relevant law and policy, including the  requirement that risks from the agency’s use of AI are adequately managed. Doing so requires a  strong governance structure and agencies are encouraged to strategically draw upon their policy,  programmatic, research and evaluation, and regulatory functions to support the implementation  of this memorandum’s requirements and recommendations. The head of each covered agency  must also consider the financial, human, information, and infrastructure resources necessary for  implementation, prioritizing current resources or requesting additional resources via the budget  process, as needed to support the responsibilities identified in this memorandum.     To improve accountability for AI issues, agencies must designate a Chief AI Officer,  consistent with Section 10.1(b) of Executive Order 14110. CAIOs bear primary responsibility on  behalf of the head of their agency for implementing this memorandum and coordinating  implementation with other agencies. This section defines CAIOs’ roles, responsibilities,  seniority, position, and reporting structure.     a. Actions    i.  Designating Chief AI Officers. Within 60 days of the issuance of this memorandum, the  head of each agency must designate a CAIO. To ensure the CAIO can fulfill the  responsibilities laid out in this memorandum, agencies that have already designated a  CAIO must evaluate whether they need to provide that individual with additional  authority or appoint a new CAIO. Agencies must identify these officers to OMB through  OMB’s Integrated Data Collection process or an OMB-designated successor process.  When the designated individual changes or the position is vacant, agencies must notify  OMB within 30 days.     ii.  Convening Agency AI Governance Bodies. Within 60 days of the issuance of this  memorandum, each CFO Act agency must convene its relevant senior officials to    10 AI innovation and risk for National Security Systems must be managed appropriately, but these systems are  governed through other policy. For example, Section 4.8 of Executive Order 14110 directs the development of a  National Security Memorandum to govern the use of AI as a component of a National Security System, and agencies  also have existing guidelines in place, such as the Department of Defense’s (DoD) Responsible Artificial  Intelligence Strategy and Implementation Pathway and the Office of the Director of National Intelligence’s  Principles of Artificial Intelligence Ethics for the Intelligence Community, as well as policies governing specific  high-risk national security applications of AI, such as DoD Directive 3000.09, Autonomy in Weapon Systems.   5    coordinate and govern issues tied to the use of AI within the Federal Government,  consistent with Section 10.1(b) of Executive Order 14110 and the detailed guidance in  Section 3(c) of this memorandum.     iii.  Compliance Plans. Consistent with Section 104(c) and (d) of the AI in Government Act  of 2020, within 180 days of the issuance of this memorandum or any update to this  memorandum, and every two years thereafter until 2036, each agency must submit to  OMB and post publicly on the agency’s website either a plan to achieve consistency with  this memorandum, or a written determination that the agency does not use and does not  anticipate using covered AI. Agencies must also include plans to update any existing  internal AI principles and guidelines to ensure consistency with this memorandum.11  OMB will provide templates for these compliance plans.    iv.  AI Use Case Inventories. Each agency (except for the Department of Defense and the  Intelligence Community) must individually inventory each of its AI use cases at least  annually, submit the inventory to OMB, and post a public version on the agency’s  website. OMB will issue detailed instructions for the inventory and its scope through its  Integrated Data Collection process or an OMB-designated successor process. Beginning  with the use case inventory for 2024, agencies will be required, as applicable, to identify  which use cases are safety-impacting and rights-impacting AI and report additional detail  on the risks—including risks of inequitable outcomes—that such uses pose and how  agencies are managing those risks.    v.  Reporting on AI Use Cases Not Subject to Inventory. Some AI use cases are not  required to be individually inventoried, such as those in the Department of Defense or  those whose sharing would be inconsistent with applicable law and governmentwide  policy. On an annual basis, agencies must still report and release aggregate metrics about  such use cases that are otherwise within the scope of this memorandum, the number of  such cases that impact rights and safety, and their compliance with the practices of  Section 5(c) of this memorandum. OMB will issue detailed instructions for this reporting  through its Integrated Data Collection process or an OMB-designated successor process.     b. Roles, Responsibilities, Seniority, Position, and Reporting Structure of Chief Artificial  Intelligence Officers     Consistent with Section 10.1(b)(ii) of Executive Order 14110, this memorandum defines  CAIOs’ roles, responsibilities, seniority, position, and reporting structures as follows:     i.  Roles. CAIOs must have the necessary skills, knowledge, training, and expertise to  perform the responsibilities described in this section. At CFO Act agencies, a primary  role of the CAIO must be coordination, innovation, and risk management for their  agency’s use of AI specifically, as opposed to data or IT issues in general. Agencies may  choose to designate an existing official, such as a Chief Information Officer (CIO), Chief  Data Officer (CDO), Chief Technology Officer, or similar official with relevant or    11 Given the importance of context-specific guidance on AI, agencies are encouraged to continue implementing their  agency’s AI principles and guidelines, so long as they do not conflict with this memorandum.  6    complementary authorities and responsibilities, provided they have significant expertise  in AI and meet the other requirements in this section.     ii.  Responsibilities. Executive Order 14110 tasks CAIOs with primary responsibility in  their agencies, in coordination with other responsible officials, for coordinating their  agency’s use of AI, promoting AI innovation, managing risks from the use of AI, and  carrying out the agency responsibilities defined in Section 8(c) of Executive Order  1396012 and Section 4(b) of Executive Order 14091.13 In addition, CAIOs, in  coordination with other responsible officials and appropriate stakeholders, are responsible  for:     Coordinating Agency Use of AI    A. serving as the senior advisor for AI to the head of the agency and other senior  agency leadership and within their agency’s senior decision-making forums;  B. instituting the requisite governance and oversight processes to achieve  compliance with this memorandum and enable responsible use of AI in the  agency, in coordination with relevant agency officials;  C. maintaining awareness of agency AI activities, including through the creation and  maintenance of the annual AI use case inventory;  D. developing a plan for compliance with this memorandum, as detailed in Section  3(a)(iii) of this memorandum, and an agency AI strategy, as detailed in Section  4(a) of this memorandum;  E. working with and advising the agency CFO on the resourcing requirements  necessary to implement this memorandum and providing recommendations on  priority investment areas to build upon existing enterprise capacity;   F. advising the Chief Human Capital Officer (CHCO) and where applicable, the  Chief Learning Officer, on improving workforce capacity and securing and  maintaining the skillsets necessary for using AI to further the agency’s mission  and adequately manage its risks;  G. sharing relevant information with agency officials involved in the agency’s major  AI policymaking initiatives;  H. supporting agency involvement with appropriate interagency coordination bodies  related to their agency’s AI activities, including representing the agency on the  council described in Section 10.1(a) of Executive Order 14110;  I. supporting and coordinating their agency’s involvement in AI standards-setting  bodies, as appropriate, and encouraging agency adoption of voluntary consensus  standards for AI, as appropriate and consistent with OMB Circular No. A-119, if  applicable;14    12 Executive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government,  https://www.govinfo.gov/content/pkg/FR-2020-12-08/pdf/2020-27065.pdf.  13 Executive Order 14091, Further Advancing Racial Equity and Support for Underserved Communities Through the  Federal Government, https://www.govinfo.gov/content/pkg/FR-2023-02-22/pdf/2023-03779.pdf.  14 OMB Circular A-119, Federal Participation in the Development and Use of Voluntary Consensus Standards and   in Conformity Assessment Activities (Feb. 10, 1998), https://www.whitehouse.gov/wp- content/uploads/2017/11/Circular-119-1.pdf.  7    J. promoting equity and inclusion within the agency’s AI governance structures and  incorporating diverse perspectives into the decision-making process;    Promoting AI Innovation     K. working with their agency to identify and prioritize appropriate uses of AI that  will advance both their agency’s mission and equitable outcomes;  L. identifying and removing barriers to the responsible use of AI in the agency,  including through the advancement of AI-enabling enterprise infrastructure, data  access and governance, workforce development measures, policy, and other  resources for AI innovation;   M. working with their agency’s CIO, CDO, and other relevant officials to ensure that  custom-developed AI code and the data used to develop and test AI are  appropriately inventoried, shared, and released in agency code and data  repositories in accordance with Section 4(d) of this memorandum;   N. advocating within their agency and to the public on the opportunities and benefits  of AI to the agency’s mission;    Managing Risks from the Use of AI    O. managing an agency program that supports the enterprise in identifying and  managing risks from the use of AI, especially for safety-impacting and rights- impacting AI;  P. working with relevant senior agency officials to establish or update processes to  measure, monitor, and evaluate the ongoing performance and effectiveness of the  agency’s AI applications and whether the AI is advancing the agency’s mission  and meeting performance objectives;  Q. overseeing agency compliance with requirements to manage risks from the use of  AI, including those established in this memorandum and in relevant law and  policy;  R. conducting risk assessments, as necessary, of the agency’s AI applications to  ensure compliance with this memorandum;   S. working with relevant agency officials to develop supplementary AI risk  management guidance particular to the agency’s mission, including working in  coordination with officials responsible for privacy and civil rights and civil  liberties on identifying safety-impacting and rights-impacting AI within the  agency;    T. waiving individual applications of AI from elements of Section 5 of this  memorandum through the processes detailed in that section; and  U. in partnership with relevant agency officials (e.g., authorizing, procurement, legal,  data governance, human capital, and oversight officials), establishing controls to  ensure that their agency does not use AI that is not in compliance with this  memorandum, including by assisting these relevant agency officials in evaluating  Authorizations to Operate based on risks from the use of AI.    8    iii.  Seniority. For CFO Act agencies, the CAIO must be a position at the Senior Executive  Service, Scientific and Professional, or Senior Leader level, or equivalent. In other  agencies, the CAIO must be at least a GS-15 or equivalent.    iv.  Position and Reporting Structure. CAIOs must have the necessary authority to perform  the responsibilities in this section and must be positioned highly enough to engage  regularly with other agency leadership, to include the Deputy Secretary or equivalent.  Further, CAIOs must coordinate with other responsible officials at their agency to ensure  that the agency’s use of AI complies with and is appropriate in light of applicable law and  governmentwide guidance.     c. Internal Agency AI Coordination    Agencies must ensure that AI issues receive adequate attention from the agency’s senior  leadership. Consistent with Section 10.1(b) of Executive Order 14110, agencies must take  appropriate steps, such as through the convening of an AI governance body, to coordinate  internally among officials responsible for aspects of AI adoption and risk management.  Likewise, the CAIO must be involved, at appropriate times, in broader agency-wide risk  management bodies and processes,15 including in the development of the agency risk  management strategy.16 The agency’s AI coordination mechanisms should be aligned to the  needs of the agency based on, for example, the degree to which the agency currently uses AI, the  degree to which AI could improve the agency’s mission, and the risks posed by the agency’s  current and potential uses of AI.    Each CFO Act agency is required to establish an AI Governance Board to convene  relevant senior officials to govern the agency’s use of AI, including to remove barriers to the use  of AI and to manage its associated risks. Those agencies are permitted to rely on existing  governance bodies17 to fulfill this requirement as long as they currently satisfy or are made to  satisfy both of the following:     i.  Agency AI Governance Boards must be chaired by the Deputy Secretary of the agency or  equivalent and vice-chaired by the agency CAIO, and these roles should not be assigned  to other officials. The full Board, including the Deputy Secretary, must convene on at  least a semi-annual basis. Working through this Board, CAIOs will support their  respective Deputy Secretaries in coordinating AI activities across the agency and  implementing relevant sections of Executive Order 14110.       15 See, e.g., OMB Circular No. A-123, Management’s Responsibility for Enterprise Risk Management and Internal  Control (July 15, 2016), https://www.whitehouse.gov/wp- content/uploads/legacy_drupal_files/omb/memoranda/2016/m-16-17.pdf.   16 See OMB Circular No. A-130, Managing Information as a Strategic Resource, Appx. I, sec. 5(b) (July 28, 2016),  https://www.whitehouse.gov/wp-content/uploads/legacy_drupal_files/omb/circulars/A130/a130revised.pdf.   17 An example of a qualifying body includes agency Data Governance Bodies, established by OMB Memorandum  M-19-23, Phase 1 Implementation of the Foundations for Evidence-Based Policymaking Act of 2018: Learning  Agendas, Personnel, and Planning Guidance, https://www.whitehouse.gov/wp-content/uploads/2019/07/m-19- 23.pdf.  9    ii.  Agency AI Governance Boards must include appropriate representation from senior  agency officials responsible for key enablers of AI adoption and risk management,  including at least IT, cybersecurity, data, privacy, civil rights and civil liberties, equity,  statistics, human capital, procurement, budget, legal, agency management, customer  experience, program evaluation, and officials responsible for implementing AI within an  agency’s program office(s). Agencies should also consider including representation from  their respective Office of the Inspector General.     Agencies are encouraged to have their AI Governance Boards consult external experts as  appropriate and consistent with applicable law. Experts’ individual viewpoints can help broaden  the perspective of an existing governance board and inject additional technical, ethics, civil  rights and civil liberties, or sector-specific expertise, as well as methods for engaging the  workforce.      4.   ADVANCING RESPONSIBLE ARTIFICIAL INTELLIGENCE INNOVATION     If implemented responsibly, AI can improve operations and deliver efficiencies across the  Federal Government. Agencies must improve their ability to use AI in ways that benefit the  public and increase mission effectiveness, while recognizing the limitations and risks of AI and  when it is not suited for a given task. In particular, agencies are encouraged to prioritize AI  development and adoption for the public good and where the technology can be helpful in  understanding and tackling large societal challenges, such as using AI to improve the  accessibility of government services, reduce food insecurity, address the climate crisis, improve  public health, advance equitable outcomes, protect democracy and human rights, and grow  economic competitiveness in a way that benefits people across the United States.     To achieve this, agencies should build upon existing internal enterprise capacity to  support responsible AI innovation, take actions to strengthen their AI and AI-enabling talent,18  and improve their ability to develop and procure AI. Agencies should both explore joint efforts  to scale these opportunities as well as take steps to responsibly share their AI resources across  the Federal Government and with the public.     a. AI Strategies     Within 365 days of the issuance of this memorandum, each CFO Act agency must  develop and release publicly on the agency’s website a strategy for identifying and removing  barriers to the responsible use of AI and achieving enterprise-wide improvements in AI maturity,  including:    i.  the agency’s current and planned uses of AI that are most impactful to an agency’s  mission or service delivery;19     18 Agencies should also ensure that they consider and satisfy applicable collective bargaining obligations regarding  their implementation of AI.  19 Consistent with Sections 7225(d) and 7228 of the Advancing American AI Act, this requirement applies to CFO  Act agencies except for the Department of Defense, and does not apply to elements of the Intelligence Community,    10    ii.  a current assessment of the agency’s AI maturity and the agency’s AI maturity goals;     iii.  the agency’s plans to effectively govern its use of AI, including through its Chief AI  Officer, AI Governance Boards, and improvements to its AI use case inventory;    iv.  a plan for developing sufficient enterprise capacity for AI innovation, including mature  AI-enabling infrastructure for the data, computing, development, testing, cybersecurity  compliance, deployment, and continuous-monitoring infrastructure necessary to build,  test, and maintain AI;    v.  a plan for providing sufficient AI tools and capacity to support the agency’s research and  development (R&D) work consistent with the R&D priorities developed by OMB and the  Office of Science and Technology Policy, the National AI R&D Strategic Plan, and  agency-specific R&D plans;    vi.  a plan for establishing operational and governance processes as well as developing the  necessary infrastructure to manage risks from the use of AI;     vii.  a current assessment of the agency’s AI and AI-enabling workforce capacity and  projected AI and AI-enabling workforce needs, as well as a plan to recruit, hire, train,  retain, and empower AI practitioners and achieve AI literacy for non-practitioners  involved in AI to meet those needs;     viii.  the agency’s plan to encourage diverse perspectives throughout the AI development or  procurement lifecycle, including how to determine whether a particular use of AI is  meeting the agency’s equity goals and civil rights commitments; and    ix.  specific, prioritized areas and planning for future AI investment, leveraging the annual  budget process as appropriate.     b. Removing Barriers to the Responsible Use of AI     Embracing innovation requires removing unnecessary and unhelpful barriers to the use of  AI while retaining and strengthening the guardrails that ensure its responsible use. Agencies  should create internal environments where those developing and deploying AI have sufficient  flexibility and where limited AI resources and expertise are not diverted away from AI  innovation and risk management. Agencies should take steps to remove barriers to responsible  use of AI, paying special attention to the following recommendations:    i.  IT Infrastructure. Agencies should ensure that their AI projects have access to adequate  IT infrastructure, including high-performance computing infrastructure specialized for AI  training and inference, where necessary. Agencies should also ensure adequate access for  AI developers to the software tools, open-source libraries, and deployment and    as defined in 50 U.S.C. § 3003(4). Information that would be protected from release if requested under 5 U.S.C. §  552 need not be included in the strategy.   11    monitoring capabilities necessary to rapidly develop, test, and maintain AI applications.     ii.  Data. Agencies should develop adequate infrastructure and capacity to sufficiently share,  curate, and govern agency data for use in training, testing, and operating AI. This  includes an agency’s capacity to maximize appropriate access to and sharing of both  internally held data and agency data managed by third parties. Agencies should also  explore the possible utility of and legal authorities supporting the use of publicly  available information, and encourage its use where appropriate and consistent with the  data practices outlined in this memorandum. Any data used to help develop, test, or  maintain AI applications, regardless of source, should be assessed for quality,  representativeness, and bias. These activities should be supported by resources to enable  sound data governance and management practices, particularly as they relate to data  collection, curation, labeling, and stewardship.    iii.  Cybersecurity. Agencies should update, as necessary, processes for information system  authorization and continuous monitoring to better address the needs of AI applications,  including to advance the use of continuous authorizations for AI. Consistent with Section  10.1(f) of Executive Order 14110, agency authorizing officials are encouraged to  prioritize review of generative AI and other critical and emerging technologies in  Authorizations to Operate and any other applicable release or oversight processes.    iv.  Generative AI. In addition to following the guidance provided in Section 10.1(f) of  Executive Order 14110, agencies should assess potential beneficial uses of generative AI  in their missions and establish adequate safeguards and oversight mechanisms that allow  generative AI to be used in the agency without posing undue risk.      c. AI Talent      Consistent with Section 10.2 of Executive Order 14110, agencies are strongly encouraged  to prioritize recruiting, hiring, developing, and retaining talent in AI and AI-enabling roles to  increase enterprise capacity for responsible AI innovation. Agencies should:    i.  follow the hiring practices described in the forthcoming AI and Tech Hiring Playbook  created by the Office of Personnel Management (OPM), including encouraging  applications from individuals with diverse perspectives, making best use of available  hiring and retention authorities and using descriptive job titles and skills-based  assessments;     ii.  designate an AI Talent Lead who, for at least the duration of the AI Talent Task Force,  will be accountable for reporting to agency leadership, tracking AI hiring across the  agency, and providing data to OPM and OMB on hiring needs and progress. The AI  Talent Task Force, established in Section 10.2(b) of EO 14110, will provide AI Talent  Leads with engagement opportunities to enhance their AI hiring practices and to drive  impact through collaboration across agencies, including sharing position descriptions,  coordinating marketing and outreach, shared hiring actions, and, if appropriate, sharing  12    applicant information across agencies; and    iii.  in consultation with Federal employees and their union representatives, where applicable,  provide resources and training to develop AI talent internally and increase AI training  offerings for Federal employees, including opportunities that provide Federal employees  pathways to AI occupations and that assist employees affected by the application of AI to  their work.     d. AI Sharing and Collaboration    Openness, sharing, and reuse of AI significantly enhance both innovation and  transparency, and must also be done responsibly to avoid undermining the rights, safety, and  security of the public. Agencies must share their AI code, models, and data, and do so in a  manner that facilitates re-use and collaboration Government-wide and with the public, subject to  applicable law, governmentwide guidance, and the following considerations:     i.  Sharing and Releasing AI Code and Models. Agencies must proactively share their  custom-developed code20—including models and model weights—for AI applications in  active use and must release and maintain that code as open source software on a public  repository,21 unless:  A. the sharing of the code is restricted by law or regulation, including patent or  intellectual property law, the Export Asset Regulations, the International Traffic  in Arms Regulations, and Federal laws and regulations governing classified  information;   B. the sharing of the code would create an identifiable risk to national security,  confidentiality of Government information, individual privacy, or the rights or  safety of the public;  C. the agency is prevented by a contractual obligation from doing so; or  D. the sharing of the code would create an identifiable risk to agency mission,  programs, or operations, or to the stability, security, or integrity of an agency’s  systems or personnel.     Agencies should prioritize sharing custom-developed code, such as commonly used  packages or functions, that has the greatest potential for re-use by other agencies or the  public.    ii.  Sharing and Releasing AI Data Assets. Data used to develop and test AI is likely to  constitute a “data asset” for the purposes of implementing the Open, Public, Electronic    20 A full definition for “custom-developed code” is provided in Section 6.  21 For guidance and best practices related to sharing code and releasing it as open source, agencies should consult  OMB Memorandum M-16-21, Federal Source Code Policy: Achieving Efficiency, Transparency, and Innovation  through Reusable and Open Source Software (Aug. 8, 2016), https://www.whitehouse.gov/wp- content/uploads/legacy_drupal_files/omb/memoranda/2016/m_16_21.pdf. Agencies are additionally encouraged to  draw upon existing collaboration methods to facilitate the sharing and release of code and models, including the  council described in Section 10.1(a) of Executive Order 14110, the General Services Administration’s AI  Community of Practice, and https://www.code.gov, as well as other publicly available code repositories.  13    and Necessary (OPEN) Government Data Act,22 and agencies must, if required by that  Act and pursuant to safety and security considerations in Section 4.7 of Executive Order  14110, release such data assets publicly as open government data assets.23 When sharing  AI data assets, agencies should promote data interoperability, including by coordinating  internally and with other relevant agencies on interoperability criteria and using  standardized data formats where feasible and appropriate.     iii.  Partial Sharing and Release. Where some portion of an AI project’s code, models, or  data cannot be shared or released publicly pursuant to subsections (i) and (ii) of this  section, the rest should still be shared or released where practicable, such as by releasing  the data used to evaluate a model even if the model itself cannot be safely released, or by  sharing a model within the Federal Government even if the model cannot be publicly  released. Where code, models, or data cannot be released without restrictions on who can  access it, agencies should also, where practicable, share them through Federally  controlled infrastructure that allows controlled access by entities outside the Federal  Government, such as via the National AI Research Resource.     iv.  Procuring AI for Sharing and Release. When procuring custom-developed code for AI,  data to train and test AI, and enrichments to existing data (such as labeling services),  agencies are encouraged to do so in a manner that allows for the sharing and public  release of the relevant code, models, and data.     v.  Unintended Disclosure of Data from AI Models. When agencies are deciding whether  to share and release AI models and model weights, they should assess the risk that the  models can be induced to reveal sensitive details of the data used to develop them.  Agencies’ assessment of risk should include a model-specific risk analysis.24      e. Harmonization of Artificial Intelligence Requirements       Interpreting and implementing AI management requirements in a consistent manner  across Federal agencies will create efficiencies as well as opportunities for sharing resources and  best practices. To assist in this effort and consistent with Section 10.1(a) of Executive Order  14110, OMB, in collaboration with the Office of Science and Technology Policy, will coordinate  the development and use of AI in agencies’ programs and operations—including the  implementation of this memorandum—across Federal agencies through an interagency council.  This will include at a minimum:    i.  promoting shared templates and formats;    ii.  sharing best practices and lessons learned, including for achieving meaningful  participation from affected communities and the public in AI development and    22 Title II of the Foundations for Evidence-Based Policymaking Act of 2018, P.L. 115-435.  23 Where such data is already publicly available, agencies are not required to duplicate it, but should maintain and  share the provenance of such data and how others can access it.  24 The risks of unintended disclosure differ by model, and agencies should also not assume that an AI model poses  the same privacy and confidentiality risks as the data used to develop it.  14    procurement, updating organizational processes to better accommodate AI, removing  barriers to responsible AI innovation, responding to AI incidents that may have resulted  in harm to an individual, and building a diverse AI workforce to meet the agency’s needs;    iii.  sharing technical resources for implementation of this memorandum’s risk management  practices, such as for testing, continuous monitoring, and evaluation; and    iv.  highlighting exemplary uses of AI for agency adoption, particularly uses which help  address large societal challenges.    5.   MANAGING RISKS FROM THE USE OF ARTIFICIAL INTELLIGENCE    Agencies have a range of policies, procedures, and officials in place to manage risks  related to agency information and systems. To better address risks from the use of AI, and  particularly risks to the rights and safety of the public, all agencies are required to implement  minimum practices, detailed below, to manage risks from safety-impacting AI and rights- impacting AI.25 However, Section 5(a) through (c) of this memorandum do not apply to elements  of the Intelligence Community.26     a. Actions    i.  Implementation of Risk Management Practices and Termination of Non-Compliant  AI. By December 1, 2024, agencies must implement the minimum practices in Section  5(c) of this memorandum for safety-impacting and rights-impacting AI, or else stop using  any AI in their operations that is not compliant with the minimum practices, consistent  with the details and caveats in that section.    ii.  Certification and Publication of Determinations and Waivers. By December 1, 2024,  and annually thereafter, each agency must certify the ongoing validity of the  determinations made under subsection (b) and the waivers granted under subsection (c) of  this section. To the extent consistent with law and governmentwide policy, the agency  must publicly release a summary detailing each individual determination and waiver, as  well its justification. Alternatively, if an agency has no active determinations or waivers,  it must publicly indicate that fact and report it to OMB. OMB will issue detailed  instructions for these summaries through its Integrated Data Collection process or an  OMB-designated successor process.            25 Agencies are not required to incorporate these practices into criteria for granting federal financial assistance  (FFA). However, they are encouraged, consistent with applicable law, to consider the minimum practices when  choosing such criteria.  26 Although elements of the Intelligence Community are not required to implement these practices, they are  encouraged to do so.   15    b. Determining Which Artificial Intelligence Is Presumed to Be Safety-Impacting or Rights- Impacting     All AI that matches the definitions of “safety-impacting AI” or “rights-impacting AI” as  defined in Section 6 must follow the minimum practices in Section 5(c) by the applicable  deadline. Agencies must review each current or planned use of AI to assess whether it matches  the definition of safety-impacting AI or rights-impacting AI. When conducting such an  assessment, as reflected by the definitions of safety-impacting AI and rights-impacting AI in  Section 6 of this memorandum, agencies must look to whether the particular AI output serves as  a principal basis for a decision or action.     Additionally, AI used for one of the purposes identified in Appendix I is automatically  presumed to be safety-impacting or rights-impacting. However, the agency CAIO, in  coordination with other relevant officials, may determine (or revisit a prior determination) that a  particular AI application or component27 subject to this presumption does not match the  definitions of “safety-impacting AI” or “rights-impacting AI” and is therefore not subject to the  minimum practices. The agency CAIO may make or revisit such a determination only with a  documented context-specific and system-specific risk assessment and may revisit a prior  determination at any time. This responsibility shall not be delegated to other officials. In addition  to the certification and publication requirements in Section 5(a)(ii) of this memorandum, CAIOs  must centrally track these determinations, reassess them if there are significant changes to the  conditions or context in which the AI is used, and report to OMB within 30 days of making or  changing a determination, detailing the scope, justification, and supporting evidence.     c. Minimum Practices for Safety-Impacting and Rights-Impacting Artificial Intelligence     Except as prevented by applicable law and governmentwide guidance, agencies must  apply the minimum risk management practices in this section to safety-impacting and rights- impacting AI by December 1, 2024, or else stop using the AI until they achieve compliance.  Prior to December 1, 2024, agency CAIOs should work with their agencies’ relevant officials to  bring potentially non-compliant AI into conformity, which may include requests that third-party  vendors voluntarily take appropriate action (e.g., via updated documentation or testing  measures). To ensure compliance with this requirement, relevant agency officials must use  existing mechanisms wherever possible, (for example, the Authorization to Operate process).28  An agency may also request an extension or grant a waiver to this requirement through its CAIO  using the processes detailed below.       27 CAIOs may also make these determinations across groups of AI applications or components that are closely  related by design or deployment context, provided that: (1) those systems have undergone a risk assessment that  adequately considers the risks from each individual system or from all possible systems in the group; and (2) the  systems are substantially identical in their risk profiles.  28 While agencies must use existing authorization and oversight processes to enforce these practices, the practices  are most effective when applied early in the research, design, and development of AI systems, and agencies should  plan for and adopt the practices throughout the relevant AI systems’ lifecycles and as early as possible, as  appropriate.  16    Agencies must document their implementation of these practices and be prepared to  report them to OMB, either as a component of the annual AI use case inventory, periodic  accountability reviews, or upon request as determined by OMB.     The practices in this section represent an initial baseline for managing risk from the use  of AI. Agencies must identify additional context-specific risks that are associated with their use  of AI and address them as appropriate. Such risk considerations may include impacts to safety,  security, civil rights, civil liberties, privacy, democratic values, human rights, equal  opportunities, worker well-being, access to critical resources and services, agency trust and  credibility, and market competition. To address these potential risk management gaps, agencies  are encouraged to promote and to incorporate, as appropriate, additional best practices for AI risk  management, such as from the National Institute of Standards and Technology (NIST) AI Risk  Management Framework,29 the Blueprint for an AI Bill of Rights,30 relevant international  standards,31 and the workforce principles and best practices for employers established pursuant  to Section 6(b)(i) of Executive Order 14110. Agencies are also encouraged to continue  developing their own agency-specific practices, as appropriate and consistent with this  memorandum and the principles in Executive Order 13960, Executive Order 14091, and  Executive Order 14110.     The practices in this section also do not supersede, modify, or direct an interpretation of  existing requirements mandated by law or governmentwide policy, and responsible agency  officials must coordinate to ensure that the adoption of these practices does not conflict with  other applicable law or governmentwide guidance.     i.  Exclusions from Minimum Practices. Agencies are not required to follow the minimum  practices outlined in this section when using AI solely to:  A. evaluate a potential vendor, commercial capability, or freely available AI  capability that is not otherwise used in agency operations, exclusively for the  purpose of making a procurement or acquisition decision; or  B.  achieve its conformity with the requirements of this section, such as using an AI  application in controlled testing conditions to carry out the minimum testing  requirements below.32     ii. Extensions for Minimum Practices. Agencies may request from OMB an extension of  up to one year, for a particular use of AI that cannot feasibly meet the minimum  requirements in this section by that date. OMB will not grant renewals beyond the initial  one-year extension. Any extension requests shall be submitted prior to October 15, 2024.  The request must be accompanied by a detailed justification for why the agency cannot  achieve compliance for the use of AI in question and what practices the agency has in    29 Artificial Intelligence Risk Management Framework (AI RMF 1.0), NIST Publication AI 100-1,  https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf.   30 Blueprint for an AI Bill of Rights, White House Office of Science and Technology Policy,  https://www.whitehouse.gov/wp-content/uploads/2022/10/Blueprint-for-an-AI-Bill-of-Rights.pdf.  31 For example, ISO/IEC 23894:2023 Information technology — Artificial intelligence — Guidance on risk  management, https://www.iso.org/standard/77304.html.  32 This exclusion must not be applied to any use of AI in real-world conditions, except as specifically allowed by  this section.   17    place to mitigate the risks from noncompliance, as well as a plan for how the agency will  come to implement the full set of required minimum practices from this section. OMB  will issue detailed instructions for extension requests through its Integrated Data  Collection process or an OMB-designated successor process.    iii. Waivers from Minimum Practices. In coordination with other relevant officials, an  agency CAIO may waive one or more of the requirements in this section for a specific  covered AI application or component33 after making a written determination, based upon  a system-specific and context-specific risk assessment, that fulfilling the requirement  would increase risks to safety or rights overall or would create an unacceptable  impediment to critical agency operations. An agency CAIO may also revoke a previously  issued waiver at any time. This responsibility shall not be delegated to other officials. In  addition to the certification and publication requirements in Section 5(a)(ii) of this  memorandum, CAIOs must centrally track waivers, reassess them if there are significant  changes to the conditions or context in which the AI is used, and report to OMB within  30 days of granting or revoking any waiver, detailing the scope, justification, and  supporting evidence.     iv. Minimum Practices for Either Safety-Impacting or Rights-Impacting AI.    No later than December 1, 2024, agencies must follow these practices before using new  or existing covered safety-impacting or rights-impacting AI:    A. Complete an AI impact assessment. Agencies should update their impact  assessments periodically and leverage them throughout the AI’s lifecycle. In their  impact assessments, agencies must document at least the following:    1.  The intended purpose for the AI and its expected benefit, supported by specific  metrics or qualitative analysis. Metrics should be quantifiable measures of  positive outcomes for the agency’s mission—for example to reduce costs,  wait time for customers, or risk to human life—that can be measured using  performance measurement or program evaluation methods after the AI is  deployed to demonstrate the value of using AI.34 Where quantification is not  feasible, qualitative analysis should demonstrate an expected positive  outcome, such as for improvements to customer experience, and it should  demonstrate that AI is better suited to accomplish the relevant task as  compared to alternative strategies.     2. The potential risks of using AI, as well as what, if any, additional mitigation  measures, beyond these minimum practices, the agency will take to help    33 CAIOs may also grant waivers applicable to groups of AI applications or components that are closely related by  design or deployment context, provided that: (1) those systems have undergone a risk assessment that adequately  considers the risks from each individual system or from all possible systems in the group; and (2) the systems are  substantially identical in their risk profiles.  34 For supervised and semi-supervised AI, agencies should use a target variable which can be reliably measured and  adequately represents the desired real-world outcomes.  18    reduce these risks. Agencies should document the stakeholders35 who will be  most impacted by the use of the system and assess the possible failure modes  of the AI and of the broader system, both in isolation and as a result of human  users and other likely variables outside the scope of the system itself.  Agencies should be especially attentive to the potential risks to underserved  communities. The expected benefits of the AI functionality should be  considered against its potential risks, and if the benefits do not meaningfully  outweigh the risks, agencies should not use the AI.     3. The quality and appropriateness of the relevant data. Agencies must assess  the quality of the data used in the AI’s design, development, training, testing,  and operation and its fitness to the AI’s intended purpose. In conducting  assessments, if the agency cannot obtain such data after a reasonable effort to  do so, it must obtain sufficient descriptive information from the vendor (e.g.,  AI or data provider) to satisfy the reporting requirements in this paragraph. At  a minimum, agencies must document:   a. the data collection and preparation process, which must also include the  provenance of any data used to train, fine-tune, or operate the AI;   b. the quality36 and representativeness37 of the data for its intended  purpose;   c. how the data is relevant to the task being automated and may reasonably  be expected to be useful for the AI’s development, testing, and  operation;   d. whether the data contains sufficient breadth to address the range of real- world inputs the AI might encounter and how data gaps and  shortcomings have been addressed either by the agency or vendor; and  e. if the data is maintained by the Federal Government, whether that data  is publicly disclosable as an open government data asset, in accordance  with applicable law and policy.38    B. Test the AI for performance in a real-world context. Agencies must conduct  adequate testing to ensure the AI, as well as components that rely on it, will work  in its intended real-world context. Such testing should follow domain-specific  best practices, when available, and should take into account both the specific  technology used and feedback from human operators, reviewers, employees, and    35 Stakeholders will vary depending on how AI is being used. For example, if an agency is using AI to control a  water treatment process, stakeholders may include (1) local residents; (2) state, local, tribal, and territorial  government representatives; and (3) environmental experts.  36 Consistent with OMB Memorandum M-19-15, Improving Implementation of the Information Quality Act,  https://www.whitehouse.gov/wp-content/uploads/2019/04/M-19-15.pdf, if applicable. Agencies should also consider  the National Science and Technology Council’s report Protecting the Integrity of Government Science,  https://www.whitehouse.gov/wp-content/uploads/2022/01/01-22- Protecting_the_Integrity_of_Government_Science.pdf.  37 Agencies should assess whether the data used can produce or amplify inequitable outcomes as a result of poor  data representativeness or harmful bias. Such outcomes can result from historical discrimination, such as the  perpetuation of harmful gender-based and racial stereotypes in society.     38 See 44 U.S.C. § 3502(20).  19    customers who use the service or are impacted by the system’s outcomes. Testing  conditions should mirror as closely as possible the conditions in which the AI will  be deployed. Through test results, agencies should demonstrate that the AI will  achieve its expected benefits and that associated risks will be sufficiently  mitigated, or else the agency should not use the AI. In conducting such testing, if  an agency does not have access to the underlying source code, models, or data,  the agency must use alternative test methodologies, such as querying the AI  service and observing the outputs or providing evaluation data to the vendor and  obtaining results. Agencies are also encouraged to leverage pilots and limited  releases, with strong monitoring, evaluation, and safeguards in place, to carry out  the final stages of testing before a wider release.      C. Independently evaluate the AI. Agencies, through the CAIO, an agency AI  oversight board, or other appropriate agency office with existing test and  evaluation responsibilities, must review relevant AI documentation to ensure that  the system works appropriately and as intended, and that its expected benefits  outweigh its potential risks. At a minimum, this documentation must include the  completed impact assessment and results from testing AI performance in a real- world context referenced in paragraphs (A) and (B) of this subsection. Agencies  must incorporate this independent evaluation into an applicable release or  oversight process, such as the Authorization to Operate process. The independent  reviewing authority must not have been directly involved in the system’s  development.     No later than December 1, 2024 and on an ongoing basis while using new or existing  covered safety-impacting or rights-impacting AI, agencies must ensure these practices are  followed for the AI:    D. Conduct ongoing monitoring. In addition to pre-deployment testing, agencies  must institute ongoing procedures to monitor degradation of the AI’s functionality  and to detect changes in the AI’s impact on rights and safety. Agencies should  also scale up the use of new or updated AI features incrementally where possible  to provide adequate time to monitor for adverse performance or outcomes.  Agencies should monitor and defend the AI from AI-specific exploits,39  particularly those that would adversely impact rights and safety.     E. Regularly evaluate risks from the use of AI. The monitoring process in  paragraph (D) must include periodic human reviews to determine whether the  deployment context, risks, benefits, and agency needs have evolved. Agencies  must also determine whether the current implementation of the memorandum’s  minimum practices adequately mitigates new and existing risks, or whether    39 For example, the AI-specific exploits outlined in the MITRE ATLAS framework, see https://atlas.mitre.org/ and  NIST’s taxonomy for adversarial machine learning, see https://csrc.nist.gov/pubs/ai/100/2/e2023/final.  20    updated risk response options are required.40 At a minimum, human review is  required at least on an annual basis and after significant modifications to the AI or  to the conditions or context in which the AI is used, and the review must include  renewed testing for performance of the AI in a real-world context.41 Reviews  must also include oversight and consideration by an appropriate internal agency  authority not directly involved in the system’s development or operation.    F. Mitigate emerging risks to rights and safety. Upon identifying new or  significantly altered risks to rights or safety through ongoing monitoring, periodic  review, or other mechanisms, agencies must take steps to mitigate those risks,  including, as appropriate, through updating the AI to reduce its risks or  implementing procedural or manual mitigations, such as more stringent human  intervention requirements. As significant modifications make the existing  implementation of the other minimum practices in this section less effective, such  as by making training or documentation inaccurate, agencies must update or  repeat those practices, as appropriate. Where the AI’s risks to rights or safety  exceed an acceptable level and where mitigation strategies do not sufficiently  reduce risk, agencies must stop using the AI as soon as is practicable.42     G. Ensure adequate human training and assessment. Agencies must ensure there  is sufficient training, assessment, and oversight for operators of the AI to interpret  and act on the AI’s output, combat any human-machine teaming issues (such as  automation bias), and ensure the human-based components of the system  effectively manage risks from the use of AI. Training should be conducted on a  periodic basis, determined by the agency, and should be specific to the AI product  or service being operated and how it is being used.    H. Provide additional human oversight, intervention, and accountability as part  of decisions or actions that could result in a significant impact on rights or  safety. Agencies must assess their rights-impacting and safety-impacting uses of  AI to identify any decisions or actions in which the AI is not permitted to act  without additional human oversight, intervention, and accountability. When  immediate human intervention is not practicable for such an action or decision,  agencies must ensure that the AI functionality has an appropriate fail-safe that  minimizes the risk of significant harm.43       40 In some cases, this may require a program evaluation, as defined under requirements of the Foundations for  Evidence-Based Policymaking Act of 2018, Pub. L. No. 115-435, to determine the extent to which the AI is  advancing the agency’s mission and objectives.  41 For customer-facing services, agencies should consider customer feedback in their human review criteria.  42 Agencies are responsible for determining how to safely decommission AI that was already in use at the time of  this memorandum’s release, without significant disruptions to essential government functions.   43 For example, an AI-enabled safety mechanism may require an immediate and automated action to prevent a harm  from occurring. It would not be practicable in this case to require human intervention to approve the activation of  the safety mechanism. However, agencies must still determine the appropriate oversight and accountability  processes for such a use of AI.   21    I. Provide public notice and plain-language documentation. Agencies must  ensure, to the extent consistent with applicable law and governmentwide  guidance, including concerning protection of privacy and of sensitive law  enforcement, national security, and other protected information, that the AI’s  entry in the use case inventory provides accessible documentation in plain  language of the system’s functionality to serve as public notice of the AI to its  users and the general public. Where people interact with a service relying on the  AI and are likely to be impacted by the AI, agencies must also provide reasonable  and timely notice44 about the use of the AI and a means to directly access any  public documentation about it in the use case inventory. Where agencies’ use  cases are not included in their public inventories, they may still be required to  report relevant information to OMB and must ensure adequate transparency in  their use of AI, as appropriate and consistent with applicable law.    v.  Additional Minimum Practices for Rights-Impacting AI.   No later than December 1, 2024, agencies must follow the above minimum practices for  AI that is either safety-impacting or rights-impacting. In addition, no later than December  1, 2024, agencies must also follow these minimum practices before initiating use of new  or existing rights-impacting AI:    A. Identify and assess AI’s impact on equity and fairness, and mitigate  algorithmic discrimination when it is present. Agencies must:    1. Identify and document in their AI impact assessment when using data that  contains information about a class protected by Federal nondiscrimination  laws (e.g., race, age, etc.). Given the risks arising when AI may correlate  demographic information with other types of information, agencies should  also assess and document whether the AI model could foreseeably use  other attributes as proxies for a protected characteristic and whether such  use would significantly influence model performance;   2. Assess the AI in a real-world context to determine whether the AI model  results in significant disparities in the model’s performance (e.g.,  accuracy, precision, reliability in predicting outcomes) across  demographic groups;    3. Mitigate disparities that lead to, or perpetuate, unlawful discrimination or  harmful bias, or that decrease equity as a result of the government’s use of  the AI; and  4. Consistent with applicable law, cease use of the AI for agency decision- making if the agency is unable to adequately mitigate any associated risk  of unlawful discrimination against protected classes. Agencies should  maintain appropriate documentation to accompany this decision-making,  and should disclose it publicly to the extent consistent with applicable law  and governmentwide policy.      44 Wherever feasible, agencies should provide notice to a user before the AI takes an action that significantly  impacts them.   22    B. Consult and incorporate feedback from affected communities and the public.  Consistent with applicable law and governmentwide guidance, agencies must  consult affected communities, including underserved communities, and they must  solicit public feedback, where appropriate, in the design, development, and use of  the AI and use such feedback to inform agency decision-making regarding the AI.  The consultation and feedback process must include seeking input on the  agency’s approach to implementing the minimum risk management practices  established in Section 5(c) of this memorandum, such as applicable opt-out  procedures. Agencies should consider and manage the risks of public consultation  in contexts like fraud prevention and law enforcement investigations, where  consulting with the targeted individual is impractical but consulting with a  representative group may be appropriate.45     Agencies are strongly encouraged to solicit feedback on an ongoing basis from  affected communities in particular as well as from the public broadly, especially  after significant modifications to the AI or the conditions or context in which it is  used.46 In the course of assessing such feedback, if an agency determines that the  use of AI in a given context would cause more harm than good, the agency should  not use the AI.     To carry out such consultations and feedback processes, agencies must take  appropriate steps to solicit input from the communities and individuals affected  by the AI, which could include:47    1.  direct usability testing, such as observing users interacting with the system;  2. general solicitations of comments from the public, such as a request for  information in the Federal Register or a “Tell Us About Your Experience”  sheet with an open-ended space for responses;  3. post-transaction customer feedback collections;48  4. public hearings or meetings, such as a listening session;  5. outreach to relevant Federal employee groups and Federal labor  organizations, including on the appropriate fulfillment of collective  bargaining obligations, where applicable; or   6. any other transparent process that seeks public input, comments, or  feedback from the affected groups in a meaningful, equitable, accessible,    45 For example, an agency using an AI tool to detect Federal benefits fraud is not required to consult with the target  of their investigation. However, an agency should discern when it is appropriate to consult with civil society groups,  academia, or other experts in the field to understand the technology’s impact.  46 The affected communities will vary depending on an agency’s deployment context, but may include customers  (for example, individuals, businesses, or organizations that interact with an agency) or Federal employee groups and  employees’ union representatives, when applicable.  47 Agencies are encouraged to engage with OMB on whether they are required to submit information collection  requests for OMB clearance under the Paperwork Reduction Act (44 U.S.C. § 3507) for the purposes of these  consultations and feedback processes.  48 Information on post-transaction customer feedback surveys can be found in OMB Circular A-11, Section 280 –  Managing Customer Experience and Improving Service Delivery, https://www.whitehouse.gov/wp- content/uploads/2018/06/s280.pdf.  23    and effective manner.    No later than December 1, 2024 and on an ongoing basis while using new or existing  covered rights-impacting AI, agencies must ensure these practices are followed for the  AI:    C. Conduct ongoing monitoring and mitigation for AI-enabled discrimination.  As part of the ongoing monitoring requirement established in Section 5(c)(iv)(D),  agencies must also monitor rights-impacting AI to specifically assess and mitigate  AI-enabled discrimination against protected classes, including discrimination that  might arise from unforeseen circumstances, changes to the system after  deployment, or changes to the context of use or associated data. Where sufficient  mitigation is not possible, agencies must safely discontinue use of the AI  functionality.    D. Notify negatively affected individuals. Consistent with applicable law and  governmentwide guidance, agencies must notify individuals when use of the AI  results in an adverse decision or action that specifically concerns them, such as  the denial of benefits or deeming a transaction fraudulent.49 Agencies should  consider the timing of their notice and when it is appropriate to provide notice in  multiple languages and through alternative formats and channels, depending on  the context of the AI’s use. The notice must also include a clear and accessible  means of contacting the agency and, where applicable, provide information to the  individual on their right to appeal. Agencies must also abide by any existing  obligations to provide explanations for such decisions and actions.50     E. Maintain human consideration and remedy processes. Where practicable and  consistent with applicable law and governmentwide guidance, agencies must  provide timely human consideration and potential remedy, if appropriate, to the  use of the AI via a fallback and escalation system in the event that an impacted  individual would like to appeal or contest the AI’s negative impacts on them.  Agencies that already maintain an appeal or secondary human review process for  adverse actions, or for agency officials’ substantive or procedural errors, can  leverage and expand such processes, as appropriate, or establish new processes to  meet this requirement. These remedy processes should not place unnecessary  burden on the impacted individual, and agencies should follow OMB guidance on    49 In some instances, such as an active law enforcement investigation, providing immediate notice may be  inappropriate or impractical, or disclosure may be more appropriate at a later stage (for example, prior to a  defendant’s trial).   50 Explanations might include, for example, how and why the AI-driven decision or action was taken. This does not  mean that agencies must provide a perfect breakdown of how a machine learning system came to a conclusion, as  exact explanations of AI decisions may not be technically feasible. However, agencies should still characterize the  general nature of such AI decisions through context such as the data that the decision relied upon, the design of the  AI, and the broader decision-making context in which the system operates. Such explanations should be  technologically valid, meaningful, useful, and as simply stated as possible, and higher-risk decisions should be  accompanied by more comprehensive explanations.   24    calculating administrative burden.51 Whenever agencies are unable to provide an  opportunity for an individual to appeal due to law, governmentwide guidance, or  impracticability, they must create appropriate alternative mechanisms for human  oversight of the AI.    F. Maintain options to opt-out for AI-enabled decisions. Agencies must provide  and maintain a mechanism for individuals to conveniently opt-out from the AI  functionality in favor of a human alternative, where practicable and consistent  with applicable law and governmentwide guidance. An opt-out mechanism must  be prominent, readily available, and accessible, and it is especially critical where  the affected people have a reasonable expectation of an alternative or where lack  of an alternative would meaningfully limit availability of a service or create  unwarranted harmful impacts. Agencies should also seek to ensure that the opt- out mechanism itself does not impose discriminatory burdens on access to a  government service. Agencies are not required to provide the ability to opt-out if  the AI functionality is solely used for the prevention, detection, and investigation  of fraud52 or cybersecurity incidents, or the conduct of a criminal investigation.  Pursuant to the authority for waivers defined in Section 5(c)(ii), CAIOs are  additionally permitted to waive this opt-out requirement if they can demonstrate  that a human alternative would result in a service that is less fair (e.g., produces a  disparate impact on protected classes) or if an opt-out would impose undue  hardship on the agency.     d. Managing Risks in Federal Procurement of Artificial Intelligence      This section provides agencies with recommendations for responsible procurement of AI,  supplementing an agency’s required risk management practices above for rights-impacting AI  and safety-impacting AI. In addition to these recommendations and consistent with section  7224(d) of the Advancing American AI Act and Section 10.1(d)(ii) of Executive Order 14110,  OMB will also develop an initial means to ensure that Federal contracts for the acquisition of an  AI system or service align with the guidance in this memorandum.      i.  Aligning with the Law. Agencies should ensure that procured AI is consistent with the  Constitution and complies with all other applicable laws, regulations, and policies,  including those addressing privacy, confidentiality, intellectual property, cybersecurity,  human and civil rights, and civil liberties.    ii.  Transparency and Performance Improvement. Agencies should take steps to ensure  transparency and adequate performance for their procured AI, including by:  A. obtaining adequate documentation to assess the AI’s capabilities, such as through  the use of model, data, and system cards;     51 See OMB M-22-10 and supporting document “Strategies for Reducing Administrative Burden in Public Benefit  and Service Programs.”  52 Some uses of AI in these categories, such as the use of biometrics for identity verification, may be subject to  requirements in other guidance that would necessitate an option to opt-out, and this memorandum does not replace,  supersede, otherwise interfere with any such requirements.  25    B. obtaining adequate documentation of known limitations of the AI and any  guidelines on how the system is intended to be used;   C. obtaining adequate information about the provenance of the data used to train,  fine-tune, or operate the AI;  D. regularly evaluating claims made by Federal contractors concerning both the  effectiveness of their AI offerings as well as the risk management measures put in  place, including by testing the AI in the particular environment where the agency  expects to deploy the capability;  E. considering contracting provisions that incentivize the continuous improvement of  procured AI; and  F. requiring sufficient post-award monitoring of the AI, where appropriate in the  context of the product or service acquired.    iii.  Promoting Competition in Procurement of AI. Agencies should take appropriate steps  to ensure that Federal AI procurement practices promote opportunities for competition  among contractors and do not improperly entrench incumbents. Such steps may include  promoting interoperability so that, for example, procured AI works across multiple cloud  environments, and ensuring that vendors do not inappropriately favor their own products  at the expense of competitors’ offerings.    iv.  Maximizing the Value of Data for AI. In contracts for AI products and services,  agencies should treat relevant data, as well as improvements to that data—such as  cleaning and labeling—as a critical asset for their AI maturity. Agencies should take  steps to ensure that their contracts retain for the Government sufficient rights to data and  any improvements to that data so as to avoid vendor lock-in and facilitate the  Government’s continued design, development, testing, and operation of AI. Additionally,  agencies should consider contracting provisions that protect Federal information used by  vendors in the development and operation of AI products and services for the Federal  Government, so that such data is protected from unauthorized disclosure and use and  cannot be subsequently used to train or improve the functionality of the vendor’s  commercial offerings without express permission from the agency.       v.  Overfitting to Known Test Data. When testing AI using data that its developer may  have access to—including test data that the agency has itself shared or released— agencies should ensure, as appropriate, that their AI developers or vendors are not  directly relying on the test data to train their AI systems.53     vi.  Responsible Procurement of AI for Biometric Identification. When procuring systems  that use AI to identify individuals using biometric identifiers—e.g., faces, irises,  fingerprints, or gait—agencies are encouraged to:  A. Assess and address the risks that the data used to train or operate the AI may not  be lawfully collected or used, or else may not be sufficiently accurate to support  reliable biometric identification. This includes the risks that the biometric  information was collected without appropriate consent, was originally collected    53 For instance, using validation data to train a model could lead the model to learn spurious correlations that make  the model appear accurate in tests but harm the real-world performance of the AI system.   26    for another purpose, embeds unwanted bias, or was collected without validation  of the included identities; and  B. Request supporting documentation or test results to validate the accuracy,  reliability, and validity of the AI’s ability to match identities.     vii.  Responsibly Procuring Generative AI. Agencies are encouraged to include risk  management requirements in contracts for generative AI, and particularly for dual-use  foundation models, including:  A. requiring adequate testing and safeguards,   B. requiring results of internal or external testing and evaluation, to include AI red- teaming against risks from generative AI, such as discriminatory, misleading,  inflammatory, unsafe, or deceptive outputs;   C. requiring that generative AI models have capabilities, as appropriate and  technologically feasible, to reliably label or establish provenance for their  content as generated or modified by AI; and  D. incorporating relevant NIST standards, defined pursuant to Sections 4.1(a) and  10.1(d) of Executive Order 14110, as appropriate.    viii.  Assessing for Environmental Efficiency and Sustainability. When procuring  computationally intensive AI services, for example those that rely on dual-use foundation  models, agencies should consider the environmental impact of those services, including  whether the vendor has implemented methods to improve the efficiency and  sustainability of such AI. This should include considering the carbon emissions and  resource consumption from supporting data centers.           6.   DEFINITIONS    The below definitions apply for the purposes of this memorandum.    Accessibility: The term “accessibility” has the meaning provided in Section 2(e) of Executive  Order 14035.    Agency: The term “agency” has the meaning provided in 44 U.S.C. § 3502(1).     Algorithmic Discrimination: The term “algorithmic discrimination” has the meaning provided in  Section 10(f) of Executive Order 14091 of February 16, 2023.    Artificial Intelligence (AI): The term “artificial intelligence” has the meaning provided in  Section 238(g) of the John S. McCain National Defense Authorization Act for Fiscal Year  2019,54 which states that “the term ‘artificial intelligence’ includes the following”:  1. Any artificial system that performs tasks under varying and unpredictable circumstances  without significant human oversight, or that can learn from experience and improve  performance when exposed to data sets.    54 Pub. L. No. 115-232, § 238(g), https://www.govinfo.gov/content/pkg/PLAW-115publ232/pdf/PLAW- 115publ232.pdf.   27    2. An artificial system developed in computer software, physical hardware, or other context  that solves tasks requiring human-like perception, cognition, planning, learning,  communication, or physical action.  3. An artificial system designed to think or act like a human, including cognitive  architectures and neural networks.  4. A set of techniques, including machine learning, that is designed to approximate a  cognitive task.  5. An artificial system designed to act rationally, including an intelligent software agent or  embodied robot that achieves goals using perception, planning, reasoning, learning,  communicating, decision making, and acting.    For the purposes of this memorandum, the following technical context should guide  interpretation of the definition above:  1. This definition of AI encompasses, but is not limited to, the AI technical subfields of  machine learning (including deep learning as well as supervised, unsupervised, and semi- supervised approaches), reinforcement learning, transfer learning, and generative AI.   2. This definition of AI does not include robotic process automation or other systems whose  behavior is defined only by human-defined rules or that learn solely by repeating an  observed practice exactly as it was conducted.   3. For this definition, no system should be considered too simple to qualify as covered AI  due to a lack of technical complexity (e.g., the smaller number of parameters in a model,  the type of model, or the amount of data used for training purposes).    4. This definition includes systems that are fully autonomous, partially autonomous, and not  autonomous, and it includes systems that operate both with and without human oversight.    AI and AI-Enabling Roles: The term “AI and AI-enabling roles” refers to individuals with  positions and major duties whose contributions are important for successful and responsible AI  outcomes. AI and AI-Enabling Roles include both technical and non-technical roles, such as data  scientists, software engineers, data engineers, data governance specialists, statisticians, machine  learning engineers, applied scientists, designers, economists, operations researchers, product  managers, policy analysts, program managers, behavioral and social scientists, customer  experience strategists, human resource specialists, contracting officials, managers, and attorneys.    AI Maturity: The term “AI maturity” refers to a Federal Government organization’s capacity to  successfully and responsibly adopt AI into their operations and decision-making across the  organization, manage its risks, and comply with relevant Federal law, regulation, and policy on  AI.    AI Model: The term “AI model” has the meaning provided in Section 3(c) of Executive Order  14110.    AI Red-Teaming: The term “AI red-teaming” has the meaning provided for “AI red-teaming” in  Section 3(d) of Executive Order 14110.    28    Applied Research: The term “applied research” refers to original investigation undertaken in  order to acquire new knowledge to determine the means by which a specific practical aim or  objective may be met.    Automation Bias: The term “automation bias” refers to the propensity for humans to inordinately  favor suggestions from automated decision-making systems and to ignore or fail to seek out  contradictory information made without automation.    Basic Research: The term “basic research” refers to experimental or theoretical work undertaken  primarily to acquire new knowledge of the underlying foundations of phenomena and observable  facts without a specific application towards processes or products in mind.    CFO Act Agency: The term “CFO Act Agency” refers to the agencies identified in 31 U.S.C. §  901(b).    Custom-Developed Code: The term “custom-developed code” has the meaning provided in  Appendix A of OMB Memorandum M-16-21.    Customer Experience: The term “customer experience” has the meaning established in Section  3(b) of Executive Order 14058.55    Data Asset: The term “data asset” has the meaning provided in 44 U.S.C § 3502.    Dual-Use Foundation Model: The term “dual-use foundation model” has the meaning provided  in Section 3(k) of Executive Order 14110.    Equity: The term “equity” has the meaning provided in Section 10(a) of Executive Order  14091.56    Federal Information: The term “Federal information” has the meaning provided in OMB Circular  A-130.    Generative AI: The term “generative AI” has the meaning provided in Section 3(p) of Executive  Order 14110.    Intelligence Community: The term “intelligence community” has the meaning provided in 50  U.S.C. § 3003.    Model Weight: The term “model weight” has the meaning provided in Section 3(u) of Executive  Order 14110.      55 Executive Order 14058, Transforming Federal Customer Experience and Service Delivery To Rebuild Trust in  Government, https://www.federalregister.gov/documents/2021/12/16/2021-27380/transforming-federal-customer- experience-and-service-delivery-to-rebuild-trust-in-government.   56 Executive Order 14091, Further Advancing Racial Equity and Support for Underserved Communities Through the  Federal Government, https://www.govinfo.gov/content/pkg/FR-2023-02-22/pdf/2023-03779.pdf.  29    National Security System: The term “National Security System” has the meaning provided in 44  U.S.C. § 3552(b)(6).    Open Government Data Asset: The term “open government data asset” has the meaning provided  in 44 U.S.C § 3502.    Open Source Software: The term “open source software” has the meaning provided in Appendix  A of OMB Memorandum M-16-21.     Rights-Impacting AI:57 The term “rights-impacting AI” refers to AI whose output serves as a  principal basis for a decision or action concerning a specific individual or entity that has a legal,  material, binding, or similarly significant effect on that individual’s or entity’s:  1. Civil rights, civil liberties, or privacy, including but not limited to freedom of speech,  voting, human autonomy, and protections from discrimination, excessive punishment,  and unlawful surveillance;   2. Equal opportunities, including equitable access to education, housing, insurance, credit,  employment, and other programs where civil rights and equal opportunity protections  apply; or  3. Access to or the ability to apply for critical government resources or services, including  healthcare, financial services, public housing, social services, transportation, and essential  goods and services.    Risks from the Use of AI: The term “risks from the use of AI” refers to risks related to efficacy,  safety, equity, fairness, transparency, accountability, appropriateness, or lawfulness of a decision  or action resulting from the use of AI to inform, influence, decide, or execute that decision or  action. This includes such risks regardless of whether:  1. the AI merely informs the decision or action, partially automates it, or fully automates it;  2. there is or is not human oversight for the decision or action;   3. it is or is not easily apparent that a decision or action took place, such as when an AI  application performs a background task or silently declines to take an action; or  4. the humans involved in making the decision or action or that are affected by it are or are  not aware of how or to what extent the AI influenced or automated the decision or action.    While the particular forms of these risks continue to evolve, at least the following factors can  create, contribute to, or exacerbate these risks:   1. AI outputs that are inaccurate or misleading;  2. AI outputs that are unreliable, ineffective, or not robust;   3. AI outputs that are discriminatory or have a discriminatory effect;  4. AI outputs that contribute to actions or decisions resulting in harmful or unsafe outcomes,  including AI outputs that lower the barrier for people to take intentional and harmful  actions;  5. AI being used for tasks to which it is poorly suited or being inappropriately repurposed in  a context for which it was not intended;   6. AI being used in a context in which affected people have a reasonable expectation that a  human is or should be primarily responsible for a decision or action; and    57 Appendix I(2) of this memorandum lists AI applications that are presumed to be rights-impacting.  30    7. the adversarial evasion or manipulation of AI, such as an entity purposefully inducing AI  to misclassify an input.    This definition applies to risks specifically arising from using AI and that affect the outcomes of  decisions or actions. It does not include all risks associated with AI, such as risks related to the  privacy, security, and confidentiality of the data used to train AI or used as inputs to AI models.    Safety-Impacting AI:58 The term “safety-impacting AI” refers to AI whose output produces an  action or serves as a principal basis for a decision that has the potential to significantly impact  the safety of:  1. Human life or well-being, including loss of life, serious injury, bodily harm, biological or  chemical harms, occupational hazards, harassment or abuse, or mental health, including  both individual and community aspects of these harms;   2. Climate or environment, including irreversible or significant environmental damage;  3. Critical infrastructure, including the critical infrastructure sectors defined in Presidential  Policy Directive 2159 or any successor directive and the infrastructure for voting and  protecting the integrity of elections; or,  4. Strategic assets or resources, including high-value property and information marked as  sensitive or classified by the Federal Government.    Significant Modification: The term “significant modification” refers to an update to an AI  application or to the conditions or context in which it is used that meaningfully alters the AI’s  impact on rights or safety, such as through changing its functionality, underlying structure, or  performance such that prior evaluations, training, or documentation become misleading to users,  overseers, or individuals affected by the system. This includes significantly changing the context,  scope, or intended purpose in which the AI is used.     Underserved Communities: The term “underserved communities” has the meaning provided in  Section 10(b) of Executive Order 14091.   58 Appendix I(1) of this memorandum lists AI applications that are presumed to be safety-impacting.   59 Presidential Policy Directive 21 (PPD-21), Critical Infrastructure Security and Resilience, or successor directive,  https://obamawhitehouse.archives.gov/the-press-office/2013/02/12/presidential-policy-directive-critical- infrastructure-security-and-resil.  31    Appendix I: Purposes for Which AI is Presumed to be Safety-Impacting and Rights- Impacting    OMB has determined that the categories in this appendix in general meet the definition of  safety-impacting AI or rights-impacting AI and are automatically presumed to be safety- impacting or rights-impacting. The following lists only identify a subset of uses of AI that impact  rights and safety, and they do not represent an exhaustive list. Additionally, the presumption that  a particular use of AI in the following lists will impact rights or safety can be waived by an  agency’s CAIO with adequate justification, pursuant to the processes outlined in Section 5.     1. Purposes That Are Presumed to Be Safety-Impacting. A use of AI is presumed to be  safety-impacting if it is used or expected to be used, in real-world conditions, to control or  significantly influence the outcomes of any of the following agency activities or decisions:     a. Controlling the safety-critical functions within dams, emergency services, electrical grids,  the generation or movement of energy, fire safety systems, food safety mechanisms,  traffic control systems and other systems controlling physical transit, water and  wastewater systems, or nuclear reactors, materials, and waste;   b. Maintaining the integrity of elections and voting infrastructure;  c. Controlling the physical movements of robots or robotic appendages within a workplace,  school, housing, transportation, medical, or law enforcement setting;   d. Applying kinetic force; delivering biological or chemical agents; or delivering potentially  damaging electromagnetic impulses;   e. Autonomously or semi-autonomously moving vehicles, whether on land, underground, at  sea, in the air, or in space;   f. Controlling the transport, safety, design, or development of hazardous chemicals or  biological agents;   g. Controlling industrial emissions and environmental impacts;   h. Transporting or managing of industrial waste or other controlled pollutants;   i. Designing, constructing, or testing of industrial equipment, systems, or structures that, if  they failed, would pose a significant risk to safety;  j. Carrying out the medically relevant functions of medical devices; providing medical  diagnoses; determining medical treatments; providing medical or insurance health-risk  assessments; providing drug-addiction risk assessments or determining access to  medication; conducting risk assessments for suicide or other violence; detecting or  preventing mental-health issues; flagging patients for interventions; allocating care in the  context of public insurance; or controlling health-insurance costs and underwriting;   k. Detecting the presence of dangerous weapons or a violent act;   l. Choosing to summon first responders to an emergency;   m. Controlling access to or security of government facilities; or  n. Determining or carrying out enforcement actions pursuant to sanctions, trade restrictions,  or other controls on exports, investments, or shipping.        32    2. Purposes That Are Presumed to Be Rights-Impacting. A use of AI is presumed to be  rights-impacting if it is used or expected to be used, in real-world conditions, to control or  significantly influence the outcomes of any of the following agency activities or decisions:    a. Blocking, removing, hiding, or limiting the reach of protected speech;  b. In law enforcement contexts, producing risk assessments about individuals; predicting  criminal recidivism; predicting criminal offenders; identifying criminal suspects or  predicting perpetrators' identities; predicting victims of crime; forecasting crime;  detecting gunshots; tracking personal vehicles over time in public spaces, including  license plate readers; conducting biometric identification (e.g., iris, facial, fingerprint, or  gait matching); sketching faces; reconstructing faces based on genetic information;  monitoring social media; monitoring prisons; forensically analyzing criminal evidence;  conducting forensic genetics; conducting cyber intrusions in the course of an  investigation; conducting physical location-monitoring or tracking of individuals; or  making determinations related to sentencing, parole, supervised release, probation, bail,  pretrial release, or pretrial detention;    c. Deciding or providing risk assessments related to immigration, asylum, or detention  status; providing immigration-related risk assessments about individuals who intend to  travel to, or have already entered, the U.S. or its territories; determining individuals’  border access or access to Federal immigration related services through biometrics or  through monitoring social media and other online activity; monitoring individuals’  physical location for immigration and detention-related purposes; or forecasting the  migration activity of individuals;  d. Conducting biometric identification for one-to-many identification in publicly accessible  spaces;   e. Detecting or measuring emotions, thought, impairment, or deception in humans;  f. Replicating a person’s likeness or voice without express consent;  g. In education contexts, detecting student cheating or plagiarism; influencing admissions  processes; monitoring students online or in virtual-reality; projecting student progress or  outcomes; recommending disciplinary interventions; determining access to educational  resources or programs; determining eligibility for student aid or Federal education; or  facilitating surveillance (whether online or in-person);   h. Screening tenants; monitoring tenants in the context of public housing; providing  valuations for homes; underwriting mortgages; or determining access to or terms of home  insurance;  i. Determining the terms or conditions of employment, including pre-employment  screening, reasonable accommodation, pay or promotion, performance management,  hiring or termination, or recommending disciplinary action; performing time-on-task  tracking; or conducting workplace surveillance or automated personnel management;  j. Carrying out the medically relevant functions of medical devices; providing medical  diagnoses; determining medical treatments; providing medical or insurance health-risk  assessments; providing drug-addiction risk assessments or determining access to  medication; conducting risk assessments for suicide or other violence; detecting or  preventing mental-health issues; flagging patients for interventions; allocating care in the  context of public insurance; or controlling health-insurance costs and underwriting;   33    k. Allocating loans; determining financial-system access; credit scoring; determining who is  subject to a financial audit; making insurance determinations and risk assessments;  determining interest rates; or determining financial penalties (e.g., garnishing wages or  withholding tax returns);   l. Making decisions regarding access to, eligibility for, or revocation of critical government  resources or services; allowing or denying access—through biometrics or other means  (e.g., signature matching)—to IT systems for accessing services for benefits; detecting  fraudulent use or attempted use of government services; assigning penalties in the context  of government benefits;   m. Translating between languages for the purpose of official communication to an individual  where the responses are legally binding; providing live language interpretation or  translation, without a competent interpreter or translator present, for an interaction that  directly informs an agency decision or action; or  n. Providing recommendations, decisions, or risk assessments about adoption matching,  child protective actions, recommending child custody, whether a parent or guardian is  suitable to gain or retain custody of a child, or protective actions for senior citizens or  disabled persons.      34    Appendix II: Consolidated Table of Actions    Responsible  Entity  Action  Section  Deadline  Each Agency Designate an agency Chief AI Officer and  notify OMB  3(a)(i)  60 days  Each CFO  Act Agency  Convene agency AI Governance Board  3(a)(ii)  60 days  Each Agency Submit to OMB and release publicly an  agency plan to achieve consistency with  this memorandum or a written  determination that the agency does not use  and does not anticipate using covered AI  3(a)(iii)  180 days and  every two years  thereafter until  2036  Each CFO  Act Agency  Develop and release publicly an agency  strategy for removing barriers to the use of  AI and advancing agency AI maturity  4(a)(i)  365 days  Each  Agency**   Publicly release an expanded AI use case  inventory and report metrics on use cases  not included in public inventories  3(a)(iv),  3(a)(v)  Annually   Each  Agency*  Share and release AI code, models, and  data assets, as appropriate  4(d)  Ongoing  Each  Agency*   Stop using any safety-impacting or rights- impacting AI that is not in compliance with  Section 5(c) and has not received an  extension or waiver  5(a)(i)  December 1, 2024  (with extensions  possible)  Each  Agency*  Certify the ongoing validity of the waivers  and determinations granted under Section  5(c) and 5(b) and publicly release a  summary detailing each and its justification  5(a)(ii)  December 1, 2024  and annually  thereafter  Each  Agency*  Conduct periodic risk reviews of any  safety-impacting and rights-impacting AI  in use  5(c)(iv)(D)  At least annually  and after  significant  modifications  Each  Agency*  Report to OMB any determinations made  under Section 5(b) or waivers granted  under Section 5(c)  5(b);  5(c)(iii)  Ongoing, within  30 days of  granting waiver      * Excluding elements of the Intelligence Community.  ** Excluding elements of the Intelligence Community. The Department of Defense is exempt from the requirement  to inventory individual use cases.|1|1|1|1|1
NIST.SP.800-206.txt|ANNUAL REPORT 2018 NIST/ITL CYBERSECURITY PROGRAM NIST SPECIAL PUBLICATION 800-206 ANNUAL REPORT 2018 NIST/ITL CYBERSECURITY PROGRAM PATRICK O’REILLY, EDITOR Computer Security Division   Information Technology Laboratory KRISTINA RIGOPOULOS, EDITOR Applied Cybersecurity Division   Information Technology Laboratory   CO-EDITORS: Larry Feldman Greg Witte G2, Incorporated (“G2”) a Huntington Ingalls Company  Annapolis Junction, Maryland THIS PUBLICATION IS AVAILABLE FREE OF CHARGE FROM  https://doi.org/10.6028/NIST.SP.800-206 JANUARY 2020 U.S. DEPARTMENT OF COMMERCE Wilbur L. Ross, Jr., Secretary NATIONAL INSTITUTE OF STANDARDS AND TECHNOLOGY Walter Copan, NIST Director and Under Secretary of Commerce for Standards and Technology THIS PAGE IS INTENTIONALLY LEFT BLANK NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 I AUTHORITY This publication has been developed by NIST in accordance with its statutory  responsibilities under the Federal Information Security Modernization Act (FISMA) of  2014, 44 U.S.C. § 3551 et seq., Public Law (P.L.) 113-283. NIST is responsible for developing  information security standards and guidelines, including minimum requirements for  federal information systems, but such standards and guidelines shall not apply to  national security systems without the express approval of appropriate federal officials  exercising policy authority over such systems. This guideline is consistent with the  requirements of the Office of Management and Budget (OMB) Circular A-130. Nothing in this publication should be taken to contradict the standards and guidelines  made mandatory and binding on federal agencies by the Secretary of Commerce under  statutory authority. Nor should these guidelines be interpreted as altering or superseding  the existing authorities of the Secretary of Commerce, Director of the OMB, or any other  federal official. This publication may be used by nongovernmental organizations on a  voluntary basis and is not subject to copyright in the United States. Attribution would,  however, be appreciated by NIST. National Institute of Standards and Technology Special Publication 800-206 Natl. Inst. Stand. Technol. Spec. Publ. 800-206, 31 pages (January 2020) CODEN: NSPUE2 This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.800-206 REPORTS ON COMPUTER SYSTEMS TECHNOLOGY The Information Technology Laboratory (ITL) at the National Institute of Standards and  Technology (NIST) promotes the U.S. economy and public welfare by providing technical  leadership for the Nation’s measurement and standards infrastructure. ITL develops  tests, test methods, reference data, proof of concept implementations, and technical  analyses to advance the development and productive use of information technology.  ITL’s responsibilities include the development of management, administrative, technical,  and physical standards and guidelines for the cost-effective security and privacy of other  than national security-related information in federal information systems. The Special  Publication 800-series reports on ITL’s research, guidelines, and outreach efforts in  information system security, and its collaborative activities with industry, government,  and academic organizations. NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 II DISCLAIMER Any mention of commercial products or organizations is for informational purposes only;  it is not intended to imply recommendation or endorsement by the National Institute of  Standards and Technology, nor is it intended to imply that the products identified are  necessarily the best available for the purpose. TRADEMARK INFORMATION All names are trademarks or registered trademarks of their respective owners. NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 III FOREWORD Cybersecurity: Picking up the pace “The more things change, the more they stay the same.” (From a French proverb) Ten years ago, the National Institute of Standards and Technology (NIST) annual  report on cybersecurity featured accomplishments and challenges in quantum  computing, encryption, identity management, personal identity verification,  vulnerability measurements, assessing the security controls in federal information  systems, mobile devices, international standardization, and addressing the needs of  small and medium-sized businesses, all of which were among the many pressing topics  of the day. Sound familiar? Reviewing those topics in the NIST Fiscal Year 2008 report on computer security  activities and accomplishments might lead some to conclude that the old French  proverb is true when it comes to cybersecurity. But in this case, a more appropriate  statement might be, “The more things appear to stay the same, the more quickly they  actually change.” That certainly is true for the threat environment in which we function today. New attack  surfaces, new vulnerabilities, and new attackers emerge constantly. The creativity,  the dramatically increased frequency of attacks, and the ready availability of new  technologically enhanced modes of attack are even more difficult to identify—much  less protect, detect, respond to, and recover from—before they inflict great harm to U.S.  organizations and our economy, security, and society in general. A decade later, these changes have enormous implications in a world that is so much  more dependent on digital devices, systems, and connectivity for carrying out both the  specialized and ordinary activities that drive our economy and safeguard our security.  They create thorny challenges as we seek balance in battling attacks and attackers while  preserving our intellectual property, privacy, civil rights, and liberties. The speed with which our cybersecurity risks change means that everyone involved in  managing those risks needs to pick up the pace. That is what NIST is doing with the help  of many partners and through varied programs and approaches. In Fiscal Year 2018, we received and worked on new cybersecurity-related assignments  from Congress and the President. Those have led us to focus our attention on assisting  small businesses, forging practical solutions to address security concerns raised by the  Internet of Things, and updating our guidance on risk management and security controls  for federal agencies and others. We also launched major new initiatives, including the  development of a voluntary framework for privacy risk management, standards for  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IV post-quantum cryptography, and revisions to Federal Information Processing Standard  (FIPS) 140-3,1 Security Requirements for Cryptographic Modules. One thing has remained constant over the past decade: our commitment to cultivating  trust in information and the technology that drives the development and handling  of that information. This annual report focuses on some of NIST’s most noteworthy  cybersecurity achievements in 2018 and offers insights into our current priorities  and strategies. For a more complete review of our work, check out our primary  cybersecurity website.2 NIST welcomes all suggestions for how we can improve our cybersecurity work to  better serve the public and private sectors. And, by all means, please join us as we pick  up the pace. Donna F. Dodson NIST Chief Cybersecurity Advisor 1 Federal Information Processing Standard (FIPS) 140-3, https://csrc.nist.gov/publications/detail/fips/140/3/final  2 NIST Cybersecurity, https://www.nist.gov/topics/cybersecurity  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 V TABLE OF CONTENTS Introduction .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .   1 Imperative 1 – Advancing Cybersecurity and Privacy Standards .  .   .  .   .  .   .  .   .  .  .  .  .  .  .   3 Imperative 2 – Enhancing Risk Management  .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .  .  .  .  .  .  .  .  .   5 Imperative 3 – Strengthening Cryptographic Standards and Validation  .  .   .  .   .  .   .  .  .  .   8 Imperative 4 – Advancing Cybersecurity Research  and Applications Development .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .  .  .  .  .  .  .    12 Imperative 5 – Improving Cybersecurity Awareness, Training, Education,  and Workforce Development  .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .  .  .  .  .  .  .  .   15 Imperative 6 – Enhancing Identity and Access Management .  .   .  .   .  .   .  .   .  .   .  .  .  .  .  .   19 Imperative 7 – Bolstering Infrastructure Protection  .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .  .  .  .  .  .  .  21 Imperative 8 – Securing Emerging Technologies .  .   .  .   .  .   .  .   .  .   .  .   .  .   .  .  .  .  .  .  .  .  .   24 Imperative 9 – Advancing Security Test and Measurement Tools  .  .   .  .   .  .   .  .   .  .  .  .  .  29 THIS PAGE IS INTENTIONALLY LEFT BLANK NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 1 INTRODUCTION It is often said that cybersecurity is about people, process, and technology. That’s a  convenient way to think about cybersecurity challenges, and it is the primary approach  that the National Institute of Standards and Technology (NIST) takes in carrying out its  cybersecurity mission. This report highlights NIST’s Fiscal Year (FY) 2018 cybersecurity- related accomplishments and includes many examples of how the NIST Information  Technology Laboratory (ITL) delivers value to the nation by focusing on each element  of the people, process, and technology triad. Importantly, NIST strives to address those  three areas in an integrated fashion, knowing that siloed thinking about cybersecurity is  not a viable path to success. NIST carries out its cybersecurity responsibilities through an open, transparent, and  inclusive approach, teaming with organizations in the private sector, non-profit sphere,  academia, and government at multiple levels. It does so by cooperating with partners in  the United States and abroad who contribute to the research, development, standards,  and applications that are all needed to advance both the state-of-the-art and the state- of-practice when it comes to cybersecurity. NIST is also assisted in identifying emerging  managerial, technical, administrative, and physical safeguard issues by the Information  Security and Privacy Advisory Board (ISPAB).3 ISPAB is the Federal Advisory Committee4  that advises NIST, the Secretaries of Commerce and Homeland Security, and the Office of  Management and Budget on security and privacy matters. All of NIST’s cybersecurity work is conducted in an environment that demands  technical excellence and integrity and that aims to cultivate trust in technologies and  institutions. NIST’s portfolio of cybersecurity programs works along the full spectrum  of cybersecurity challenges and potential, from foundational research to applied  engineering and transition to practice. NIST knows that improving all aspects of cybersecurity is an imperative, not just for the  agency but for all of society that relies on technologies, products, and systems. NIST’s  FY18 premier cybersecurity accomplishments and brief insights into FY19 priorities are  captured in the cybersecurity imperatives that follow. 3 Charter of the Information Security and Privacy Advisory Board (ISPAB), https://csrc.nist.gov/CSRC/media/ Projects/ISPAB/documents/ispab_charter_2016-2018.pdf  4 Federal Advisory Committee Act, https://uscode.house.gov/view.xhtml?path=/prelim@title5/title5a/ node2&edition=prelim  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 INTRODUCTION 2 FROM STATE-OF-THE-ART TO STATE-OF-ACTUAL PRACTICE NIST’s research, standards, guidelines, and recommended best practices are  aimed at improving the effectiveness of cybersecurity strategies and measures.  They offer an expanding and up-to-date toolkit that information technology  (IT) and operational technology (OT) providers and users can put to work to  justifiably enhance trust in IT and related technologies. Historically, much of  NIST’s outputs have been developed primarily for government agencies that are  directed by statute and policy to use the Institute’s cybersecurity standards and  guidelines. However, they also are widely and voluntarily relied upon by private  sector cybersecurity practitioners, product vendors and integrators, state and  local agencies, and others who see value in NIST’s information and services. NIST recognizes that many systems owners and administrators often find it  difficult to sort through the voluminous output of NIST cybersecurity guidance to  identify standards and guidelines that apply to their areas of interest. They may  struggle to understand how these resources apply to their environments and  how NIST’s cybersecurity products can be used to address their requirements.  They may have difficulty with usability implications of integrating cybersecurity  into their systems. These current and potential users need assistance in finding  the kind of applications support that NIST offers in addressing some specific  cybersecurity implementation problems. In FY18, NIST increased its attention to  issues associated with transition to practice with an eye on accelerating adoption  of cybersecurity standards, guidelines, and best practices. NIST is ramping up  that priority even more steeply in 2019, often through the National Cybersecurity  Center of Excellence (NCCoE) as well as throughout the Information Technology  Laboratory (ITL). This report spotlights some of those initiatives. NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 3 3 IMPERATIVE 1  Advancing Cybersecurity and Privacy Standards NIST leverages foundational and applied research as well as extensive experience  acquired through decades of leadership and participation in developing national and  international standards to advance cybersecurity and privacy standards. Today, these  standards activities span cybersecurity, privacy, and cryptography, as well as newer and  emerging technology spaces such as Artificial Intelligence and the Internet of Things. About 40 NIST staff work with industry and other agencies to develop cybersecurity and  privacy standards through voluntary consensus Standards Developing Organizations  (SDOs). The NIST Cybersecurity Framework (CSF) has provided additional focus and a  systematic approach to standards work in this space. This international standards  strategy follows the guidelines defined in NISTIR 8074, Cybersecurity Interagency Report  on Strategic U.S. Government Engagement in International Standardization to Achieve  U.S. Objectives for Cybersecurity. Figure 1. NIST staff participation in cybersecurity standards activities NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 1 — Advancing Cybersecurity and Privacy Standards 4 During FY18, NIST staff actively contributed to and held leadership positions in various  SDOs, including the American National Standards Institute (ANSI), the International  Organization for Standardization (ISO), the International Electrotechnical Commission  (IEC), the Institute of Electrical and Electronics Engineers (IEEE), the Internet  Engineering Task Force (IETF), the T Trusted Computing Group (TCG), the World Wide  Web Consortium (W3C), and the 3rd Generation Partnership Project (3GPP). Many of the  international SDOs have domestic counterparts. Figure 1, on the previous page; indicates  the SDOs in which NIST is actively engaged. NIST staff have been actively participating in ISO standards bodies to raise awareness  and influence the development of privacy standards, including a new family of  ISO standards (developed primarily in ISO/IEC Joint Technical Committee (JTC 1)/ Subcommittee (SC) 27) that is aligned with the principles of the NIST Cybersecurity  Framework. Notably, NIST staff participates in the Technical Committee ISO/Program  Committee (PC) 317 – Consumer Protection: Privacy by Design for Consumer Goods  and Services, which focuses on developing ISO 31700 Consumer protection: Privacy by  Design for Consumer Goods and Services. NIST staff has also been engaged with several  privacy standards activities of the ITC 1/SC 27/Working Group (WG) 5. By participating,  NIST aims to sustain and promote the development and use of the NIST Privacy  Framework and its principles in the international arena. NIST participation has also grown considerably in Internet of Things (IoT)  standardization activities, including JTC 1/SC 41 (IoT architecture and vocabulary,  IoT Interoperability, and IoT Applications), JTC 1/SC 27 (IoT aspects of Security and  Privacy), IETF – SW Updates for IoT, and the International Telecommunications  Union–Telecommunication Standardization Sector (ITU-T): Sector Joint Coordination  Activity on IoT and “IoT and Smart Cities.” NIST has been instrumental in promoting  and participating in the development of a family of voluntary ISO standards that align  with NIST’s cryptographic module validation standard and related specifications.  NIST serves as the project editor for nine of those standards. FIPS 140-3, Security  Requirements for Cryptographic Modules, points to ISO/IEC 19790, Security  Requirements for Cryptographic Modules. Testing for these requirements will be  performed in accordance with ISO/IEC 24759, Test Requirements for Cryptographic  Modules. This is an ongoing effort and will continue over the next several years to  support a smooth transition path to those using FIPS 140-3 specifications. In FY19, NIST staff will continue to lead and participate in cybersecurity and privacy  standardization efforts with an increased focus on cybersecurity, privacy, and  cryptography, as well as on new and emerging areas such as Artificial Intelligence and  the Internet of Things. NIST will continue to provide thoughtful leadership in many SDOs  by actively participating in those organizations and contributing publications and papers. NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 5 IMPERATIVE 2 Enhancing Risk Management Increasingly, NIST’s work is driven by a recognition that risk management should  be a fundamental driver for every organization’s decisions about investments in  cybersecurity—whether those investments take shape as people, processes, or  technology. NIST considers managing risk to be an essential principle and process  for organizations of any size or type to employ in planning and carrying out their  cybersecurity programs and in making decisions about priorities. During FY18, NIST made significant strides in producing cybersecurity risk management  approaches and tools and assisting organizations in their use. Most notably, these  achievements included: •	 Release of the first update of the Framework for Improving Critical  Infrastructure Cybersecurity, popularly known as simply the “Cybersecurity  Framework.” First produced in February 2014 with the active involvement of  hundreds of organizations in the private and public sectors, Version 1.1 was  released in April 2018. Based on the experience of Framework users and changes  in the cybersecurity environment and technological advances, the revision  added more content related to supply chain risk management and coordinated  vulnerability disclosure. NIST added and clarified language regarding cybersecurity  measurement with an emphasis on self-assessment. The number of downloads of  Version 1.1 has far surpassed that of the initial version. Recognizing and seeking to further stimulate international usage of the  Cybersecurity Framework, NIST worked on a Spanish language translation, which  is now available along with translations or adaptations provided by government  agencies and industry leaders in Italian, Japanese, Hebrew, Arabic, and  Portuguese. NIST also engaged more purposefully to promote the alignment and  use of the Framework, especially in Latin America and Europe. In the international  standards arena, NIST vigorously assisted in efforts to include the Framework into  a key International Organization for Standardization (ISO) cybersecurity standard,  a process that is still under way. In addition, NIST included many more resources to aid in using the Cybersecurity  Framework, many of which were generated by private sector, non-profit, and  government organizations. NIST also launched a series of “Success Stories”  highlighting the variety of ways in which organizations of all types and in all  sectors are finding value in the Framework. Also in FY18, NIST recognized that  new information products and tools are regularly being produced well beyond  the initial Informative References included in Version 1.0 and pushed forward with  an initiative that establishes a transparent, equitable process for organizations to  propose new references, which are intended to be included as online Informative  References associated with the Framework. NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 2 – Enhancing Risk Management 6 •	 Update of the NIST Risk Management Framework (RMF). For years, the RMF  has been a mainstay for federal agencies and others to use in assessing and  managing cybersecurity needs and challenges. Two drafts of the new version  were produced in FY18—NIST Special Publication (SP) 800-37, Revision 25— before becoming final in early FY19. Updates included significant new guidance  in addressing privacy risks and integrating security and privacy into the  design of systems. The RMF also references NIST systems security engineering  guidance at appropriate points, including NIST SP 800-160,6 which addresses the  engineering of trustworthy secure systems. The revised RMF also offers additional guidance on: •	 Better preparing an organization’s senior leaders to execute the RMF, as well  as how to communicate their protection plans and risk management strategies  to system implementers and operators. •	 Incorporating supply chain risk management considerations. The RMF now  addresses growing supply chain concerns, such as counterfeit components,  tampering, theft, insertion of malicious software and hardware, poor  manufacturing and development practices, and other potentially harmful  activities that can impact an organization’s systems and systems components. •	 Supporting security and privacy safeguards. The RMF update provides  organizations with a disciplined and structured process to select controls  from the newly developed consolidated security and privacy control catalog  in NIST’s SP 800-53 Revision 5.7 This should be valuable to companies and  organizations beyond the federal government, considering how high profile the  subject of privacy has become as of late. Importantly, the revised RMF clarifies its relationship to the Cybersecurity Framework.  Aligning the RMF with other NIST guidance and publications will help federal agencies  which are required to implement multiple frameworks. While adhering to the CSF is  voluntary for private companies, its use for the federal government is not optional  under Executive Order 13800.8 Use of the RMF is mandatory for federal agencies in  accordance with the Federal Information Security Modernization Act (FISMA9). The RMF  is also required and in widespread use in the Department of Defense and the intelligence  5 Special Publication (SP) 800-37, Rev. 2, Risk Management Framework for Information Systems and Organizations: A  System Life Cycle Approach for Security and Privacy, https://csrc.nist.gov/publications/detail/sp/800-37/rev-2/final  6 SP 800-160, Vol. 2, Developing Cyber Resilient Systems: A Systems Security Engineering Approach, https://csrc. nist.gov/publications/detail/sp/800-160/vol-2/draft  7 SP 800-53, Rev. 5, Security and Privacy Controls for Information Systems and Organizations, https://csrc.nist.gov/ publications/detail/sp/800-53/rev-5/draft  8 Presidential Executive Order on Strengthening the Cybersecurity of Federal Networks and Critical Infrastructure,  https://www.whitehouse.gov/presidential-actions/presidential-executive-order-strengthening-cybersecurity- federal-networks-critical-infrastructure/ 9 Federal Information Security Modernization Act (FISMA), https://www.dhs.gov/fisma  IMPERATIVE 2 – Enhancing Risk Management NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 7 community. That alignment of the Cybersecurity Framework and the RMF is still a work  in progress with NIST committed to producing improved guidance in 2019. •	 Systems Security Engineering: Considerations for a Multidisciplinary Approach  in the Engineering of Trustworthy Secure Systems. The FY18 update of this  publication addresses the engineering-driven perspective and actions necessary to  develop more defensible and survivable systems, including the machine, physical,  and human components that compose the systems as well as the capabilities and  services delivered by those systems. It builds upon well-established international  standards for systems and software and fuses systems security engineering  methods, practices, and techniques. The objective is to address security  issues from the perspective of stakeholder protection needs, concerns, and  requirements using established engineering processes to ensure that those needs  are addressed with appropriate fidelity and rigor early on and in a sustainable  manner throughout the life cycle of the system. NIST also published a draft of the  first in a series of specialty publications developed to support the flagship NIST  Systems Security Engineering guideline. Volume 2 addresses cyber resiliency  considerations for two important yet distinct communities of interest represented  by organizations: those conducting new development of IT component products,  systems, and services; and those with legacy systems (installed base) currently  carrying out day-to-day missions and business functions. •	 Privacy Framework: An Enterprise Risk Management Tool. There is growing  concern regarding privacy issues across the country and throughout the  world. Government organizations are putting into place multiplying privacy  requirements that cross borders in their implementation. That is why in FY18,  NIST laid the groundwork for developing a risk management-driven approach  that could be used by any organization that chooses to do so. The Privacy  Framework project, announced in September 2018 and expected to result in a  final Framework by late 2019, is being carried out based on the same type and  degree of extensive private-public sector collaboration that led to the widely  regarded Cybersecurity Framework. Other risk management-focused NIST cybersecurity accomplishments in FY18 included  an update of NIST’s recommendations for protecting the confidentiality of controlled,  unclassified information (CUI) in non-federal systems and organizations. Safeguarding  that CUI is of paramount importance and can directly affect the ability of federal  agencies to successfully conduct their assigned missions and business operations.  NIST SP 800-171 Revision 110 recommends security requirements to those agencies. The  FY18 update included editorial changes to select CUI security requirements, additional  references and definitions, and an expanded discussion about each CUI requirement. 10 SP 800-171, Rev. 1, Protecting Controlled Unclassified Information in Nonfederal Systems and Organizations,  https://csrc.nist.gov/publications/detail/sp/800-171/rev-1/final  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 8 IMPERATIVE 3 Strengthening Cryptographic Standards and Validation Cryptography—the technological foundation for most cybersecurity functions— is constantly under attack by a multiplying array of adversaries that range from  individual criminals seeking financial gains to terrorist groups and nation states. If the  cryptographic protection for an organization’s information technology is defeated or  bypassed, the organization—and, potentially, our nation’s entire infrastructure system— may be wide open to malicious attack. NIST is responsible for developing U.S. federal cryptographic standards as well as the  technologies and programs used to determine and validate correct implementation of  those standards. This has been a mainstay of NIST’s computer security work for nearly 50  years. In FY18, NIST continued to work with partners in government, the private sector,  and academia in the United States and around the globe to confront a variety of urgent  technical and implementation challenges to cryptographic security. These included: •	 Post-Quantum Cryptography. In recent years, there has been a substantial amount  of research on quantum computers—machines that exploit quantum mechanical  phenomena to solve mathematical problems that are difficult or intractable for  conventional computers. If large-scale quantum computers are ever built, they will  be able to break many of the public key cryptosystems currently in use, seriously  compromising the confidentiality and integrity of digital data. During FY18, NIST  continued to prioritize work on post-quantum cryptography (also called quantum- resistant cryptography) with the ambitious goal of developing cryptographic  systems that are secure against both quantum and classical computers and can  interoperate with existing communications protocols and networks.  After an earlier call to the public to submit post-quantum algorithms11 that  could resist a quantum computer’s onslaught, from FY18 to FY19 NIST worked  with the larger cryptography community to narrow the field from 69 submitted  algorithms to 26. The remaining algorithms are those which NIST mathematicians  and computer scientists consider to be the strongest candidates. Next, NIST is  asking the cryptography community to focus on analyzing how these algorithms  will perform in the real world (e.g., how they will fit into the internet protocols  currently in use). This second round of analysis focuses more heavily on evaluating  the submissions’ performance across a wide variety of systems, not just in big  computers and smartphones.  11 NIST Asks Public to Help Future-Proof Electronic Information, https://www.nist.gov/news-events/news/2016/12/ nist-asks-public-help-future-proof-electronic-information  IMPERATIVE 3 – Strengthening Cryptographic Standards And Validation  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 9 •	 Lightweight Cryptography. Many different devices and systems—including  smart cards, tiny devices for IoT use, and individual microchips—need effective  encryption yet have limited processor power. These devices and systems are  vital in sensor networks, healthcare, distributed control systems, and cyber  physical systems. Because the majority of current cryptographic algorithms were  designed for desktop/server environments, many do not fit into constrained  devices. Quantum-resistant algorithms that can perform this sort of “lightweight  cryptography” are sorely needed. In FY18, NIST invited submissions of candidate  lightweight cryptographic algorithms12 that provide adequate security for  environments in which processing performance is constrained and where the  performance of current NIST cryptographic standards is not acceptable. In FY19,  NIST aims to pare down those candidates for further evaluation and public review.   •	 Responding to and Anticipating Other Technology Developments. Recent  developments in cryptographic technologies, including the use of blockchain  techniques, have driven the need for updated cryptographic key establishment  and implementation guidance. In FY18, NIST released multiple documents  designed to fulfill those needs. Significant accomplishments included: •	 Release of random number generation guidelines13 and an online public  source14 of random numbers •	 Updated recommendations for best practices for key management  organization •	 Additional guidance on transitioning15 to more effective cryptography •	 Advanced automated testing and validation 16 of the correctness of algorithms  (see Text Box #3), including introduction of a new protocol to the international  standards development process In FY19, NIST will begin revising additional guidance, convene17 a stakeholder workshop  on threshold cryptography, complete development18 of a hardware chip that implements  improved techniques, and continue to work with external collaborators on developing  consensus-based blockchain standards. 12 Lightweight Cryptography Project, https://csrc.nist.gov/projects/lightweight-cryptography  13 SP 800-90B, Recommendation for the Entropy Sources Used for Random Bit Generation, https://csrc.nist.gov/ news/2018/nist-announces-the-release-of-sp-800-90b  14 NIST Randomness Beacon, Version 2.0 Beta, https://beacon.nist.gov/home  15 SP 800-131A, Rev. 2, Transitioning the Use of Cryptographic Algorithms and Key Lengths, https://csrc.nist.gov/ publications/detail/sp/800-131a/rev-2/archive/2018-07-19 16 Automated Cryptographic Validation (ACV) Testing, https://csrc.nist.gov/publications/detail/itl-bulletin/2018/09/ automated-cryptographic-validation-testing/final 17 Threshold Cryptography, https://csrc.nist.gov/projects/threshold-cryptography 18 Circuit Complexity, https://csrc.nist.gov/projects/circuit-complexity NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 3 – Strengthening Cryptographic Standards And Validation  10 CRYPTO STANDARD = HUGE $$ IMPACT In FY18, NIST released a study19 that estimated a $250 billion economic impact  from the development of its Advanced Encryption Standard (AES) from 1996- 2017. AES is a cryptographic algorithm20 to encrypt and decrypt electronic  information. It was approved for use by the Federal Government in November  2001 and has since been widely adopted by private industry. Today, AES protects  everything from classified data and bank transactions to online shopping and  social media apps. According to the study, NIST’s investment in AES has been  repaid many times over—the study’s most conservative estimate shows a 29 to 1  benefit-to-cost ratio for the AES program. The estimated benefit-to-cost ratio for  the whole economy was 1,976 to 1. The report relied on a survey of government  and private sector consumers of encryption systems and private integrators who  develop and produce encryption hardware or software. In 1997, NIST launched its effort to identify a new standard encryption algorithm  for the Federal Government after recognizing that the Data Encryption Standard  (DES), adopted 20 years earlier, was growing vulnerable in the face of advances  in cryptanalysis and the exponential growth in computing power. In October  2000, following a three year, open international competition, NIST announced  its proposal for the replacement standard: an algorithm submitted by two  cryptographers from Belgium. The unclassified, publicly disclosed encryption  algorithm is available royalty-free and is used by the U.S. Federal Government  in its Federal Information Processing Standard (FIPS) and voluntarily by private  organizations worldwide. 19 The Economic Impact of the Advanced Encryption Standard, 1996-2017, https://csrc.nist.gov/publications/detail/ white-paper/2018/09/07/economic-impacts-of-the-advanced-encryption-standard-1996-2017/final 20 FIPS 197, https://csrc.nist.gov/publications/detail/fips/197/final IMPERATIVE 3 – Strengthening Cryptographic Standards And Validation  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 11 AUTOMATING CRYPTOGRAPHIC TESTING Several years ago, NIST launched a project to automate much of the testing  required under its cryptographic validation programs. Automated cryptographic  algorithm testing was completed in 2018, and NIST is now developing methods to  automate the testing of cryptographic modules. These efforts in automation are  intended to provide a higher trust in the assurance claims made by the product  developers in an efficient and cost-effective manner that allows the vendors’  conformance efforts to keep pace with the changing IT landscape. By investing  in a more robust testing infrastructure, NIST anticipates that product vendors  will take advantage of this service by validating their products more often, which  will produce more secure products. In FY19, NIST will put more responsibility  for generating evidence of conformance in the hands of industry by leveraging  automated test processes that will reduce time to market, slice costs to maintain  compliance, and ensure that the Federal Government has effective and up-to- date technologies. NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 12 IMPERATIVE 4 Advancing Cybersecurity Research and Applications Development Beyond cryptography-related priorities, NIST’s cybersecurity research and applications  development activities include identifying emerging and high-priority technologies,  developing security solutions that will have a high impact on the U.S. critical information  infrastructure, and developing and showing how to manage foundational building-block  security mechanisms and techniques that can be integrated into organizations’ mission- critical information systems. Accomplishments included: •	 Cloud Computing Security and Forensics. During FY18, NIST continued to develop  publications, promote national and international standards and specifications,  and demonstrate how to meet those standards and specifications to support the  effective and secure use of cloud computing.21 NIST collaborated with industry22  to implement the Cloud Security Architecture Tool (CSAT),23 leveraging the  NIST Cybersecurity Framework to architect a cloud-based information system  and identify necessary security controls. Aiding transition to practice, NIST  collaborated with industry to initiate a cloud security project to show one way  to increase security and privacy for cloud workloads on hybrid cloud platforms.  Based on public comments, NIST also updated a guide that shows how to reduce  opportunities for bad actors to attack enterprises via the cellular and WiFi  connections on mobile devices; final publication is planned for FY19.24 In addition,  the Cloud Forensic Science Working Group continued to define a cloud forensics  reference architecture. Continuation of a trusted cloud capability demonstration,  CSAT, and forensics activities are planned for FY19. •	 Low Power Wide Area Networks. Diverse applications of Internet of Things  and Cyber-physical Systems (IoT/CPS) require various network technologies  be optimized for specific use cases. A low-power wide-area network (LPWAN)  is a set of telecommunication specifications and protocols that satisfy both  low power consumption (i.e., network of inexpensive sensors) and wide area  coverage (i.e., smart city deployments). In order to build an understanding  of the security and trust issues of LPWAN-based telecommunications, NIST  prototyped two use cases for LPWAN-based sensor networks in FY18. The use  case prototypes included a remote access adapter for a NIST climate monitoring  station and a real-time tracking system for the NIST Campus Shuttle that was  21 NIST Cloud Computing Program (NCCP), https://www.nist.gov/programs-projects/nist-cloud-computing- program-nccp 22 SBIR Success Story: InfoBeyond Technology LLC, https://www.nist.gov/tpo/sbir-success-story-infobeyond- technology-llc 23 NIST Cloud Security Architecture Tool (CSAT), https://www.fbcinc.com/e/fitsc/presentations/Iorga-fitsc-csat_ with_rmfoscal.pdf 24 SP 1800-4, Mobile Device Security: Cloud and Hybrid Builds, https://www.nccoe.nist.gov/projects/building- blocks/mobile-device-security/cloud-hybrid IMPERATIVE 4 – Advancing Cybersecurity Research And Applications Development  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 13 able to receive sensor data from devices located 5 miles away from the receptor.  Potential security weaknesses and possible mitigations arising out of this  experience with LPWAN technology are now being investigated. Researchers  have learned that WiFi and Bluetooth do not naturally apply to some IoT/CPS  deployment scenarios, particularly when deployment is outside, requires battery  operation for extended periods, or must operate over a long range. In FY18, the  team also deployed an LPWAN Infrastructure that was integrated with NIST’s  internal network, NIST-Net, satisfying all security policies required by NIST. In  FY19, the NIST Engineering Laboratory (EL), NIST/ITL, and National Institute of  Metrology, Standardization and Industrial Quality (INMETRO) team will study  the implementation of LPWAN on the Smart Grid and investigate LPWAN in the  context of Smart Grid cybersecurity guidelines. •	 Open Security Controls Assessment Language (OSCAL). Today, concepts like  security controls and profiles are largely represented in proprietary ways, making  it more difficult for many organizations to move forward as quickly as they need  to in order to take advantage of these approaches. Organizations also often  struggle with information systems that have many different components. To help  address these problems, NIST is developing OSCAL—a standard for representing  different categories of information about the publication, implementation, and  assessment of security controls. In FY18, NIST completed a control catalog  and profile schemas and began developing an implementation schema for  representing system security plans (SSP) in OSCAL. The team validated the  approach with several use cases. In FY19, NIST will continue to develop other  approaches involving the Cybersecurity Framework as well as assessments and  assessment results. •	 Combating Ransomware. NIST has placed a high priority on identifying and  demonstrating tools for identifying, protecting against, detecting, responding  to, and recovering from ransomware attacks and other events that are  destructive to systems and operations. In FY18, NIST released Data Integrity:  Recovering from Ransomware and Other Destructive Events (SP 1800-11)25 for  public comment and initiated two other ransomware projects, Data Integrity:  Identifying and Protecting Against Ransomware and Other Destructive  Events26 and Detecting and Responding to Ransomware and Other Destructive  Events.27 Finalization of SP 1800-11 and development of practice guides for  the identification and protection and the detection and response projects are  planned for FY 2019. 25 SP 1800-11, Data Integrity: Recovering from Ransomware and Other Destructive Events, https://www.nccoe.nist. gov/projects/building-blocks/data-integrity/recover 26 Data Integrity: Identifying and Protecting Assets Against Ransomware and Other Destructive Events, https:// www.nccoe.nist.gov/projects/building-blocks/data-integrity/identify-protect 27 Data Integrity: Detecting and Responding to Ransomware and Other Destructive Events, https://www.nccoe.nist. gov/projects/building-blocks/data-integrity/detect-respond NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 4 – Advancing Cybersecurity Research And Applications Development  14 •	 Roots of Trust. Modern computing devices consist of varied hardware, firmware,  and software components at multiple layers of abstraction. Many security and  protection mechanisms are rooted in software that, along with all underlying  components, must be trusted and not tampered with. A vulnerability in any  of those components could compromise the trustworthiness of the security  mechanisms that rely upon them. Achieving stronger security assurances may  be possible by grounding security mechanisms in highly reliable and secure  hardware, firmware, and software components that perform specific, critical  security functions. In FY18, NIST built on earlier work focused on protecting  boot firmware to develop new technical guidelines and recommendations for  supporting the resiliency of platform firmware and data against potentially  destructive attacks. Platform Resiliency Guidelines (SP 800-193)28 promotes  resiliency in platforms by describing security mechanisms for protecting against  unauthorized changes, detecting unauthorized changes that occur, and securely  recovering from attacks. In FY19, NIST plans to continue outreach to stakeholders  to encourage the development of more secure and reliable systems and  investigate how roots of trust can support other security needs, including supply  chain assurance. 28 SP 800-193, Platform Firmware Resiliency Guidelines, https://csrc.nist.gov/publications/detail/sp/800-193/final NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 15 IMPERATIVE 5 Improving Cybersecurity Awareness, Training, Education,  and Workforce Development People are often the most underappreciated ingredient in the people, process, and  technology formula that determines an organization’s readiness to understand and deal  with cybersecurity challenges. This includes gaps in users’ and providers’ awareness  about how to access cybersecurity guidelines and tools that apply to their own  operations and environments along with a shortage of people who have the needed  cybersecurity education, training, and experience. In FY18, NIST placed a greater emphasis on improving the public availability of information  resources by providing them to small businesses in the form of briefings, advancing  awareness and understanding of human factors contributing to cybersecurity challenges,  and recommendations for steps to address workforce shortage and training issues. FY18 web-accessible information resources included: the Computer Security Resource  Center (CSRC),29 National Software Reference Library (NSRL),30 Security Automation  Reference Data and the National Vulnerability Database (NVD),31 National Checklist  Program (NCP) repository,32 Software Assurance and Quality Software Assurance  Reference Dataset (SARD),33 and Computer Forensics Tool Testing Project tool  catalog34 and reference data sets.35 Ongoing federal outreach efforts include the  Software and Supply Chain Assurance (SSCA) Forum,36 the Federal Computer Security  Managers’ (FCSM) Forum,37 and the Federal Information Systems Security Educators’  Association (FISSEA).38 •	 Cybersecurity for Small Businesses. Small and medium-sized businesses (SMBs)  represent approximately 95 % of all businesses and are often considered to be the  backbone of the U.S. economy. Typically faced with limited budgets, SMBs need  practical resources that enable them to understand and cost-effectively address  their cybersecurity risks. NIST has been working on behalf of SMBs for many years,  29 Computer Security Resource Center (CSRC), https://csrc.nist.gov 30 National Software Reference Library (NSRL), https://www.nist.gov/software-quality-group/national-software- reference-library-nsrl 31 National Vulnerability Database (NVD), https://nvd.nist.gov/ 32 National Checklist Program (NCP) Repository, https://nvd.nist.gov/ncp/repository 33 NIST Software Assurance Reference Dataset (SARD) Project, https://samate.nist.gov/sard/ 34 Computer Forensics Tools and Techniques Catalog, https://toolcatalog.nist.gov/ 35 Computer Forensic Reference Data Sets (CFReDS), https://www.cfreds.nist.gov/ 36 Software and Supply Chain Assurance (SSCA) Forum, https://csrc.nist.gov/projects/cyber-supply-chain-risk- management/ssca 37 Federal Computer Security Managers’ Forum, https://csrc.nist.gov/events/2018/federal-computer-security- managers-forum-2-day 38 Federal Information Security Educators (FISSEA), https://csrc.nist.gov/projects/fissea NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 5 – Improving Cybersecurity Awareness, Training, Education, and Workforce Development  16 together with interagency and industry partners and collaborators. The NIST  Small Business Cybersecurity Act, which became law on August 14, 2018, codified  the Institute’s focus on small businesses. Specifically, the statute directed NIST to  “disseminate clear and concise resources to help small business concerns identify,  assess, manage, and reduce their cybersecurity risks.” During FY18, the NIST Small  Business Outreach Program began updating the Small Business Cybersecurity  Corner website to make resources easier to find and use. In FY19, those training  materials and accompanying resources will be expanded based on contributed  cybersecurity resources and feedback received from federal partners and the public. •	 National Initiative for Cybersecurity Education (NICE). Hosted by NIST, NICE  seeks to help equip, promote, and energize a robust network of organizations  that address cybersecurity education, training, and workforce development.  Efforts to achieve this goal include: 1) accelerating learning and skills development,  2) nurturing a diverse learning community, and 3) guiding career development and  workforce planning to achieve each of the objectives identified in the NICE  Cybersecurity Workforce Framework.39 Figure 2: The Seven Categories of the NICE Framework In FY18, fulfilling a directive in Executive Order 13800,40 NICE joined with the  Department of Homeland Security to prepare a report to the President41 that  made a series of recommendations. In their transmittal to the President, the  Commerce and Homeland Security Secretaries noted that “in both the private  and public sectors, cybersecurity practitioners and educators are vital to our  national security—especially since other nations are paying greater attention  to their cybersecurity workforce needs and the cybersecurity weaknesses of  their adversaries.” The report was based on an analysis of available data and  the information and views shared by businesses, educational organizations,  training and certification providers, government agencies at multiple levels,  and individuals. Findings and specific, forward-thinking, and actionable  39 NICE Cybersecurity Workforce Framework, https://www.nist.gov/itl/applied-cybersecurity/nice/resources/nice- cybersecurity-workforce-framework 40 Executive Order (EO) 13800, Strengthening the Cybersecurity of Federal Networks and Critical Infrastructure,  https://www.whitehouse.gov/presidential-actions/presidential-executive-order-strengthening-cybersecurity- federal-networks-critical-infrastructure/ 41 A Report to the President: Supporting the Growth and Sustainment of the Nation’s Cybersecurity Workforce:  Building the Foundation for a More Secure American Future, https://csrc.nist.gov/publications/detail/white- paper/2018/05/30/supporting-growth-and-sustainment-of-the-cybersecurity-workforce/final IMPERATIVE 5 – Improving Cybersecurity Awareness, Training, Education, and Workforce Development  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 17 recommendations addressed both public and private sector needs (see Text Box  #4). In FY19, the President issued another executive order42 directing agencies to  implement those recommendations CYBERSECURITY WORKFORCE RECOMMENDATIONS  TO THE PRESIDENT •	 The Nation should set an ambitious vision and action plan-of-attack to  “prepare, grow, and sustain a national cybersecurity workforce that safeguards  and promotes America’s national security and economic prosperity.” •	 The Federal Government should lead in launching a high-profile national  Call to Action to draw attention to and mobilize public and private sector  resources to address cybersecurity workforce needs. •	 The Administration should focus on and recommend long-term authorization  and sufficient appropriations for high-quality, effective cybersecurity  education and workforce development programs in its budget proposals in  order to grow and sustain the cybersecurity workforce. •	 Federal departments and agencies must move quickly to address major needs  relating to recruiting, developing, and retaining cybersecurity employees and  continue to implement the Federal Cybersecurity Workforce Strategy43 and  the Federal Cybersecurity Workforce Assessment Act of 2015 (FCWAA).44 •	 The private and public sectors need to transform, elevate, and sustain the  learning environment to grow a dynamic and diverse cybersecurity workforce.  •	 The private and public sectors need to align education and training with  employers’ cybersecurity workforce needs, improve coordination, and prepare  individuals for lifelong careers. •	 The private and public sectors need to establish and leverage measures  that demonstrate the effectiveness and impact of cybersecurity  workforce investments. In FY18, the NICE program worked to enhance CyberSeek.45 This tool was  developed in partnership with industry to help employers, job seekers, policy  makers, training providers and guidance counselors find information on the supply  42 Executive Order on America’s Cybersecurity Workforce, https://www.whitehouse.gov/presidential-actions/ executive-order-americas-cybersecurity-workforce/ 43 Federal Cybersecurity Workforce Strategy, https://chcoc.gov/content/federal-cybersecurity-workforce-strategy 44 Federal Cybersecurity Workforce Assessment Act, https://www.congress.gov/bill/114th-congress/senate-bill/2007 45 CyberSeek, https://www.nist.gov/itl/applied-cybersecurity/nice/cyberseek NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 5 – Improving Cybersecurity Awareness, Training, Education, and Workforce Development  18 of workers with relevant credentials and to show career pathways in cybersecurity  that map opportunities for advancement in the field. •	 Usability and Cybersecurity. Too often, sound cybersecurity components and  practices are not employed because of user resistance. In FY18, NIST advanced  research that examined usability attributes and reasons for user resistance.  The topics addressed included phishing, password policies, usable privacy,  cryptographic development, and youth password perceptions and behaviors. FY18  marked the beginning of a multi-year research effort to examine youth security  practices, behaviors, and perceptions with the goal of developing useful guidance  to help youth learn, understand, and ultimately practice good cybersecurity  behaviors. In FY18, a paper documenting the results of the first school district  surveyed (Ohio)—which included students in grades 3 to 8—was completed.  Further analysis of all five school systems’ survey data is being conducted in  FY19 and will include descriptive statistics and a follow-up survey that examines  parents’ password practices and their involvement (or lack of involvement) with  the password usage of their grade school children. RESULTS OF ONLINE SURVEYS  OF CRYPTOGRAPHIC DEVELOPERS During FY18, the Usability and Cybersecurity team published a paper reporting the  results of online surveys of organizations involved in cryptographic development.  Findings revealed that organizations used cryptography for a wide range of  purposes. Most relied on generally accepted, standards-based implementations  as guides while others developed their own implementations by drawing from  non-standards resources. The results highlighted the challenges for organizations  that undertake cryptographic development, including difficulties in recruiting  and managing talent, disruptions to the product lifecycle, and trouble explaining  the security value of products to customers. Interviews of representatives  from organizations that include cryptography in their products suggested a  strong security mindset demonstrated by robust organizational security culture  and the deep expertise of those performing cryptographic development. The  results encourage additional research initiatives to explore variations in those  implementing cryptography, which can aid in transferring lessons learned from  more security-mature organizations to the broader development community  through educational opportunities, tools, and other mechanisms. NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 19 IMPERATIVE 6 Enhancing Identity and Access Management Properly managing access to IT systems, processes, and information is central to managing  cybersecurity risks and a priority for NIST’s cybersecurity program. NIST engages and  collaborates with standards bodies and consortia such as the International Organization  for Standardization (ISO),46 the Internet Engineering Task Force (IETF),47 the Fast IDentity  Online (FIDO) Alliance,48 the Open Identity Federation (OIF), and the Kantara Initiative.49 MOVING AWAY FROM PASSWORDS In response to a widely recognized need, NIST continued to advance the  challenge of moving away from passwords and plans to publish a key report in  2019 with its latest research findings and recommendations. In 2018, NIST: •	 Continued to investigate Authentication for Law Enforcement Vehicle  Systems50 project requirements and •	 Revised high visibility publications, including: the Attribute Based Access Control  (SP 1800-3)51 guide that shows how to manage access to networked resources  more securely and efficiently; the Mobile Application Single Sign-on: Improving  Authentication for Public Safety and First Responders (SP 1800-13)52 guide  that shows how single sign-on can be used efficiently and securely on mobile  devices to access restricted information; and the Multifactor Authentication for  E-Commerce (SP 1800-17)53 guide that shows how organizations can efficiently  implement multi-factor authentication in a user-friendly manner to protect  customers’ information from being used for fraudulent purchases. In FY19, NIST intends to finalize SP 1800-3, SP 1800-13, and SP 1800-17 based on  comments from industry and the public. 46 International Organization for Standardization (ISO), https://www.iso.org/home.html 47 Internet Engineering Task Force (IETF), https://www.ietf.org/about/ 48 Fast IDentity Online Alliance (FIDO), https://fidoalliance.org/ 49 Kantara Initiative, https://kantarainitiative.org/ 50 Authentication for Law Enforcement Vehicle Systems, https://www.nccoe.nist.gov/projects/project-concepts/ authentication-law-enforcement-vehicle-systems 51 SP 1800-3, Attribute Based Access Control, https://www.nccoe.nist.gov/projects/building-blocks/attribute-based- access-control 52 SP 1800-13, Mobile Application Single Sign-On, https://www.nccoe.nist.gov/projects/use-cases/mobile-sso 53 SP 1800-17, Multifactor Authentication for E-Commerce, https://www.nccoe.nist.gov/projects/use-cases/ multifactor-authentication-ecommerce NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 6 – Enhancing Identity and Access Management   20 •	 Personal Identity Verification (PIV).54 As charged by Homeland Security  Presidential Directive-12, NIST developed and maintains the Federal  Information Processing Standard (FIPS) for personal identity verification (PIV)  of federal employees and contractors (FIPS 201).55 In FY18, the agency revised  its guideline for the use of PIV credentials for physical access (SP 800-116)56  and sought comments on a draft report demonstrating how commercial  technologies can be used to issue and use PIV credentials on mobile devices.  NIST also requested public comments SP 1800-12,57 a guide that shows how  to leverage identity-proofing and vetting results of current and valid PIV  credentials to enable two-factor authentication to information technology  systems via mobile devices. In FY19, NIST plans to revise FIPS 201 to reflect  technology changes and evolving practices regarding authentication using  PIV credentials. •	 Policy Machine – Next Generation Access Control. NIST is developing  an Attribute-Based Access Control (ABAC) framework called the “Policy  Machine,” which is designed to align with a Next Generation Access Control  emerging standard from the American National Standards Institute/ International Committee for Information Technology Standards (ANSI/ INCITS). In FY18, NIST updated its Policy Machine Web Services through  GitHub as an open-source distribution to support widespread experimentation  of web-based applications. •	 Access Control and Privilege Management. NIST continued to provide  improved guidance for dealing with access control and privilege management  issues. This included publishing an Attribute-Based Access Control textbook;  researching a general access control mechanism for cloud computing services;  enhancing conformance verification for access control policies; revising SP  1800-958—a guide that shows how to manage disparate identity and access  mechanisms and systems as a comprehensive identity and access management  system; and developing SP 1800-1859—a guide that shows a way to detect  and protect against IT user accounts that have administrative or “super user”  privileges being misused. In FY19, NIST plans to finalize SP 1800-9 and continue  its work with private sector communities of interest to align its guidance and  capability demonstrations with the needs of industry and private citizens. 54 Personal Identity Verification of Federal Employees and Contractors, https://csrc.nist.gov/projects/piv 55 FIPS 201-2, https://csrc.nist.gov/publications/detail/fips/201/2/final 56 SP 800-116, Rev. 1, Guidelines for the Use of PIV Credentials in Facility Access, https://csrc.nist.gov/publications/ detail/sp/800-116/rev-1/final 57 SP 1800-12, Derived PIV Credentials Practice Guide, https://www.nccoe.nist.gov/library/derived-piv-credentials- nist-sp-1800-12-practice-guide 58 SP 1800-9, Access Rights Management for Financial Services Sector, https://www.nccoe.nist.gov/projects/use- cases/access-rights-management 59 SP 1800-18, Privileged Account Management for Financial Services Sector, https://www.nccoe.nist.gov/projects/ use-cases/privileged-account-management NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 21 IMPERATIVE 7 Bolstering Infrastructure Protection NIST is focusing on three major national critical infrastructure programs where effective  cybersecurity is a vital element: the internet infrastructure, Energy Infrastructure  Cybersecurity, and cybersecurity aspects of electronic voting. •	 Internet Infrastructure Protection (IIP). NIST works with industry to develop the  measurement science, new standards, and standards implementation capabilities  necessary to ensure the resilience and security of the global Internet. In FY18,  research focused on developing measurement and modeling techniques necessary  to understand, predict, and control the behavior of internet-scale networked  information systems, especially foundational routing and communications  protocols. This includes the internet’s Domain Name System (DNS), Border  Gateway Protocol (BGP), and electronic mail and messaging infrastructures. In  addition, NIST has been giving special attention to systemic vulnerabilities in  core internet technologies such as those that enable distributed denial-of-service  (DDoS) attacks on a massive scale. In FY18, multiple NIST publications contributed to showing how to remedy serious  security and robustness vulnerabilities in network infrastructures. Cybersecurity  practice guides were developed with step-by-step example solutions using  commercially available technologies. These included SP 1800-6,60 which shows  how to combat spear phishing by improving assurance of the correctness of email  sources, destinations, and cryptographic protection of email, and SP 1800-14,61  which explains how service providers can better control routing of internet traffic.  NIST also initiated a network infrastructure project to illustrate how to better  manage cryptographic certificates in order to reduce system outage and security  breach risks (SP 1800-16).62 In addition to completing ongoing network security  activities in FY19, NIST plans to collaborate with government and private industry  on a major feasibility demonstration effort for transitioning systems and services  from IPv4 to improved IPv6 information transfer protocols. 60 SP 1800-6, Domain Name System-Based Electronic Mail Security, https://www.nccoe.nist.gov/projects/building- blocks/secured-email 61 SP 1800-14, Protecting the Integrity of Internet Routing Practice Guide, https://www.nccoe.nist.gov/projects/ building-blocks/secure-inter-domain-routing 62 SP 1800-16, Securing Web Transactions Practice Guide, https://www.nccoe.nist.gov/projects/building-blocks/tls- server-certificate-management NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 7 – Bolstering Infrastructure Protection  22 DOMAIN NAME SYSTEM-BASED ELECTRONIC MAIL SECURITY NIST’s Special Publication 1800-6, Domain Name System-Based Electronic  Mail Security, was published in January 2018. This publication documents the  use of products that improve security and integrity for electronic mail users by  employing the DNS-based Authentication of Named Entities (DANE) protocol  to authenticate domain addresses (e.g., @nist.gov) for electronic mail and  STARTTLS protocol, which provides a way to take an existing insecure connection  and upgrade it to a secure connection using secure transmission protocols. Since  its publication, there has been a sharp increase in the number of internet domains  that deploy DANE (608 % from February 2018 to February 2019). Moreover, the  Department of Homeland Security’s Binding Operational Directive 18-01 now  provides compulsory direction to federal executive branch departments and  agencies to employ email authentication and specifies that all internet-facing  email servers offer the STARTTLS protocol. •	 Energy Infrastructure Cybersecurity. Under the Energy Independence and  Security Act (EISA),63 NIST has a leading role in collaborating with the private  sector to coordinate and accelerate smart grid64 interoperability and security  standards. Multiple NIST laboratories are involved in this effort to advance  measurement science, leading to better utilization of assets, improved grid  reliability, and greater use of renewable energy sources in the grid. This is  accomplished through a combination of research, standardization, testing and  implementation of the NIST Smart Grid Interoperability Framework.65 In FY18, NIST: •	 Conducted a grid edge experiment to understand the performance impact of  cybersecurity capabilities on resource-constrained components of the grid; •	 Published SP 1800-2,66 a guide that shows how organizations can more  securely and effectively manage access to networked operational technology  (OT) devices and systems on which their operations depend; and •	 Revised SP 1800-7,67 Smart Grid Cybersecurity Committee, a guide that  explains how energy companies can capture, transmit, analyze, and store real- time or near real-time data for their systems to detect, analyze, and remediate  anomalous conditions. 63 Energy Independence and Security Act of 2007, https://www.congress.gov/bill/110th-congress/house-bill/6 64 SmartGrid, https://www.smartgrid.gov/ 65 Smart Grid Framework, https://www.nist.gov/engineering-laboratory/smart-grid/smart-grid-framework 66 SP 1800-2, Identity and Access Management (IdAM), https://www.nccoe.nist.gov/projects/use-cases/idam 67 SP 1800-7, Situational Awareness for Electric Utilities, https://www.nccoe.nist.gov/projects/use-cases/ situational-awareness  IMPERATIVE 7 – Bolstering Infrastructure Protection  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 23 NIST also began to incorporate cybersecurity risk management into the next  version of the Smart Grid Interoperability Framework. In FY19, NIST will continue  to develop the next version of the Smart Grid Interoperability Framework 2.0,  chair the Smart Electric Power Alliance (SEPA)68 Smart Grid Cybersecurity  Committee, support the Department of Energy’s Cyber Resilient Energy Delivery  Consortium (CREDC) program,69 finalize SP 1800-7,70 and undertake capability  demonstration and documentation activities for an Energy Sector Asset  Management71 project.  SECURITY ASPECTS OF ELECTRONIC VOTING The Help America Vote Act (HAVA)72 encouraged upgrading voting equipment  across the United States and established the Election Assistance Commission  (EAC)73 and the Technical Guidelines Development Committee (TGDC).74 NIST  chairs the TGDC and provides technical support related to human factors,  security, and laboratory accreditation. Most significantly, in FY18, the TGDC  adopted a new version of the Voluntary Voting Systems Guidelines (VVSG)75 and  principles developed by NIST and the EAC. In FY19, NIST will continue leading the  public working groups to inform the development of voting system requirements  and test strategies based on the principles and guidelines. Additionally, NIST will  support the development of a Cybersecurity Framework Profile to help better  manage cybersecurity risk in the election infrastructure. 68 Smart Electric Power Alliance (SEPA), https://sepapower.org/ 69 Cyber Resilient Energy Delivery Consortium (CREDC), https://cred-c.org/ 70 SP 1800-7, Situational Awareness for Electric Utilities, https://www.nccoe.nist.gov/projects/use-cases/ situational-awareness 71 SP 1800-23, Energy Sector Asset Management Practice Guide, https://www.nccoe.nist.gov/projects/use-cases/ energy-sector/asset-management 72 Help America Vote Act of 2002 (HAVA), https://www.law.cornell.edu/wex/hava 73 U.S. Election Assistance Commission, https://www.eac.gov/ 74 Technical Guidelines Development Committee, https://www.eac.gov/about/technical-guidelines- development-committee/ 75 Voluntary Voting System Guidelines, https://www.eac.gov/voting-equipment/voluntary-voting-system-guidelines/ NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 24 IMPERATIVE 8 Securing Emerging Technologies Information technology is always rapidly evolving. The extraordinary growth of smart,  connected technologies is a particularly promising and concerning arena since any  small, connected element is a potential point of vulnerability to the local system and all  connected systems. Recent attacks on major network providers have included sustained  cyber-attacks launched through fringe (but connected) parts of the system. These  have included WiFi controllers, closed circuit television (CCTV) cameras, printers, and  baby monitors. NIST focused significant attention in FY18 on emerging network security  topics, such as cybersecurity for the Internet of Things (IoT), low-power wide-area  networks, public safety broadband networks, fog computing, and quantum computing.  In FY19, more effort will shift to cybersecurity issues associated with the potential for 5G  networks on security requirements of new use cases and new network architectures as  well as the impact of artificial intelligence and machine learning on cyber defenses. •	 Cybersecurity for IoT. In FY18, to help federal agencies and other organizations  better understand and manage the cybersecurity and privacy risks associated with  their IoT devices, NIST published NISTIR 822876 for review and comment by  industry and the public. Also in FY18, NIST initiated an Identity and Access  Management for Smart Home Devices77 project and a feasibility demonstration  project mapped to NISTIR 8228 that shows how to use existing security protocols  to reduce opportunities for malicious access to IoT devices from the internet. That  set the stage for publishing the resulting practice guide, SP 1800-15,78 in FY19 for  review and comment by industry and the public. The guide is being expanded to  include improved support for small business environments, control of access to  and by IoT devices that do not yet support the Manufacturer Usage Description  (MUD) protocol, and access to and utilization of threat signaling capabilities. NIST  also co-chairs the IoT Task Group of the Interagency International Cybersecurity  Standardization Working Group (IICS WG) with the Department of Homeland  Security (DHS). In addition, NIST is actively engaging public and private sector  stakeholders to better understand the IoT threat landscape and challenges (see  the draft of NISTIR 8200).79 In FY19, after considering public comments, NIST plans  to publish final versions of those documents and continue collaborating with  stakeholders in developing guidance for IoT security and privacy.  76 NISTIR 8228, Considerations for Managing Internet of Things (IoT) Cybersecurity and Privacy Risks, https://csrc. nist.gov/publications/detail/nistir/8228/final 77 Identity and Access Management for Smart Home Devices, https://www.nccoe.nist.gov/projects/project- concepts/idam-smart-home-devices 78 SP 1800-15, Securing Small Business and Home Internet of Things (IoT) Devices: Mitigating Network-Based  Attacks Using Manufacturer Usage Description (MUD), https://www.nccoe.nist.gov/projects/building-blocks/ mitigating-iot-based-ddos 79 NISTIR 8200, Interagency Report on Status of International Cybersecurity Standardization for the Internet of  Things (IoT), https://csrc.nist.gov/publications/detail/nistir/8200/archive/2018-02-14 IMPERATIVE 8 – Securing Emerging Technologies  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 25 Figure 3: IoT for the GSA Smart Building, from NISTIR 8200 NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 8 – Securing Emerging Technologies  26 BOTNET REPORT A Report to the President on Enhancing the Resilience of the Internet and  Communications Ecosystem Against Botnets and Other Automated, Distributed  Threats,80 released on May 30, 2018, outlines government and the private sector  actions that would reduce the threat of botnets and similar cyberattacks. It  responds to the May 11, 2017, Executive Order on Strengthening the Cybersecurity  of Federal Networks and Critical Infrastructure.81 That order directed the  Secretaries of Commerce and Homeland Security to lead “an open and  transparent process to identify and promote action by appropriate stakeholders”  with the goal of “dramatically reducing threats perpetrated by automated and  distributed attacks (e.g., botnets).” In the course of the year-long effort launched  by the Executive Order, the Departments determined that the opportunities and  challenges of working toward dramatically reducing threats from automated,  distributed attacks can be summarized in six principal themes: 1.	 Automated, distributed attacks are a global problem. 2.	 Effective tools exist but are not widely used. 3.	 Products should be secured during all stages of the lifecycle. 4.	 Awareness and education are needed. 5.	 Market incentives should be more effectively aligned. 6.	 Automated, distributed attacks are an ecosystem-wide challenge Five complementary and mutually supportive goals were identified that, if  realized, would dramatically reduce the threat of automated, distributed attacks  and improve the resilience and redundancy of the ecosystem: Goal 1: Identify a clear pathway toward an adaptable, sustainable, and secure  technology marketplace. Goal 2: Promote innovation in the infrastructure for dynamic adaptation to  evolving threats. Goal 3: Promote innovation at the edge of the network to prevent, detect, and  mitigate automated, distributed attacks. Goal 4: Promote and support coalitions between the security, infrastructure,  and operational technology communities domestically and around the world. Goal 5: Increase awareness and education across the ecosystem. 80 A Report to the President on Enhancing the Resilience of the Internet and Communications Ecosystem  Against Botnets and Other Automated, Distributed Threats, https://csrc.nist.gov/publications/detail/white- paper/2018/05/30/enhancing-resilience-against-botnets--report-to-the-president/final 81 Executive Order (EO) 13800, Strengthening the Cybersecurity of Federal Networks and Critical Infrastructure,  https://csrc.nist.gov/topics/laws-and-regulations/executive-documents/eo-13800 IMPERATIVE 8 – Securing Emerging Technologies  NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 27 •	 National Public Safety Broadband Network (NPSBN). Federal statute directs  NIST to conduct research and development that supports the acceleration and  advancement of a nationwide broadband network that will help police, firefighters,  emergency medical service professionals and other public safety officials stay safe  and do their jobs. In FY18, supported by the joint National Telecommunications  and Information Administration (NTIA) and NIST Public Safety Communications  Research (PSCR)82 program, NIST expanded its support to provision mobile  application vetting tools for public safety mobile application security and  continued its research into identity management, data and application isolation  technologies, wearable devices, and broadband standards. In FY19, NIST will  continue to strengthen its relationship with public safety and commercial  telecommunications stakeholders. Work concerning mobile application vetting and  cybersecurity will continue to evolve as NIST refines methods for evaluating tools  as well as the test cases used in those evaluations. The PSCR program also plans  fund grants and prize challenges to solve current problems and fill future gaps in  public safety broadband technology. •	 Fog Computing. New concepts and technologies are needed to manage  a growing fleet of IoT devices to ensure minimal latency (delay) across a  distributed and decentralized model. Fog computing is the next new technology  that offers a distributed and federated compute model, which provides low- latency computational resources, elastic capabilities, and data analytics and  management. In 2018, NIST facilitated an effort to document the fog computing  conceptual model to help foster productive conversations among practitioners  and researchers and to guide the advancement of technologies that support this  model. The publication83 also introduces fog computing’s subsidiary concept, mist  computing, and identifies these concepts in relation to cloud computing, cloudlets,  and edge computing. 82 Public Safety Communications Research Division (PSCR), https://www.nist.gov/ctl/pscr 83 SP 500-325, Fog Computing Conceptual Model, https://csrc.nist.gov/publications/detail/sp/500-325/final NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 8 – Securing Emerging Technologies  28 Figure 4: Fog computing supporting a cloud-based ecosystem for smart end-devices NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 29 IMPERATIVE 9 Advancing Security Test and Measurement Tools Widespread adoption and continuing support for cybersecurity requires: •	 Testing and documentation of the effectiveness of recommended technologies and practices, •	 Means for measuring the effectiveness of recommended technologies and practices, •	 Monitoring and collection of information regarding asset inventories and security status, and •	 Effective presentation of the results to those who have responsibility for allocating resources within organizations. NIST is approaching the challenge of security status determination, monitoring, and  measurement along several avenues.		 • Anomaly Detection. NIST is working with industry to identify and demonstrate  automated tools for detecting security-relevant fault conditions. In FY18, a project  was initiated to show ways to improve detection of cybersecurity attacks in  manufacturing infrastructure environments. Preliminary project findings were  published for public comment as NISTIR 8219.84 The anomaly detection activity will  be expanded in FY19 to include additional detection tools. • Cyber Risk Analysis (CRA) Project.  This project promotes technical solutions that  enable organizations to bridge diverse, new, and existing data sets to advance  analysis of cyber risks. The goal is to enable information sharing among risk  owners about historical, current, and future cyber risk conditions. NIST is  leveraging past and present efforts such as a data repository for cyber incident  analysis, predictive analytics and strategic analysis on threat coverage, and  prioritization and gap identification. The near-term focus is on reconstructing the  University of Maryland (UMD) CyberChain Portal, which is the foundational  platform for a Cyber Incident Data and Analysis Repository (CIDAR)85 prototype.  NIST is exploring the frameworks, formats, and standards that could help to  automate data collection. In FY18, nearly 60 open and private cyber incident  databases were reviewed. In FY19, data elements associated with the CIDAR  categories will be examined for linkages that support data enrichment and bind 84 NISTIR 8219, Securing Manufacturing Industrial Control Systems: Behavioral Anomaly Detection, https://csrc.nist. gov/publications/detail/nistir/8219/draft 85 Enhancing Resilience Through Cyber Incident Data Sharing and Analysis, https://www.cisa.gov/sites/default/files/ publications/Overcoming%20Perceived%20Obstacles%20White%20Paper_1.pdf NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 IMPERATIVE 9 – Advancing Security Test and Measurement Tools 30 to services from NIST’s NVD and DHS’s Automated Indicator Sharing (AIS)86 tool.  The research effort’s output will be an initial proposed set of cyber incident data  points with associated metrics. A common lexicon is being explored through  collaboration with the Department of Defense and its DoD Cybersecurity Analysis  and Review (DoDCAR)87 program to incorporate the Open Security Controls  Assessment Language (OSCAL) to support the automation of reporting for  cybersecurity assessment-related information. •	 Asset Management. One of the immediate challenges most frequently cited by  organizations is how they can establish and maintain complete records of their  hardware, software, and information assets. Without knowing what hardware  and software assets are in their inventories, they cannot effectively ascertain  and monitor the security status of systems. NIST is collaborating with industry  partners to promote the adoption of ISO/IEC 19770-2:2015,88 which establishes  a specification for representing software identification and management  information, and with DHS to produce guidelines for interoperable software  identification (SWID) tags (NISTIR 8060).89 In FY18, NIST worked with the  IETF to integrate SWID tags into the Network Endpoint Assessment (NEA)90  protocol (RFC 8412),91 helped to develop a draft software inventory message and  attributes (SWIMA) specification,92 and released project webpages93 to provide  information on tagging. In FY18, NIST also finalized a guide that shows healthcare  organizations how to protect electronic health records from being exploited in a  manner that endangers patient health or compromises identity and privacy (SP  1800-1).94 NIST also released a guide that shows financial service organizations a  way to more securely and efficiently monitor and manage their many information  technology hardware and software assets (SP 1800-5).95 In FY19, NIST plans to  continue its SWID standardization activity and begin work on a Practice Guide  for using cybersecurity mechanisms to protect systems from exploitation from  internal and external network access. 86 Automated Indicator Sharing (AIS), https://www.dhs.gov/cisa/automated-indicator-sharing-ais 87 DoD Cybersecurity Analysis and Review, https://csrc.nist.gov/CSRC/media/Projects/cyber-supply-chain-risk- management/documents/SSCA/Fall_2018/WedPM2.2-STARCAR%20SCRM%20FINAL%20508.pdf 88 ISO/IEC 19770-2:2015, https://www.iso.org/standard/65666.html 89 NISTIR 8060, Guidelines for the Creation of Interoperable Software Identification (SWID) Tags, https://csrc.nist. gov/publications/detail/nistir/8060/final 90 Network Endpoint Assessment (NEA), https://tools.ietf.org/html/rfc5209 91 Software Inventory Message and Attributes (SWIMA) for PA-TNC, https://tools.ietf.org/html/rfc8412 92 Software Inventory Message and Attributes (SWIMA), https://datatracker.ietf.org/doc/rfc8412/ 93 Software Identification (SWID) Tagging, https://csrc.nist.gov/projects/software-identification-swid 94 SP 1800-1, Securing Electronic Health Records on Mobile Devices, https://www.nccoe.nist.gov/projects/use-cases/ health-it/ehr-on-mobile-devices 95 SP 1800-5, IT Asset Management, https://www.nccoe.nist.gov/projects/use-cases/financial-services-sector/it- asset-management IMPERATIVE 9 – Advancing Security Test and Measurement Tools NIST/ITL CYBERSECURITY PROGRAM ANNUAL REPORT  2018 31 •	 Automated Combinatorial Testing. Engineers often encounter security failures that  result from unexpected interactions between components. If all faults in a system  can be triggered by a combination of a known number of parameters, then testing  all possible combinations of those parameters with a practical number of tests  can provide strong fault detection efficiency. These methods96 are being applied  to software and hardware testing for reliability, safety, and security. NIST’s focus  is on empirical test results and their impact on real-world problems. Advances  for 2018 included the development of parallel processor code for measuring the  combinatorial coverage of very large (>1,000 variables) test arrays. This tool is being  applied by industry in evaluating the quality of their test suites for commercial  products. In FY19, these methods will be applied to verification and testing for the  Internet of Things in industrial control and home automation applications. •	 Security Automation and Continuous Monitoring. IT environments are under  constant threat of attack and are frequently undergoing change with new and  updated software being deployed along with updated configurations. The wide  variety of computing products, dynamic nature of software, speed of configuration  change, and diversity of threats challenges organizations to maintain situational  awareness over their IT assets and utilize this information to make informed risk- based decisions. Security automation employs standardized data formats and  transport protocols to enable data to be exchanged between business, operational,  and security systems that support security processes. This is done by identifying  IT assets, providing awareness over the operational state of computing devices,  enabling security reference data to be collected from internal and external sources,  and supporting analysis processes that measure the effectiveness of security  controls and provide visibility into security risks, thereby enabling risk-based  decision making. In FY18, NIST continued to create reference data, offer guidance,  and participate in international engagement for the development of flexible,  open standards. These efforts will be continued in FY19 as NIST seeks to improve  the interoperability, broad acceptance, and adoption of security automation  solutions to address current and future security challenges. In turn, this will create  opportunities for innovation. 96 Automated Combinatorial Testing for Software, https://csrc.nist.gov/projects/automated-combinatorial-testing- for-software THIS PAGE IS INTENTIONALLY LEFT BLANK|0|1|1|1|1
nih_artificial_intelligence_symposium_2025-05-16_program_booklet_v1.txt|"May 16th, 2025 Masur Auditorium, Building 10 Join us for a day-long symposium exploring a broad range  of AI approaches in biomedical science NIH Artificial Intelligence Symposium  Friday, May 16th, 2025  Building 10, Masur Auditorium    Biomedical science is in the early phase of a technological revolution, driven in large  part by innovations in deep learning neural network architecture and availability of  computational power. These cutting-edge techniques are being applied to every  sub-field of the biological sciences, and with novel ground-breaking advancements  arriving every week it is challenging for researchers to stay current on what is  available and possible. This one day symposium will bring together researchers  from a broad range of disciplines to share their AI-related research, with the goal  of disseminating the newest AI research, providing an opportunity to network, and  to cross-pollinate ideas across disciplines in order to advance AI research in  biomedicine.   2025 Planning Committee:  Ryan O’Neill (NHLBI)        Kristen Morgan (NHLBI)   Samar Samarjeet (NHLBI)      Chris Wanjek (OD)  Vineeta Das (NEI)         Colby Lewallen (NEI)  Katerina Atallah-Yunes (NCI)      Amy Stonelake (NCI)  Tiarnan Keenan (NEI)        Nick Asendorf (NHLBI)  Chris Combs (NHLBI)        Lana Yeganova (NLM/NCBI)    Sponsored by NHLBI and NIH Office of Intramural Research,                  in partnership with FAES  Artwork was created using generative AI, and no government funds were used for the artwork  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Agenda    8:30 – 9:00  Badge Pick-up  9:00 – 9:15  Opening Remarks – Nina Schor, MD, PhD, Deputy Director for Intramural  Research, NIH  9:15 – 10:15 Leo Anthony Celi, MD, MPH, MSc - Senior Research Scientist, Massachusetts  Institute of Technology; Associate Professor of Medicine, Harvard Medical School  - “AI by Us, for All of Us”  10:15 – 10:45 Zhiyong Lu, PhD, FACMI, FIAHSI - Senior Investigator, National Library of  Medicine - “Large Language Models in Medicine: From TrialGPT to GeneAgent”  10:45 – 11:00 Break, Coffee (FAES)  11:00 – 12:00 Poster Session 1 – Odd numbers (FAES Terrace)      12:00 – 12:30 Lunch  12:30 – 1:30 Poster Session 2 – Even numbers (FAES Terrace)  1:30 – 2:30  Alexander Rives, PhD - Chief Scientist and Co-founder, EvolutionaryScale; Core  Institute Member, Broad Institute; Assistant Professor, Massachusetts Institute  of Technology - “From Language to Life: AI’s Role in Redesigning Biology”  2:30 – 3:15  Short Talks  2:30 – Yoshitaka Inoue, NLM/NCI, “Interpretable Drug Response and Drug- Target Interaction Prediction Using Artificial Intelligence”  2:45 – Caroline Maclaren, NINDS, “Deep Learning Approach to Video-based  Behavioral Classification Through Human Pose Estimation”  3:00 – Sungrim Moon, PhD, NCATS, “Using Genomics Data for Basket Trial  Design in Rare Diseases”  3:15 – 3:30  Break  3:30 – 4:00  Ronald Summers, MD, PhD, FSAR, FAIMBE, FSPIE - Senior Investigator, NIH  Clinical Center - “The AI Revolution in Radiology Informatics”  4:00 – 4:45  Short Talks    4:00 – Xiaoyu Duan, PhD, NIDDK, “Deep learning cellular dynamics from single- cell RNA sequencing”    4:15 – Gefei Lin, NHLBI, “Multi-Task DeepHit: Simultaneous Prediction of Survival  and Progressions of Risk Factors with An Application of Predicting Mortality in  Sickle Cell Disease”  4:45 – 5:00  Final Remarks  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters    P1 - Lillich, A; Mornini, J  “Empowering AI literacy: Library-led training and support at NIH”    P2 - Mason, A; Patel, K; Nachabe, F; Firrincieli, D; Saraiya, D; Meyer, A; Footer, K; Croghan, J;  Rosenthal, A; Tartakovsky, M  “Transforming NIAID Operations with GenAI Tools”    P3 - Bruno, FP; Plevock Haase, KM,; Perez, L; Khan, S  “Harnessing AI in Implementation Science: Insights and Strategies from a Multidisciplinary Think  Tank”    P4 - Vineyard, N; Gao, J; Gao, J; Mudd, L; Peng, G; Sen, S; Kano, C; Kinsinger, C; Resat, H  “NIH Common Fund Bridge2AI Program”    P5 - Forsyth, AD  “AI for Health Science in Low-Resource Settings: NIH Portfolio Landscape, Gaps, and  Opportunities”    P6 - Radujevic, A; Wood, S; Muhoberac, M; Iyer, S; Parikh, A; Vakharia, N; Virani, S; Verma, M;  Masquelin, T; Godfrey, A; Gardner, S; Rudnicki, D; Hall, MD; Klumpp-Thomas, C; Chopra, G  “SciBORGs: Scientific Bespoke Artificial Intelligence Agents Optimized for Research Goals”    P7 - Wang, J; Sra, A; Weiss, JC  “Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS- CoV-2”    P8 - Kambara, MS; Chukka, O; Choi, KJ; Tsenum, J; Gupta, S; English, NJ1,; Jordan, IK; Mariño- Ramírez, L  “Explainable machine learning for health disparities: type 2 diabetes in the All of Us research  program”  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters    P9 - Egle, M; Groechel, RC; Johansen, MC; Kucharska-Newton, AM; Gottesman, RF; Koton, S  “The role of morbidity clusters in midlife on stroke incidence and severity: The ARIC study”    P10 - Olinger, B; Anerillas, C; Herman, AB; Tsitsipatis, D; Banarjee, R; Tanaka, T; Candia, J;  Walker, KA; Simonsick, EM; Gorospe, M; Basisty, N  “Machine Learning to Identify Tissue-Specific Clinical Associations of Senescence Signatures”    P11 - Sun, S; Do, AD; Zhu, Q  “AI-based biomarker discovery in CLN3”    P12 - Campagnolo, EM; Shulman, ED; Lodha, R; Stemmer, A; Jiang, P; Caldas, C; Knott, S; Hoang,  DT; Aldape, K; Ruppin, E  “Path2Space: An AI approach for cancer biomarker discovery via histopathology inferred spatial  transcriptomics”    P13 - Salazar-Cavazos, E; Jia, D; Missolo-Koussou, Y; Kenet, AL; Achar, S; Dada, H; Kondo, T;  Krishnan, A; Taylor, N; Jiang, P; Waterfall, J; DeVoe DL; Altan-Bonnet, G  “Stochasticity in cancer immunotherapy maps with the rarity of critical Spark T cells”    P14 - Huang, D; Ovcharenko, I  “Deep Learning reveals the significant contribution of silencer variants to human diseases and  traits”    P15 - Ahrend, F; Meister, G; Haase, AD  “Predicting piRNA cluster regions from genomic sequences using deep learning”    P16 - Manzo, G; Borkowski, K; Ovcharenko, I  “Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory Variants”      NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters  P17 - Srivastava, J; Ovcharenko, I  “Regulatory plasticity of the human genome”    P18 - Li, Q; Hanchard, N  “DNA methylation differences in children of severe acute malnutrition suggest epigenetic  networks at play via machine learning”    P19 - Hudaiberdiev, S; Ovcharenko, I  “Modeling cCREs using deep learning with applications to prioritizing candidate causal  mutations from GWAS data”    P20 – Um, S; Mooney  “Variant Effect Predictions for PTPN11 Missense Variants with MutPred2”    P21 - Moon, S; Maine J; Mathe, E; Zhu, Q  “Using Genomics Data and Literature for Basket Trial Design in Rare Diseases”    P22 - Halder, S; Periwal, V  “Donor-specific digital twin for living donor liver transplant recovery”    P23 - Shi, G; Nagarajan, V; Caspi, RR  “Identification of essential transcription factors by IAN: a new perspective on T cell licensing”    P24 - Park, M; Yan, C; Chen, Q; Khanna, R; Tanis, J; Meerzaman, D  “WSIomics: An Automated Pipeline for Training Multimodal AI Models to Classify therapy  response of cancer patients using whole slide images and transcriptome data”    P25 - Marini, N; Liang, Z; Rajaraman, S; Xue, Z; Antani, S  “Combining Real and Synthetic Data to Overcome Limited Training Datasets in Multimodal  Learning”    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters  P26 - Cordes, S  “Towards a better CAR  through in vitro and in silico Perturbations”    P27 - Kelly, C; Bahr, R; Zhu, W; Keyvanfar, K; Dagur, P; Cordes, S  “Optimizing CAR costimulatory domains using contrastive learning and optimal transport on  high-throughput screening data”    P28 - Duan, X; Periwal, V  “Deep learning cellular dynamics from single-cell RNA sequencing”    P29 - Zeng, W; Yadaw, AS; Mehta, K; Sanjak, J; Nguyen, D-T; Huang, R; Mathé, EA  “Predicting Chemical Toxicity by Applying a Hierarchical Bayesian Approach with Priors to the  Tox21 Assay Data”    P30 - Inoue, YI; Song, TS; Fu, TF; Luna, AL  “Interpretable Drug Response and Drug-Target Interaction Prediction Using Artificial  Intelligence”    P31 - Oyinloye, P; Wu, F; Lee, KH; Shi, L  “Advancing antidepressant discovery through machine learning-based QSAR modelling and  insights from SHAP features”    P32 - Jain, SJ; Yasgar, AY; Nilova, AN; Dalal, AD; Rai, GR; Zakharov, AZ  “AI-driven development of ALDH3A1 selective inhibitors”    P33 - Shah, P; Weber, C; Lim, G; Zhao, T; Sun, H; Jain, S; Zakharov, A; Siramshetty, V; Mathe, E;  Xu, T; Huang, R; Xu, X  “In silico ADME models in drug discovery”        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters  P34 - Colelough, BC ; Bartels, D; Demner-Fushman, D  “ClinIQLink: A Neuro-Symbolic Pipeline for QA generation with Crowd-Sourced Human-in-the- Loop Verification”    P35 - Mollerus, P; Seideman, J; Saraiya, D; Meyer, A; Footer, K; Chang, R; Nguyen, L; Croghan, J;  Rosenthal, A; Klinkenberg, L; Meyers, J  “Scientific Review NLP Conflict of Interest Identification”    P36 - Seideman, J; Do, W; Tembo, M; Opsahl-Ong, L; Meyer, A; Saraiya, D; Footer, K; Desai, A;  Lee, L; Nguyen, L; Croghan, J; Rosenthal, A; Tartakovsky, M  “Supervised Machine Learning for Scientific Coding Assistance”    P37 - Piatkowski, GS  “AI helped me write this: using AI to analyze NIH's AI and data science grant portfolio”    P38 - Balci, H; Luna, A  “Automating conversion of hand-drawn SBGN diagrams to SBGNML using large language  models”    P39 - Rotenberg, NH; Leaman, R; Islamaj, R; Fluharty, B; Kuivaniemi, H; Richardson, S; Tromp, G;  Lu, Z; Scheuermann, RH  “Cell phenotypes in the biomedical literature: First look at a new corpus”    P40 - Kaiyrbekov, K; Dobbins, NJ; Mooney, S  “Automated Survey Collection with LLM-based Conversational Agents”    P41 - Ornek, ME; Zahnen, CR; Chen, M-C  “Author and affiliated institution extraction from free-form letters using GenAI”    P42 - Heymann, D; Mykins, M; Zhou, N  “AI in action at the NICHD: Case studies and developmental pathways”  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters    P43 - Alodadi, M; Lyons, E; Che, A; Watson, D; Tawa, GJ; Porter, F; Haugabook, SJ; Ottinger, E;  Mudunuri, U  “RARe-SOURCE Literature AI: Rare Disease Genotype-Phenotype Associations from Biomedical  Literature”    P44 - Jin, Q; Wang, Z; Floudas, CS; Wan, N; Chan, J; Chen, F; Gong, C; Bracken-Clarke, D; Xue, E;  Fang, Y; Tian, S; Yang, Y; Sun, J; Lu, Z  “TrialGPT: matching patients to clinical trials with large language models”    P45 - Soni, SS; Demner-Fushman, DD  “A Dataset for Grounded Question Answering from Electronic Health Records to Relieve  Clinician Burden”    P46 - Kumar, SK; Noroozizadeh, SN; Weiss, JCW  “Forecasting from Clinical Textual Time Series: Adaptations of the BERT and Decoder Families”    P47 - Aston, SA; Cheng, H  “Responsible Integration of Large Language Models in Biomedical Research”    P48 - Liang, Z; Rajaraman, S; Marini, N; Xue, Z; Antani, S  “Multi-Agent Cross-Modal Large Language Model Framework for Chest X-ray Analysis and  Integrating COVID-19 Pneumonia Predictions”    P49 - Nolte, S; Saddler, TO; Reif, DM;  Schmitt, CP; Auerbach, SS; Hsieh, J-H  “RAG2SQL”    P50 - Erkan, CN; Gu, G; Tandilashvili, E; Meigs, JM; Lee, K; Metcalf, O; Livinski, A; Pine, DS;  Pereira, F; Brotman, MA; Henry, LA  “Leveraging Large Language Models for data extraction and quality assessment in psychiatry  systematic reviews: A comparison of inter-rater reliability between Elicit and human coders”  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters    P51 - Lowekamp, BC; Gabrielian, A; Hurt, DE; Rosenthal, A; Yaniv, Z  “Tuberculosis chest X-ray image retrieval system using deep learning based biomarker  predictions”    P52 - Arlova, A; Weller, C; Nalls, M; Kelpsch, D; Faghri, F; Ryan, V  “Artificial Intelligence-based Segmentation of Neurites in High-Resolution Microscopy Images of  iPSC-derived neurons”    P53 - Lowekamp, B; Yaniv, Z; Cobean, R; Hoppes, M; Rosenfeld, G; Grinev, A; Gabrielian, A;  Hurt, D; Rosenthal, A; Tartakovsky, M  “Tuberculosis Portals AI in Image Processing and Abnormality Detection”    P54 - May, CM; Kasi, KK; Kobayashi, LK; Conway, BC; Pare, JP  “Machine Learning Classification of Clinical Edema”    P55 - Khanna, K; Chen, Q; Yan, C; Meerzaman, D  “Leveraging an MRI-Based Foundation Model to Enhance Predictions of Survival in  Glioblastoma: A Multimodal Deep Learning Approach”    P56 - MacLaren, CE; Jackson, SN; Fruchet, OE; Volkman, RA; Inati, SK; Zaghloul, KA  “Deep learning approach to video-based behavioral classification through human pose  estimation”    P57 - Shive, HR  “AI-based analysis of complex pigmentation phenotypes in zebrafish embryos”    P58 - Lohmann, JJGL; Witte, AW; Maier, AM; Saak, CCS; Sauter, GS; Zimmermann, MZ; Bonn,  SB; Baumbach, JB  “Privacy-preserving and communication-efficient prediction of ISUP grades from prostate  cancer histopathology images with foundation models”  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters    P59 - Cheng, J; Flaharty, KA; Duong, D; Waikel, RL; Hu, P; Ledgister Hanchard SE; Solomon, BD  “The effects of syndromic facial feature editing on AI and clinician diagnosis of genetic  conditions”    P60 - Kantipudi, K; Gabrielian, A; Hurt, DE; Rosenthal, A; Yaniv, Z  “Predicting tuberculosis from frontal chest X-rays: A Radiomics Analysis Portal research service”     P61 - von Buchholtz, LJ  “Deep learning assisted matrix factorization improves cell recognition in calcium imaging  analysis”    P62 - Patel, MH; Stecko, H; Pramod, N; Esengur, O; Stevenson, E; Saini, J; Loebach, L; Blachman- Braun, R; Millan, B; Nethala, D; Gurram, S; Linehan, WM; Turkbey, B; Ball, MW  “Predicting Renal Tumor Pathology from Gross Appearance: An AI-based Pilot Study”    P63 - Bhadra, S; Liu, J; Summers, RM  “Weakly supervised learning for subcutaneous edema segmentation of abdominal CT using  pseudo-labels and multi-stage nnU-Nets”    P64 - Chan, S; Mathai, TS; Balamuralikrishna, PTS; Batheja, V; Liu, J; Lubner, MG; Pickhardt, PJ;  Summers, RM  “Staging Liver Fibrosis with Hepatic Perivascular Adipose Tissue as a CT Biomarker”    P65 - Kantipudi, K; Bui, V; Yu, H; Lure, YMF; Jaeger, S; Yaniv, Z  “Semantic segmentation of TB in chest X-rays: A new dataset and generalization evaluation”    P66 - Joseph, TL; Yu, ZX; Siddique, MAH; Chen, LY; Elinoff, JM  “Developing a deep learning algorithm to quantify pulmonary vascular remodeling in a pre- clinical model of pulmonary arterial hypertension and comparing performance to formal  histopathological assessment”  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters    P67 - Harouni, M; Voss TC  “Scalable deep learning-based vessel segmentation and morphological quantification”    P68 - Bhardwaj, A; Narayan, K  “Empanada - a napari plugin with pre-packaged segmentation models for nuclei, lipid droplets  and mitochondria”    P69 - Elsawy, A; Keenan, T; Chew, EY; Lu, Z  “Optical Coherence Tomography: A Reliable Imaging Modality for Detecting Age-Related  Macular Degeneration Features”    P70 - Mathai, TS; Balamuralikrishna, PTS; Batheja, V; Kassin, M; Hannah, C; Ukeh, I; Hernandez,  J; Summers, RM  “Deep Learning-based Contouring of Couinaud Segments on CT: Utility for Volumetric Analysis  of Future Liver Remnant”    P71 - Aggarwal, M; Cogan, N; Periwal, V1  “Sensitivity based model agnostic scalable explanations of deep learning”    P72 - Lita, A; Sjöberg, J; Păcioianu, D; Siminea, N; Celiku, O; Dowdy, T; Păun, A; Gilbert, MR;  Noushmehr, H; Petre, I; Larion, M1  “Raman-based machine-learning platform reveals unique metabolic differences between  IDHmut and IDHwt glioma”    P73 - Bodosa, J; Pastor, R  “Understanding and simulating membrane pore formation by piscidin1 using AI informed  enhanced sampling”    P74 - Weaver, A; Tuvikene, J; Koivomagi, M  “AlphaFold2 screen reveals novel G1 cyclin docking modalities”  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Posters    P75 - Sahakyan, HK; Babajanyan, SG; Wolf, YI; Koonin, EV  “In silico evolution of globular protein folds from random sequences”    P76 - Tuvikene, J; Esvald, EE; Heidebrink G; Koivomagi, M  “Computational modeling of Cyclin D1 protein-protein interactions”    P77 - Kanno, T; Kalchschmidt JS; Brooks, SR; Sun, H  “Integrating Network Analysis and Localization Prediction Using B-LEARN and ProtGPS”    P78 - Nguyen, TH; Ghedin, E; Sormanni, P  “Efficient Computational Prioritization of Local Host Structures Mimicking Pathogen Antibody  Epitopes”      Keynote Speakers NIH ArƟficial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10    Leo Anthony Celi, MD, MPH, MSc  Principal Research ScienƟst, MassachuseƩs  InsƟtute of Technology; Associate Professor of  Medicine (Part Time), Harvard Medical School;  Instructor, Harvard T.H. Chan School of Public  Health; Associate Program Director, Department  of Medicine, Beth Israel Deaconess Medical  Center; Co-Director, Sana (MIT)  Leo Anthony Celi is a global leader in AI-driven healthcare,  renowned for his groundbreaking work in leveraging data science to improve clinical outcomes  and promote health equity. With a medical career spanning three conƟnents, he brings a unique,  inclusive perspecƟve to his mission of transforming healthcare through technology. He is the  creator of the Medical InformaƟon Mart for Intensive Care (MIMIC) database, a publicly  accessible resource that has become a cornerstone for AI research in criƟcal care, enabling  thousands of researchers in over 30 countries to develop innovaƟve soluƟons. His partnership  with Philips produced the eICU CollaboraƟve Research Database, which provides comprehensive  data on over 2 million ICU paƟents, further expanding the foundaƟon for AI-driven advancements  in medicine.  Through his leadership in Sana, an MIT-based iniƟaƟve, Dr. Celi develops open-source mobile  health technologies that empower healthcare providers in underserved regions, delivering criƟcal  care to communiƟes with limited resources. This work has earned internaƟonal acclaim, including  first place in the 2012 Mobile Health University Challenge and a finalist posiƟon for the 2011  INDEX: Award for Design to Improve Life. His research, cited over 36,000 Ɵmes, reflects his  profound impact on the field, with contribuƟons that bridge clinical pracƟce, data science, and  global health policy.  Dr. Celi is also a passionate advocate for ethical AI, pushing for sustainable pracƟces that miƟgate  the environmental impact of AI infrastructure, such as the energy demands of data centers, and  ensure equitable access to technological benefits. He emphasizes the importance of  understanding clinical data’s context to build trustworthy AI models, challenging the field to  prioriƟze transparency and inclusivity. His vision for a healthcare system where AI serves all  communiƟes makes him a vital voice in shaping the future of biomedicine.  hƩps://imes.mit.edu/people/celi-leo   hƩps://www.youtube.com/watch?v=3StkjrQ8n5Y       NIH ArƟficial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10    Alexander Rives, PhD  Chief ScienƟst and Co-founder, EvoluƟonaryScale;  Core InsƟtute Member, Broad InsƟtute of MIT and  Harvard; Assistant Professor, Department of  Electrical Engineering and Computer Science, MIT  Alexander Rives, PhD, is a trailblazer in the integraƟon of  arƟficial intelligence and biology, driving transformaƟve  advancements in biomedicine through computaƟonal  innovaƟon. At EvoluƟonaryScale, the public-benefit startup  he co-founded, he spearheads the development of advanced AI models like ESM3, which has  demonstrated remarkable capabiliƟes in generaƟng novel proteins such as esmGFP. These  innovaƟons hold immense potenƟal for acceleraƟng scienƟfic discovery and developing new  therapeuƟcs, reshaping how we approach biological research and healthcare soluƟons.  Prior to EvoluƟonaryScale, Rives led the EvoluƟonary Scale Modeling (ESM) project at Meta’s AI  research lab, where he pioneered the creaƟon of the first large-scale transformer language  models for proteins. These models have been widely adopted by the global scienƟfic community,  enabling breakthroughs in drug design, predicƟng the clinical effects of geneƟc mutaƟons, and  modeling cellular processes. His research, cited over 9,800 Ɵmes, underscores his profound  influence on the field, with publicaƟons like the 2019 paper “Biological structure and funcƟon  emerge from scaling unsupervised learning to 250 million protein sequences” seƫng new  benchmarks in computaƟonal biology.  Rives’s entrepreneurial vision extends to co-founding biotech companies such as Fate  TherapeuƟcs and Syros PharmaceuƟcals, both publicly traded on NASDAQ, and Kallyope,  demonstraƟng his ability to translate cuƫng-edge research into impacƞul commercial ventures.  In his academic roles at the Broad InsƟtute and MIT, he fosters interdisciplinary collaboraƟon,  developing AI systems that support global scienƟfic efforts and mentoring the next generaƟon of  researchers at the intersecƟon of AI and biology. With a PhD in Computer Science from New York  University and a B.S. in Philosophy and Biology from Yale University, Rives brings a  mulƟdisciplinary perspecƟve to his work, ensuring that AI advancements in biomedicine are not  only technically groundbreaking but also ethically responsible. His leadership conƟnues to shape  the future of AI-driven healthcare, making him a pivotal voice in the field.  hƩps://www.evoluƟonaryscale.ai/   hƩps://www.broadinsƟtute.org/bios/alex-rives   hƩps://www.youtube.com/watch?v=TiDo7xXMbUI       NIH ArƟficial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10    Zhiyong Lu, PhD, FACMI, FIAHSI   Senior InvesƟgator, NaƟonal Library of Medicine;  Deputy Director for Literature Search, NaƟonal  Center for Biotechnology InformaƟon; Adjunct  Professor, University of Illinois Urbana-Champaign  Zhiyong Lu, PhD, FACMI, FIAHSI, is a leading innovator in  biomedical informaƟcs, renowned for his pioneering work in  applying arƟficial intelligence and machine learning to  enhance biomedical research and healthcare. At the  NaƟonal Library of Medicine (NLM), he drives advancements  in text mining and informaƟon retrieval, significantly  improving access to scienƟfic literature through widely used plaƞorms like PubMed and LitCovid.  These resources empower millions of researchers and clinicians worldwide by providing efficient,  AI-driven tools to navigate vast biomedical datasets.  Dr. Lu’s research has led to the development of transformaƟve tools such as LitVar 2.0, which  tracks geneƟc variants in biomedical literature, and TrialGPT, a large language model that  streamlines paƟent matching for clinical trials. His work on advanced AI models, including  GeneAgent, showcases his experƟse in harnessing large language models to address complex  challenges in precision medicine and geneƟc research. As an Adjunct Professor at the University  of Illinois Urbana-Champaign, he mentors students in computaƟonal biology, fostering the next  generaƟon of AI researchers.  Recognized as a Fellow of the American College of Medical InformaƟcs and the InternaƟonal  Academy of Health Sciences InformaƟcs, Dr. Lu’s contribuƟons have earned him internaƟonal  acclaim. He plays a pivotal role in shaping AI policy, serving on the US federal AI R&D inter-agency  commiƩee and the NIH Intramural Research Program AI task force. His leadership in organizing  workshops and ediƟng special issues, such as the 2024 JAMIA special issue on large language  models in biomedicine, underscores his commitment to advancing the field. Dr. Lu’s work  conƟnues to bridge technology and medicine, driving innovaƟon in AI applicaƟons for healthcare.   hƩps://irp.nih.gov/pi/zhiyong-lu   hƩps://www.youtube.com/watch?v=AHPPGECs7KQ       NIH ArƟficial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10    Ronald M. Summers, MD, PhD, FSAR,  FAIMBE, FSPIE   Senior InvesƟgator, NIH Clinical Center; Chief,  Clinical  Image Processing  Service;  Director,  Imaging  Biomarkers  and  Computer-Aided  Diagnosis Laboratory  Ronald M. Summers, MD, PhD, is a visionary radiologist  whose pioneering work in arƟficial intelligence has  revoluƟonized medical imaging, parƟcularly in cancer  diagnosis and treatment. His research harnesses deep  learning to develop advanced computer-aided diagnosis systems, significantly improving the  detecƟon of lung, colon, and other cancers through techniques like virtual colonoscopy. By  creaƟng large-scale radiologic image databases, he has provided the global research community  with criƟcal resources to train AI models, fostering innovaƟon in radiology informaƟcs. His prolific  output, with over 500 publicaƟons cited more than 57,700 Ɵmes and 12 patents, underscores his  transformaƟve impact on the field.  Summers’s experƟse in thoracic and abdominal radiology and body cross-secƟonal imaging  informs his AI-driven innovaƟons, which have set new standards for precision and efficiency in  clinical pracƟce. His leadership extends to mentoring dozens of radiology fellows, many of whom  have become leaders in the field, and serving on editorial boards for presƟgious journals like  Radiology: ArƟficial Intelligence and Journal of Medical Imaging. Recognized with the PresidenƟal  Early Career Award for ScienƟsts and Engineers in 2000 and the NIH Director’s Award in 2012, he  holds fellowships from the Society of Abdominal Radiologists, the American InsƟtute for Medical  and Biological Engineering, and the Society of Photo-OpƟcal InstrumentaƟon Engineers. His  global influence is further evidenced by his roles in shaping AI standards through conferences like  SPIE Medical Imaging, where he has served as program co-chair. With a career bridging clinical  experƟse and technological innovaƟon, Summers conƟnues to drive the AI revoluƟon in radiology,  advancing the future of precision medicine.  hƩps://www.cc.nih.gov/meet-our-doctors/rsummers     Abstracts - Short Talks NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10    Short Talk Abstracts    Interpretable Drug Response and Drug-Target Interaction Prediction Using Artificial  Intelligence  Inoue, YI1,2,3; Song, TS1; Fu, TF4; Luna, AL2,3  1. Computer Science, University of Minnesota, Minneapolis, MN  2. Computational Biology Branch, National Library of Medicine, National Institutes of Health, Bethesda,  MD  3. Developmental Therapeutics Branch, National Cancer Institute, National Institutes of Health,  Bethesda, MD  4. Department of Computer Science, Nanjing University Nanjing, Jiangsu, China    A challenge of using machine learning (ML) in biomedical research is a lack of interpretability, which  limits its support of data-driven decisions with explanations. We explore this topic here, focusing on  cancer drug response and mechanism prediction. We introduce two components: GraphPINE (Graph  Propagating Importance Network for Explanation) and DrugAgent. GraphPINE is a graph neural network  (GNN) model for drug response prediction using multi-omics data (e.g., gene expression) and interaction  networks (e.g., protein-protein). The novelty of GraphPINE lies in its initialization of importance scores  using biological prior knowledge (drug-target interactions, DTIs) from literature and a dynamic updating  mechanism. We build on concepts from LSTM (Long Short-Term Memory), relying on previous  predictions as hidden states to advance GNNs such that GraphPINE initializes importance scores using  prior knowledge and updates these scores during model training. We apply GraphPINE to NCI60 data;  GraphPINE achieves AUROC of 0.796 and AUPRC of 0.894 for 952 drugs. Separately, we developed  DrugAgent, a multi-agent system integrating knowledge graphs, internet searches, ML methods, and  large language models (LLMs) to improve DTI prediction. DrugAgent was evaluated using 178 kinase  inhibitors against 300 kinases; DrugAgent achieves superior performance (AUROC: 0.905 and AUPRC:  0.529). Interpretable subgraphs accompany GraphPINE results, while DrugAgent results are enriched  with prior knowledge. Multiple lines of evidence must support conclusions in biomedical research. The  bioinformatics efforts here build on this fundamental notion to draw in additional data from  heterogeneous sources uniformly and transparently as part of ensemble results presented to users.       See also Poster P31    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10    Short Talk Abstracts    Deep learning approach to video-based behavioral classification through human pose  estimation  MacLaren, CE1; Jackson, SN1; Fruchet, OE1; Volkman, RA1; Inati, SK2; Zaghloul, KA1  1. Functional Neurosurgery Section, Surgical Neurology Branch, National Institute of Neurological  Disorders and Stroke, National Institutes of Health, Bethesda, MD  2. Neurophysiology of Epilepsy Unit, Surgical Neurology Branch, National Institute of Neurological  Disorders and Stroke, National Institutes of Health, Bethesda, MD    There exist current methods for identifying human action in videos, but little advancements in behavior  in a hospital environment, where patients are monitored 24/7 via a live-stream camera. With new  advances in machine learning and computer vision, different deep learning models can now identify  objects and track their movement throughout a video. Through such, human movement – described as  human pose – can be extracted in videos, creating opportunity for tracking and classifying different  actions. We are interested in applying these methods for our own video data, where we record 24/7  clinical footage for epilepsy patients admitted at the NIH for seizure monitoring. Pre-trained human pose  models achieve very high mean average precision (mAP) and are useful for transferring to different  datasets. Utilizing a pre-trained network and fine-tuning for refined features, we can identify more  positional information to the standard pre-trained network for our non-uniform environments, where  patients may be in different settings with various obstructions, such as staff and family interruptions,  blankets, tables, etc. We are able to achieve a mAP of 0.78147, identifying 18 different points of interest  on the human body, and a precision of 0.99668 for identifying the proper boundaries of our patient. By  correctly identifying human movement in videos, we can cluster different behaviors to classify a patient’s  unique behaviors. With this annotated data, we can extract neural correlates for precise behaviors  throughout a patient’s entire stay.    See also Poster P57    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10    Short Talk Abstracts    Using Genomics Data for Basket Trial Design in Rare Diseases  Moon, S1; Maine, J1; Mathe, E1; Zhu, Q1  1. Division of Pre-Clinical Innovation, National Center for Advancing Translational Sciences, Rockville, MD    Gaining insight into the underlying molecular etiologies of rare diseases can aid cross-disease research,  inform the design of basket trials, and identify drug repurposing opportunities. In our preliminary study,  we identified 36 rare disease clusters based on common genetic causes and biological mechanisms.  However, there clusters were too broad for basket trial applications. In this study, we refined these  clusters by collecting allelic variant data from the Online Mendelian Inheritance in Man (OMIM), along  with corresponding Sorting Intolerant From Tolerant (SIFT) scores for single nucleotide polymorphisms  (SNPs) and transcripts from Ensemble  validated from the Medical Genomics Japan Variant Database  (MGeND). We assessed the functional impact of gene mutations using SIFT scores, calculating the ratio of  deleterious to tolerated cases (deleterious cases / (deleterious cases + tolerated cases)). We generated  an matrix with imputed data by extracting the deleterious level of genetic and mutation data for each rare  disease, and identified shared mutations across diseases. Then, we applied Density-Based Spatial  Clustering of Applications with Noise (DBSCAN) to the imputed matrix, creating sub-clusters on the top of  the 36 clusters. Our results illustrate consistent findings with the published studies of basket trial design  for instance, a subcluster of NLRP3 mutation-related diseases including Neonatal Onset Multisystem  Inflammatory Disease, Familial Cold Autoinflammatory Syndrome, and Muckle-Wells Syndrome.    See also Poster P22        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10    Short Talk Abstracts    Deep learning cellular dynamics from single-cell RNA sequencing  Duan, X1; Periwal, V1  1. Lab of Biological Modeling, National Institute of Diabetes and Digestive and Kidney Diseases, National  Institutes of Health, Bethesda, MD    Single-cell RNA sequencing (scRNA-seq) provides a powerful framework for studying cellular  heterogeneity, transitions, and regulatory networks. However, reconstructing the underlying dynamical  processes governing these transitions remains a major challenge due to the high-dimensional nature of  gene expression data. To address this, we develop a variational autoencoder (VAE)-based approach that  learns a low-dimensional latent representation of cellular states and models their temporal evolution. We  apply our framework to gene expression data from Drosophila melanogaster blastoderm embryos,  compiled by Fowlkes et al., which includes measurements across multiple time points using a registration  technique. In this approach, gene expression profiles are encoded into a low-dimensional latent space,  where we train a neural stochastic differential equation (SDE) network to capture the continuous  dynamics of latent states over developmental time. The learned neural SDE models the progression of  cellular states, and a decoder subsequently maps these evolving latent representations back to the  original high-dimensional gene expression space, allowing for both accurate reconstruction of observed  transcriptional patterns and insight into the underlying dynamical processes. Future directions include the  use of symbolic regression to extract dynamical models from the inferred trajectories.    See also Poster P29        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10    Short Talk Abstracts    Multi-Task DeepHit: Simultaneous Prediction of Survival and Progressions of Risk  Factors with An Application of Predicting Mortality in Sickle Cell Disease  Lin, GL1,4; Tian, XT2; Wu, COW2; Thein, SLT3; Miao, RM2  1. Office of Clinical Director, National Heart, Lung, and Blood Institute, National Institutes of Health,  Bethesda, MD  2. Office of Biostatistics Research, National Heart, Lung, and Blood Institute, National Institutes of  Health, Bethesda, MD  3. Sickle Cell Branch, National Heart, Lung, and Blood Institute, National Institutes of Health, Bethesda,  MD  4. Department of Statistics, The George Washington University, Washington, DC    Survival analysis aims to estimate time-to-event outcomes but is frequently challenged by censoring due  to limited study duration, loss to follow-up, and competing risks, reducing the number of observed events.  Traditional approaches usually rely on surrogate or composite endpoints can diminish interpretability, and  the proportional hazards assumption in commonly used Cox models is often violated in real-world  settings. Progressions of risk factors, such as laboratory measurements, offer meaningful signals for  survival prediction but are typically modeled separately, which may lead to inconsistencies in variable  selection or effect directions.  To overcome these challenges, we develop a deep learning-based multi-task model (Multi-task DeepHit)  that simultaneously predicts long-term mortality and short-term trajectories of risk factors. The model  includes a shared representation network with a masked attention-based encoder pre-trained on baseline  variables, and task-specific networks for predicting yearly mortality and trajectories of risk factors. This  architecture fully utilizes available variables and allows partial missingness.  We applied our approach to a Sickle Cell Disease dataset of 598 patients with 68 baseline covariates and  13 key intermediate risk factors. Compared with Cox models using top variables selected by random  survival forest and original DeepHit with baseline data, our model achieved superior discrimination and  calibration, with a higher bootstrap-corrected C-statistic of 0.8648 (compared to 0.7324) and a lower 4- year integrated Brier score of 0.0364 (compared to 0.05). The model remains robust even when partial  missingness exists during evaluation. At both the population and individual levels, we further explain our  model using SHAP values to identify important variable contributions to mortality risk, which are clinically  interpretable.     Abstracts - Posters   NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P1  Empowering AI literacy: Library-led training and support at NIH  Lillich, A1; Mornini, J1  1. ORS Division of Library Services, National Institutes of Health, Bethesda, MD    As artificial intelligence (AI) continues to shape biomedical research and administrative processes, AI  literacy has become essential for professionals at NIH. The NIH Library plays a critical role in supporting AI  adoption by providing training, consultations, and curated resources to help researchers, administrators,  and technical staff integrate AI tools responsibly and effectively.  This poster presents the NIH Library’s approach to AI literacy, outlining key training programs,  personalized support services, and collaborative efforts. Through workshops, events, one-on-one  consultations, and learning materials, the NIH Library equips NIH staff with practical AI skills while  addressing ethical considerations and best practices.  We highlight the impact of these NIH Library initiatives, showcasing engagement metrics and user  feedback that demonstrate AI’s transformative potential in research and operations. Additionally, we  discuss common challenges in AI adoption and the strategies we use to overcome them.  By sharing insights from the NIH Library’s AI literacy program, this poster aims to inform best practices for  integrating AI support in a federal research environment and inspire further collaboration on AI education  at NIH.         NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P2  Transforming NIAID Operations with GenAI Tools  Mason, A1; Patel, K1; Nachabe, F1; Firrincieli, D1; Saraiya, D1; Meyer, A1; Footer, K1; Croghan, J1;  Rosenthal, A1; Tartakovsky, M1  1. Office of Cyber Infrastructure and Computational Biology, National Institute of Allergy and Infectious  Diseases, National Institutes of Health, Bethesda, MD    The introduction of GenAI tools at the National Institute of Allergy and Infectious Diseases (NIAID) marks  a significant advancement in artificial intelligence application. Incorporating the open-source AI  middleware GovConnect.ai, NIAID swiftly deployed applications including GenAI Chat, GenAI Doc Bot,  and GenAI SummarizeIt, empowering over 5,000 staff to enhance productivity through large language  models (LLMs). These tools are used for content generation and summarization, translation,  document comparison, and information synthesis. The GovConnect.ai (platform) is specifically designed  to enable enterprise AI adoption by providing connectors to essential AI components integrating diverse  databases, cloud storage, and transformation code blocks for unified data orchestration. It ensures secure  access and offers observability tools for cost monitoring. As a cloud-agnostic platform, GovConnect.ai  boosts software development efficiency, promotes experimentation, and accelerates AI innovation at  NIAID.   GenAI Chat serves as an advanced chatbot, enabling users to interact with LLMs by posing questions  through prompts. It delivers detailed and precise answers, enhancing decision-making and expediting  information retrieval.    GenAI Doc Bot enables users to upload multiple documents like PDFs, PowerPoints, and Word files for  comprehensive insights, improving document analysis and understanding with contextually accurate  responses.   GenAI SummarizeIt optimizes productivity by summarizing lengthy documents, such as policies or  publications, reducing review time and enabling NIAID personnel to focus on more critical tasks.     Collectively, these GenAI tools represent a transformative leap in applying artificial intelligence to  everyday business processes, significantly enhancing operational efficiency and productivity at NIAID,  with broader implications for the National Institutes of Health.         NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P3  Harnessing AI in Implementation Science: Insights and Strategies from a  Multidisciplinary Think Tank  Bruno, FP1; Plevock Haase, KM,1; Perez, L1; Khan, S1  1. Center for Translation Research and Implementation Science, National Heart, Lung, and Blood  Institute, National Institutes of Health, Bethesda, MD    Background: Integrating artificial intelligence (AI) and implementation science (IS) offers critical  opportunities to advance evidence-based practices and improve health outcomes. A recent think tank  convened by the Center for Translation Research and Implementation Science (CTRIS) brought together  clinicians, researchers, and AI experts to examine challenges and explore future directions.  Objectives: This abstract summarizes key discussed themes, including ethical considerations, innovative  applications, data management, and strategies for further growth.  Discussion:  • Applications: AI facilitates personalized healthcare, supports guideline adherence, and streamlines EHR  management using large language models. Risk-prediction tools can also inform targeted interventions  for hospital readmission.  • Data and privacy: Transparency and privacy remain paramount. Explainable AI fosters clarity and trust,  while federated learning safeguards data. Evaluation frameworks, such as CONSORT-AI and RE-AIM,  support responsible innovation. Community-led governance and Community-Based Participatory  Research are essential for building trust and safeguarding data sovereignty.  • Challenges, Gaps, and Opportunities: Conflicting priorities between market-driven and academic  approaches underscore the need for adaptable, transparent practices. Obstacles include the “black box”  challenge, limited AI literacy among stakeholders, and privacy concerns. However, locally-tailored  frameworks, collaborative knowledge-sharing, and balanced oversight can enhance capacity-building and  accelerate health translation across diverse populations.  • Future steps: Addressing population-specific biases, integrating context-specific factors influencing  health outcomes, and developing adaptive AI platforms can advance comprehensive care in under- resourced settings.  Conclusions: Realizing AI’s potential in IS requires robust interdisciplinary collaboration, transparent  governance, and sustained community engagement. Researchers emphasize co-created frameworks that  leverage diverse data sources, incorporate real-world contexts, and promote equitable adoption for  improved healthcare outcomes.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P4  NIH Common Fund Bridge2AI Program  Vineyard, N1; Gao, J2; Gao, J3; Mudd, L4; Peng, G5; Sen, S2; Kano, C1; Kinsinger, C1; Resat, H1  1. Office of the Director/Office of Strategic Coordination, National Institutes of Health, Bethesda, MD  2. National Human Genome Research Institute, National Institutes of Health, Bethesda, MD  3. National Eye Institute, National Institutes of Health, Bethesda, MD  4. National Center for Complementary and Integrative Health, National Institutes of Health, Bethesda,  MD  5. National Institute of Biomedical Imaging and Bioengineering, National Institutes of Health, Bethesda,  MD    The NIH Common Fund’s Bridge to Artificial Intelligence (Bridge2AI) program aims to propel biomedical  research forward by setting the stage for widespread adoption of artificial intelligence and machine  learning (AI/ML) to tackle complex biomedical and behavioral research challenges. The Bridge2AI program  bridges the gap between the biomedical and behavioral research communities through a consortium of  experts to set the stage for widespread adoption of AI/ML in medicine. The Bridge2AI program is  generating flagship data, developing best practices for making data ready for use in AI/ML models, and  developing the workforce for the next generation of AI/ML specialists. Bridge2AI established four grand  challenges to motivate these goals. The data from these grand challenges include: voice and speech for  predictive diagnostics; functional genomic mapping of human cells; clinical data for diagnosis and risk  prediction in acute care settings; and salutogenesis of Type II diabetes as a model for health restoration.  To-date, the program has achieved several milestones such as: assembling large multidisciplinary teams  of experts; hosting jamborees and hackathons of data resources; providing hands-on training to teach the  next generation of researchers; hosting symposiums; creating educational modules for skills and  workforce development; and publishing best practice guidelines on the program’s lessons learned.     Bridge2AI released preliminary pilot data in 2024 with ongoing additional data releases occurring.  Additional data releases are scheduled until the end of program in 2026. The program will also release  best practices guidelines for the collection and preparation of AI/ML-ready data, considerations for data  collection and (re)use, and how to make the future data collection projects AI/ML ready.         NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P5  AI for Health Science in Low-Resource Settings: NIH Portfolio Landscape, Gaps, and  Opportunities  Forsyth, AD1  1. Fogarty International Center, National Institutes of Health, Bethesda, MD    Artificial Intelligence (AI) demonstrates potential to deliver innovative, cost-efficient solutions in resource- constrained settings by addressing local health priorities while strengthening health systems, disease  surveillance, and responses to critical challenges, regardless of geographic settings. AI investments can  also bolster U.S. health security by detecting threats before they become domestic crises and sparking  health advances that benefit all.  The Fogarty International Center (FIC) has launched a strategic initiative to catalyze ethical and  responsible AI use in low-resource settings, ranging from rural America to low- and middle-income  countries (LMICs), through a scoping literature review, NIH portfolio analysis, and partner mapping. A  review of nearly 80 peer-reviewed articles enabled AI-assisted categorization of ~2,000 NIH-funded grants  by eight primary use-cases across country income status. Although only 5.2% of NIH’s AI portfolio supports  LMIC-based research, almost 70% focuses on diagnostics and treatment, mirroring early priorities for the  technology’s use.  The current distribution of grants also shows notable gaps in disease surveillance, health systems  optimization, and remote care, particularly in LMICs; further, it does not align with the health conditions  that account for the highest probability of premature mortality. This suggests a mismatch between  research investment and global health burden. FIC’s initiative aims to inform the strategic use of  accessible, fair, and trustworthy AI to advance health that maximizes NIH’s impact across all resource- limited settings. By identifying gaps and opportunities, the project seeks to foster coordination,  knowledge-sharing, and collaborative research to ensure AI delivers sustainable benefits for domestic and  international health.       NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P6  SciBORGs: Scientific Bespoke Artificial Intelligence Agents Optimized for Research  Goals  Radujevic, A1; Wood, S1; Muhoberac, M2; Iyer, S2; Parikh, A3; Vakharia, N4; Virani, S2; Verma, M1;  Masquelin, T1; Godfrey, A1; Gardner, S1; Rudnicki, D1; Hall, MD1; Klumpp-Thomas, C1; Chopra,  G2,4,5  1. National Center for Advancing Translational Sciences, National Institutes of Health, Bethesda, MD  2. Department of Chemistry, Purdue University, West Lafayette, IN  3. Department of Statistics, Purdue University, West Lafayette, IN  4. Department of Computer Science, Purdue University, West Lafayette, IN  5. Purdue Institute for Drug Discovery, Integrative Data Science Institute, Purdue Center for Cancer  Research, Purdue Institute for Inflammation, Immunology, and Infectious Disease, Purdue Institute for  Integrative Neuroscience    The integration of artificial intelligence (AI) agents to assist scientists in planning, executing, and analyzing  experiments is one of the core objectives of the ASPIRE initiative. A key milestone in this endeavor has  been connecting AI agents with modern automation and laboratory equipment, demonstrating that  rather than replacing human input, AI serves as a powerful tool to tackle some of the most complex  challenges in drug discovery programs.  One of our significant achievements has been the development of cross-communication between an AI  agent and a microwave reactor. By using text prompts, we successfully controlled the microwave reactor  via the AI agent, turning the AI into a kind of co-pilot for microwave synthesis. In this context, the AI  operates much like ChatGPT but for guiding microwave reactor experiments.  This integration allowed us to achieve two major objectives:  1. We successfully executed a benchmark reaction, N-alkylation, on the Biotage Initiator by inputting  commands in the form of text prompts through the AI co-pilot.   2. We conducted a reaction optimization on the Biotage Initiator+, where the AI agent employed an  optimization algorithm to improve percent conversion. The system intelligently searched for optimal  conditions, adjusting key parameters like reaction temperature, duration, reagent selection (such as bases  and catalysts), and solvent choice.   These advancements exemplify how AI can enhance experimental workflows, empowering scientists to  unlock new levels of efficiency and precision in chemical research.         NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P7  Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of  SARS-CoV-2  Wang, J1; Sra, A2; Weiss, JC1  1. National Library of Medicine, National Institutes of Health, Bethesda, MD  2. George Washington University, Washington, DC    The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC, pose a significant challenge  to healthcare systems worldwide. Accurate identification of progression events—such as hospitalization  and reinfection—is essential for effective patient management and resource allocation. However,  traditional models trained on structured data struggle to capture the nuanced progression of PASC. In this  study, we introduce the first publicly available cohort of 18 PASC patients, with text time series features  based on Large Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical expert. We  propose an Active Attention Network to predict the clinical risk and identify progression events related to  the risk. By integrating human expertise with active learning, we aim to enhance clinical risk prediction  accuracy and enable progression events identification with fewer number of annotation.  The ultimate  goal is to improves patient care and decision-making for  SARS-CoV-2 patient.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P8  Explainable machine learning for health disparities: type 2 diabetes in the All of Us  research program  Kambara, MS1; Chukka, O2; Choi, KJ1; Tsenum, J2; Gupta, S1; English, NJ1,3; Jordan, IK2,3; Mariño- Ramírez, L1  1. National Institute on Minority Health and Health Disparities, National Institutes of Health, Bethesda,  MD   2. School of Biological Sciences, Georgia Institute of Technology, Atlanta, GA  3. IHRC-Georgia Tech Applied Bioinformatics Laboratory, Atlanta, GA    Type 2 diabetes (T2D) is a disease with high morbidity and mortality and a disproportionate impact on  minority groups. Machine learning (ML) is increasingly used to characterize T2D risk factors; however, it  has not been used to study T2D health disparities. Our objective was to use explainable ML methods to  discover and characterize T2D health disparity risk factors. We applied SHapley Additive exPlanations  (SHAP), a new class of explainable ML methods that provide interpretability to ML classifiers, to this end.  ML classifiers were used to model T2D risk within and between self-identified race and ethnicity (SIRE)  groups, and SHAP values were calculated to quantify the effect of T2D risk factors. We then stratified  SHAP values by SIRE to quantify the effect of T2D risk factors on prevalence differences between groups.  We found that ML classifiers (random forest, lightGBM, and XGBoost) accurately modeled T2D risk and  recaptured the observed prevalence differences between SIRE groups. SHAP analysis showed the top  seven most important T2D risk factors for all SIRE groups were the same, with the order of importance for  features differing between groups. SHAP values stratified by SIRE showed that income, waist  circumference, and education best explain the higher prevalence of T2D in the Black or African American  group, compared to the White group, whereas income, education and triglycerides best explain the higher  prevalence of T2D in the Hispanic or Latino group. This study demonstrates that explainable ML can be  used to elucidate health disparity risk factors and quantify their group-specific effects.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P9  The role of morbidity clusters in midlife on stroke incidence and severity: The ARIC  study  Egle, M1; Groechel, RC1; Johansen, MC2; Kucharska-Newton, AM34; Gottesman, RF1; Koton, S56  1. National Institute of Neurological Disorders and Stroke, Intramural Research Program, National  Institutes of Health, Bethesda, MD  2. Department of Neurology, The Johns Hopkins University School of Medicine, Baltimore, MD  3. Department of Epidemiology, University of North Carolina at Chapel Hill Gillings School of Global  Public Health, Chapel Hill, NC  4. Department of Epidemiology, University of Kentucky, Lexington, KY  5. Department of Nursing, The Stanley Steyer School of Health Professions, Tel Aviv University, Tel Aviv,  Israel  6. Department of Epidemiology, Johns Hopkins University School of Public Health, Baltimore, MD    OBJECTIVE: Standardized scores summarizing clinical information have found application in risk  stratification but their ability to capture stroke-related risk factors in midlife and predict risk in biracial  populations remains less explored. This study employed a cluster analysis approach to group individuals  into clusters based on similar clinical profiles in midlife and assessed the clusters’ association with stroke  risk and severity in a community-based prospective cohort.  METHODS: Participants (N=15,404) without prevalent stroke from the Atherosclerosis Risk in  Communities (ARIC) study were included. An hierarchical clustering approach was used to allocate  participants into clusters based on clinical information. In Cox proportional hazard models, the association  of the clusters with overall ischemic stroke incidence was tested while accounting for age, sex, education,  and race-center.   RESULTS: Of 1424 incident ischemic strokes diagnosed from baseline to 2020, 1104 included NIHSS  grading. The cluster analysis identified 9 distinct midlife clusters in the population with the following  defining features: cluster 1 (relatively healthy); cluster 2 (smoking); cluster 3 (cancer); cluster 4 (peripheral  artery disease); cluster 5 (obesity, diabetes, hypertension, and hypertriglyceridemia); cluster 6 (coronary  heart disease); cluster 7 (atrial fibrillation); cluster 8 (heart failure); cluster 9 (renal dysfunction). Clusters  2-9, compared to cluster 1 were each associated with a greater stroke risk, with the greatest hazard ratio  (HR) for cluster 9 (HR(95%CI)= 3.00 (2.00,4.50)).   INTERPRETATION: The findings emphasizes the importance of morbidity clusters in midlife for stroke.  Cluster analysis may be a powerful tool when stratifying large diverse populations based on morbidity  burden.    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P10  Machine Learning to Identify Tissue-Specific Clinical Associations of Senescence  Signatures  Olinger, B1,2; Anerillas, C5; Herman, AB4; Tsitsipatis, D4; Banarjee, R1; Tanaka, T1; Candia, J1;  Walker, KA3; Simonsick, EM1; Gorospe, M4; Basisty, N1  1. Translational Gerontology Branch, National Institute on Aging, NIH, Baltimore, MD  2. Department of Biology, Johns Hopkins University, Baltimore, MD  3. Laboratory of Behavioral Neuroscience, National Institute on Aging, NIH, Baltimore, MD  4. Laboratory of Genetics and Genomics, National Institute on Aging, NIH, Baltimore, MD  5. Tissue and Organ Homeostasis Program, Centro de Biología Molecular Severo Ochoa (CBM), Consejo  Superior de Investigaciones Científicas (CSIC), Madrid, Spain    Purpose: Cellular senescence is a hallmark of aging, and a key contributor to age-related disease.  Senescence-Associated Proteins (SAPs), when measured in plasma, are promising biomarkers for  assessing senescence burden; however, senescent cells are numerous and heterogeneous by cell type and  the relative importance of SAPs originating from different tissues is unknown. This study leverages the  SenCat, a novel database of cell-type specific senescence signatures, to evaluate the clinical relevance of  tissue-specific senescence burden.  Methods: This study uses machine learning to investigate clinical associations of tissue-specific SAPs in  circulating plasma in two longitudinal studies, including 1275 individuals from the Baltimore Longitudinal  Study of Aging (BLSA) and 997 from the Italian InCHIANTI study. Tissue-specific SAPs were identified using  mass spectrometry from 15 cell types including preadipocytes, astrocytes, and PBMCs, among others.  Plasma levels of these tissue-specific senescence signatures were assessed for associations with a broad  range of clinical parameters including mobility and disease status.  Results: Tissue-specific senescence burden showed unique clinical associations that map to their  corresponding health domain. For example, renal senescence best associated with kidney disease and  lung senescence best predicted pulmonary disease. Additionally, a panel of SAP were identified as a high- impact panel that predicted many clinical traits across several health domains.   Conclusion: These findings demonstrate that health status can be modeled non-invasively and with higher  resolution than previously determined, and that senescence signatures can serve as biomarkers to inform  clinical studies.          NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P11  AI-based biomarker discovery in CLN3  Sun, S1; Do, AD2; Zhu, Q1  1. Division of Preclinical Innovation, National Center for Advancing Translational Sciences, Rockville, MD  2. Unit on Cellular Stress in Development and Diseases, Eunice Kennedy Shriver National Institute of  Child Health and Human Development, Bethesda, MD    CLN3, also known as juvenile neuronal ceroid lipofuscinosis, is a rare and progressive neurodegenerative  disorder characterized by the accumulation of lipopigments in the brain, leading to cognitive decline,  seizures, and vision loss. This devastating condition primarily affects children and young adults, with  symptoms typically appearing between the ages of 4 and 10. The pathogenesis of CLN3 disease involves  variants in the CLN3 gene, which encodes a protein of unclear function but is crucial for normal cellular  processes. Current treatments are largely symptomatic and supportive, including seizure management  and physical therapy, but there is no cure or disease-modifying therapy available. Biomarkers are critical  for understanding disease mechanisms, monitoring disease progression, and evaluating therapeutic  responses. In this study, we developed various machine learning models to systematically predict  potential novel proteins biologically relevant to CLN3, by analyzing proteomics and laboratory data  collected via a prospective CLN3 natural history study protocol (NCT03307304). To further examine the  biological mechanism of those predicted proteins to CLN3 disease, we performed two different  approaches, 1) conducting KEGG pathway enrichment analysis to obtain any enriched pathways  associated with CLN3; and 2) building Protein-Protein Interaction (PPI) network with the proteins from  those enriched pathways to identify hub proteins, such as EGFR, HIF1A, ACAN, and BSG, as biomarker  candidates based on calculated network measurements. The biological associations of the identified  biomarkers with CLN3 will be further evaluated via biological experiments.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P12  Path2Space: An AI approach for cancer biomarker discovery via histopathology inferred  spatial transcriptomics  Campagnolo, EM1; Shulman, ED1; Lodha, R1; Stemmer, A1; Jiang, P1; Caldas, C2; Knott, S3; Hoang,  DT1; Aldape, K4; Ruppin, E1  1. Cancer Data Science Laboratory, Center for Cancer Research, National Cancer Institute, Bethesda, MD  2. Department of Clinical Biochemistry and Institute of Metabolic Science, University of Cambridge,  Cambridge, UK  3. Department of Biomedical Sciences, Cedars-Sinai Medical Center, Los Angeles, CA  4. Laboratory of Pathology, Center for Cancer Research, National Cancer Institute, Bethesda, MD    Spatial transcriptomics (ST) assays are transforming our understanding of tumor heterogeneity by  enabling high-resolution, location-specific mapping of gene expression across the tumor  microenvironment. However, the high cost of these assays has limited their application to modest cohort  sizes, limiting their application in large-scale spatial biomarker discovery. Here we present Path2Space, a  deep learning approach that predicts spatial gene expression directly from histopathology slides. Trained  on substantial breast cancer ST data, it robustly predicts the spatial expression of over 4,300 genes in  independent validation cohorts, significantly outperforming 12 state-of-the-art ST prediction methods.  Path2Space accurately infers cell-type abundances in the tumor microenvironment (TME) based on the  inferred ST data. It characterizes the TME of ~1,100 TCGA breast tumors, identifying three new spatially- defined breast cancer subgroups with distinct survival rates. Notably, Path2Space H&E-inferred TME  landscapes enable more accurate predictions of patient response to both chemotherapy and trastuzumab  than those obtained by established sequencing-based biomarkers. By enabling spatial transcriptomic  profiling directly from widely available histopathology slides, Path2Space provides a scalable and cost- effective alternative to sequencing-based assays. It opens new avenues for large-cohort spatial biomarker  discovery, and facilitates clinically actionable insights into tumor biology, prognosis, and therapy  response.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P13  Stochasticity in cancer immunotherapy maps with the rarity of critical Spark T cells  Salazar-Cavazos, E1; Jia, D1; Missolo-Koussou, Y2,3; Kenet, AL1; Achar, S1,4; Dada, H1,4; Kondo, T5;  Krishnan, A1,4; Taylor, N5; Jiang, P6; Waterfall, J2; DeVoe DL7; Altan-Bonnet, G1  1. Immunodynamics Group, Laboratory of Integrative Cancer Immunology, Center for Cancer Research,  National Cancer Institute, Bethesda, MD   2. Inserm U830 and Translational Research Department, Institut Curie, PSL Research University, Paris,  France  3. Inserm U932, Institut Curie, PSL Research University, Paris, France  4. Kennedy Institute of Rheumatology, Nuffield Department of Orthopaedics, Rheumatology and  Musculoskeletal Sciences, University of Oxford, Oxford, UK  5. Pediatric Oncology Branch, Center for Cancer Research, National Cancer Institute, Bethesda, MD  6. Cancer Data Science Laboratory, Center for Cancer Research, National Cancer Institute, Bethesda, MD  7. Department of Mechanical Engineering, University of Maryland, College Park, MD    Cancer immunotherapies result in highly variable responses in different patients even when the treatment  is the same. Strikingly, some murine tumor models show large variability in the outcome of cancer  immunotherapies, even when the mice, tumor cells and anti-tumor immune cells injected into mice are  all genetically identical. Here, we sought to analyze this variability in adoptive cell therapies to identify  the immune cell subset driving this variability.   To quantify variability ex vivo, we extracted mouse TCR-transgenic CD8+ T cells, and co-cultured them  with antigen-expressing tumor cells in a high-throughput robotic system. We used multiplexed in vitro  assays and single-cell analysis of thousands of samples to analyze the inter-replicate variability of tumor  cell killing and immune activation. We identified conditions (e.g. cell numbers, antigen quality, tumor cell  types) where macroscopic large variations in the immune response against cancer cells are observed  between highly-controlled technical replicates. Stochastic activation of a rare subset of CD8+ T cells, so- called Spark T cells, coupled to a paracrine IFN-γ-driven positive feedback explains this measured “noise”  in immunotherapeutic reactions. We then developed a custom-designed machine-learning pipeline  (Stochasticity-based Identification of Cell Subsets a.k.a. StoICS) to identify the subset of immune cells  responsible for the immunotherapeutic variability. We applied StoICS to identify the Spark T cells in  murine naïve T cells, and in human TCR-engineered T cell blasts prepared as for adoptive T cell therapy.  We then show that diverse levels of Spark T cells in tumor samples explain variable outcomes in cancer  immunotherapies.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P14  Deep Learning reveals the significant contribution of silencer variants to human  diseases and traits  Huang, D1; Ovcharenko, I1  1. National Library of Medicine, National Institutes of Health, Bethesda, MD    Although disease-causal genetic variants have been found within silencer sequences, we still lack a  comprehensive analysis of the association of silencers with diseases. Here, we profiled 2.8 million silencers  across 97 human samples derived from a diverse panel of tissues and developmental time points, using  deep learning models.   These enhancers exhibit strong enrichment in disease-associated variants, which are comparable in  overrepresentation but differ in functional characteristics from their target genes compared to enhancers,  highlighting the distinguishing role of silencers in human health. For example, in neuronal biosamples,  Parkinson’s disease variants exhibit an average of over 2 times enrichment within silencers compared to  enhancers. The disruption of apoptosis in neuronal cells is associated with both schizophrenia and bipolar  disorder and can largely be attributed to variants within silencers, similar to the disruption of GABAergic  interneurons, a central factor in the onset of schizophrenia. Our model permits a mechanistic explanation  of causative SNP effects by identifying altered binding of tissue-specific repressors and activators,  validated with 70% of directional concordance between our predictions and SNP-SELEX experimental  quantification of transcription factor binding affinity.   In summary, our results indicate that advances in deep learning models for silencers enable the discovery  of disease-causal silencer mutations on a whole-genome scale, effectively 'doubling' the number of  functionally characterized GWAS variants. This provides a basis for explaining mechanisms of action and  designing novel diagnostics and therapeutic methods addressing dysregulated pathways with disrupted  silencers.           NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P15  Predicting piRNA cluster regions from genomic sequences using deep learning  Ahrend, F1,2; Meister, G2; Haase, AD1  1. National Institutes of Diabetes and Digestive and Kidney Diseases, National Institutes of Health,  Bethesda, MD  2. Regensburg Center for Biochemistry (RCB), Laboratory for RNA Biology, University of Regensburg,  Regensburg, Germany    piRNA clusters are discrete genomic regions that give rise to PIWI-interacting RNAs (piRNAs), a specialized  class of small non-coding RNAs essential for safeguarding germ cells against the mutagenic activity of  transposable elements. piRNA clusters serve as the information source for the small RNA defense system,  producing thousands of piRNAs that collectively recognize and silence genomic threats.   While their biological role is well established, the sequence features and targeting rules that govern piRNA  cluster activity remain poorly understood. This early-stage project aims to systematically decode these  patterns using machine learning. Leveraging annotated piRNA clusters across multiple mammalian  species, we plan to train convolutional neural networks (CNNs) to learn how factors like sequence  composition, piRNA read coverage, mismatch tolerance, and positional bias contribute to effective  silencing.   Our approach will use sigmoid-based output layers to predict not just the presence of clusters but their  relative strength – potentially revealing design principles of this selective RNA-guided system.   We are currently shaping the modeling pipeline and welcome feedback on CNN architectures, input  feature selection, and cross-species generalization strategies. We're especially eager to connect with  colleagues interested in transfer learning, genome annotation, or piRNA biology.     NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P16  Comparative Analysis of Deep Learning Models for Predicting Causative Regulatory  Variants  Manzo, G1; Borkowski, K1; Ovcharenko, I1  1. Computational Biology Branch, Division of Intramural Research, National Library of Medicine, National  Institutes of Health, Bethesda, MD    Motivation: Genome-wide association studies (GWAS) have identified many noncoding variants linked to  complex traits and diseases. However, distinguishing causal variants from those merely associated  remains a major challenge. A small subset of noncoding variants has true regulatory effects, which can  only be detected through accurate models assessing DNA sequence variation. Deep learning  approaches—especially Convolutional Neural Networks (CNNs) and transformer-based models—have  shown promise in predicting these effects, particularly in enhancers, by leveraging genomic and  epigenomic patterns. Yet, the absence of standardized benchmarks and consistent evaluation criteria  across studies makes model selection difficult.  Results: This study benchmarks cutting-edge deep learning models for predicting the regulatory effects of  genetic variants on enhancers. Using nine datasets derived from MPRA, raQTL, and eQTL experiments, we  assessed 54,859 SNPs across four human cell lines. Our findings show that CNN-based models, such as  TREDNet and SEI, consistently achieve high accuracy in predicting SNP effects. Meanwhile, hybrid CNN- transformer models like Borzoi excel at identifying causal variants within linkage disequilibrium blocks.  Although fine-tuning improves the performance of transformer models, they generally remain less  effective than CNN and hybrid models under optimized training conditions.    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P17  Regulatory plasticity of the human genome  Srivastava, J1; Ovcharenko, I1  1. Computational Biology Branch, National Library of Medicine, National Institutes of Health, Bethesda,  MD  Evolutionary turnover in unconstrained enhancers has driven phenotypic divergence during past  speciation events and continues to facilitate environmental adaptation through variants. Even closely  related species like humans and chimpanzees with ~98% genomic identity show significant differences in  morphology and cognition which are largely attributed to enhancer divergence, raising key questions:  What fraction of the genome undergoes regulatory turnover due to accumulating variants? Are certain  genes or transcription factor binding sites more prone to changes? We addressed these questions using a  deep learning model to identify substrates of regulatory turnover using genome wide mutations  mimicking three evolutionary pathways: recent history (human-chimp substitutions), modern population  (human population variation), and mutational susceptibility (random mutations). We observed that >80%  of the novel activity arises from repurposing of enhancers between cell-types. Turnover in a cell-type is  remarkably similar across the three pathways, despite only ∼19% overlap in the source regions. The  highest turnover occurring near neurodevelopmental genes including CNTNAP2, NPAS3, and AUTS2.  Flanking enhancers of these genes undergo high turnover irrespective of the mutational model pathway,  suggesting their intrinsic plasticity in cognitive evolution and recurrent roles in neuropsychiatric diseases.  Based on susceptibility to random mutations, these enhancers were identified as vulnerable by nature  and feature a higher abundance of cell type-specific TFBSs, whereas mutationally robust enhancers are  enriched for globally expressed general transcription regulators. Our findings suggest that while robust  enhancers contribute to the stability of core regulatory networks, enhancer repurposing within vulnerable  loci serve as hotspots of cell type- and species-specific regulatory innovation.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P18  DNA methylation differences in children of severe acute malnutrition suggest  epigenetic networks at play via machine learning  Li, Q1; Hanchard, N1  1. Childhood Complex Disease Genomics Section, National Human Genome Research Institute, National  Institutes of Health, Bethesda, MD    Childhood Severe Acute Malnutrition (SAM) poses serious health risk in children worldwide. The two  primary forms of SAM, the Kwashiorkor (ESAM) and the milder form Marasmus (NESAM) have distinct  differences in morbidity and mortality risk. However, those difference cannot be attributed to dietary nor  environmental factors alone. In our previous study of children with SAM from Jamaican (N=110) and  Malawi (N=191), we found strong evidence of lower DNA methylation at >800 CpG sites across the  genome in individuals with ESAM relative to those without. Here, we employed a random forest (RF)  approach to identify top CpG sites with the highest prediction scores, and correlated them with biological  evidence to hypothesize a network of genes that collectively, indicates an elevated risk of ESAM.  Given ~486,000 CpG sites, we tested various tuning parameters to allow for a deep search, and used  permutation test to rank the predictors. To account for population stratification and confounding  variables, we included covarying factors as fixed features besides randomly assigned features. Those  factors included sex, the principal components (PCs) estimated from genotypes and from DNA  methylation data. At average prediction error of ~.37 across various models, we found that XYLT1 on  chr16, MFAP2 and ZC3H12A on chr1, and SOX7 on chr8 as the top CpG predictors. We then searched the  literature using PubTator3  to find interactions between the top CpG sites and related diseases, in order  to identify potential biological networks. Our results are a promising foundation for employing a multi- omics machine learning model.      NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P19  Modeling cCREs using deep learning with applications to prioritizing candidate causal  mutations from GWAS data.  Hudaiberdiev, S1; Ovcharenko, I1  1. National Center for Biotechnology and Information, National Library of Medicine, National Institutes  of Health, Bethesda MD    Developmental stuttering is a speech disorder affecting 7-15% of children aged 2-4 and around 1% of adult  population. Varying heritability estimates of stuttering have been reported to range from 0.42 to 0.84 in  twin studies. The biggest evidence on the genetic components of stuttering came from familial genetic  studies. However, two different GWAS studies on stuttering did not produce hits within the significance  threshold level of p<1-E8 accepted in the field. In addition, these two GWAS studies resulted in completely  disjoint set of suggestive SNP sets associated with the trait, highlighting the heterogeneity and complex  nature of the phenotype. To aid in search for candidate causal mutations, we employed a method that we  have developed and successfully applied for analyzing Type 2 Diabetes GWAS data. First, we conducted  wide range of analyses for detecting implicated tissues and cell types, using the data from GTEx,  PsychENCODE and ENCODE Projects. We observed that stuttering-related mutations are active in wide  range of tissues, resulting in only 4 brain and neural cell types out of 23 significantly enriched cell types.  Crucially, the astrocyte of cerebellum were among the top enriched tissues, dysfunctions in which have  been shown to be correlated with stuttering. Using our model trained to recognize enhancers, we  prioritized SNPs from fine-mapped GWAS sets for each locus, and short-listed 11 GWAS SNPs in the  enhancer regions of astrocytes with increased likelihood affecting expression of genes previously linked  to stuttering.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P20  Variant Effect Predictions for PTPN11 Missense Variants with MutPred2  Um1; Mooney1  1. National Human Genome Research Institute, National Institutes of Health, Bethesda, MD    The PTPN11 gene encodes the Src homology 2 domain-containing protein tyrosine phosphatase (SHP2), a  key regulator of cell growth, differentiation, and apoptosis through its modulation of various signaling  pathways, including the RAS/ERK signaling pathway. Missense variants in PTPN11 disrupt SHP2's proper  catalytic activity and the regulation of signaling pathways, leading to conditions such as Noonan  syndrome, LEOPARD syndrome, or juvenile myelomonocytic leukemia. SHP2 variants have a wide  spectrum of molecular disruptions leading to both gains and losses of function at the molecular level as  well as gains and losses of function at the phenotypic level. While NS and JMML are associated with gain- of-function variants of SHP2, loss-of-function variants are thought to underlie LEOPARD syndrome. In this  study, we model the underlying causes of pathogenicity of missense variants in PTPN11 and compare gain  and loss of function variants that cause disease.     NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P21  Using Genomics Data and Literature for Basket Trial Design in Rare Diseases  Moon, S1; Maine J1; Mathe, E1; Zhu, Q1  1. Division of Pre-Clinical Innovation, National Center for Advancing Translational Sciences, Rockville, MD    Gaining insight into the underlying molecular etiologies of rare diseases can aid cross-disease research,  inform the design of basket trials, and identify drug repurposing opportunities. In our preliminary study,  we identified 36 rare disease clusters based on common genetic causes and biological mechanisms.  However, there clusters were too broad for basket trial applications. In this study, we refined these  clusters by collecting allelic variant data from the Online Mendelian Inheritance in Man (OMIM), along  with corresponding Sorting Intolerant From Tolerant (SIFT) scores for single nucleotide polymorphisms  (SNPs) and transcripts from Ensemble  validated from the Medical Genomics Japan Variant Database  (MGeND). We assessed the functional impact of gene mutations using SIFT scores, calculating the ratio of  deleterious to tolerated cases (deleterious cases / (deleterious cases + tolerated cases)). We generated  an matrix with imputed data by extracting the deleterious level of genetic and mutation data for each rare  disease, and identified shared mutations across diseases. Then, we applied Density-Based Spatial  Clustering of Applications with Noise (DBSCAN) to the imputed matrix, creating sub-clusters on the top of  the 36 clusters. Our results illustrate consistent findings with the published studies of basket trial design  for instance, a subcluster of NLRP3 mutation-related diseases including Neonatal Onset Multisystem  Inflammatory Disease, Familial Cold Autoinflammatory Syndrome, and Muckle-Wells Syndrome.    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P22  Donor-specific digital twin for living donor liver transplant recovery  Halder, S1; Periwal, V1  1. Laboratory of Biological Modeling, National Institutes of Diabetes and Digestive and Kidney Diseases,  National Institutes of Health, Bethesda, MD    The remarkable capacity of the liver to regenerate its lost mass after resection makes living donor liver  transplantation a successful treatment option. However, donor heterogeneity significantly influences  recovery trajectories, highlighting the need for individualized monitoring. With the rising incidence of liver  diseases, safer transplant procedures and improved donor care are urgently needed. Current clinical  markers provide only limited snapshots of recovery, making it challenging to predict long-term outcomes.  Following partial hepatectomy, precise liver mass recovery requires tightly regulated hepatocyte  proliferation. We identified distinct gene expression patterns associated with liver regeneration by  analyzing blood-derived gene expression measurements from twelve donors followed over a year using  weighted gene co-expression network analysis. Using a deep learning-based framework, we integrated  these patterns with a mathematical model of hepatocyte transitions to develop a Personalized Progressive  Mechanistic Digital Twin - a virtual liver model predicting patient- specific recovery trajectories. This  approach integrates clinical genomics and computational modeling to enhance post-surgical care,  ensuring safer transplants and improved donor recovery.    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P23  Identification of essential transcription factors by IAN: a new perspective on T cell  licensing  Shi, G1; Nagarajan, V1; Caspi, RR1  1. Laboratory of Immunology, National Eye Institute, National Institutes of Health, Bethesda, MD    The rapid accumulation of “Omics” data has revolutionized biological research, enabling researchers to  explore complex biological systems with unprecedented depth. In transcriptomic studies, analysis  typically involves generating a list of differentially expressed genes (DEGs) and then performing  enrichment analysis to gain an overall understanding of the system. Despite numerous enrichment  analysis tools and methods available, along with thousands of gene sets to select from, current methods  often produce biased results. IAN (Intelligent System for Omics Data Analysis) is an R package addressing  the challenge of integrating, analyzing, and interpreting high-throughput Omics data using a multi-agent  artificial intelligence (AI) system. We performed enrichment analysis on a publicly available dataset  (GSE38645) regarding T cell licensing, using IAN.  T cell “licensing” is the process by which circulating T  cells specific to brain or eye antigens transiently lodge in the spleen or lung, where they undergo gene- expression and functional reprogramming that enables them to migrate and cross the blood-brain or  blood-retinal barrier, and initiate autoimmune inflammation. Our findings indicate that  cytokine/chemokine signaling and cell adhesion molecules are positively associated with licensed T cells,  whereas cell cycling, metabolism, and protein processing are negatively correlated. Notably, three  transcription factors – FOXO1, MECOM, and JUN – act as key regulators, bridging those positively and  negatively related pathways. The roles of these transcription factors were further validated using single  sample gene set enrichment analysis (ssGSEA) in an independent publicly available dataset (GSE57098).  Collectively, these findings provide new insights into the molecular mechanisms underlying T cell licensing.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P24  WSIomics: An Automated Pipeline for Training Multimodal AI Models to Classify  therapy response of cancer patients using whole slide images and transcriptome data  Park, M1; Yan, C1; Chen, Q1; Khanna, R1; Tanis, J1; Meerzaman, D1  1. Center for Biomedical Informatics and Information Technology, National Cancer Institute, National  Institutes of Health, Bethesda, MD    Prediction of therapy responses in cancer patients is essential for personalized treatment strategies to  improve treatment outcomes. Multimodal AI models that integrate various data types, such as whole slide  images (WSIs), transcriptome data, genetic data, and clinical data, are reported to perform better than  single modalities. In this study, we have developed a fully automated pipeline to train multimodal AI  models to predict therapy responses of cancer patients. WSIs and transcriptome datasets were used to  train multimodal AI models. The WSI training pipeline was developed based on the CLAM pipeline, which  uses attention algorithms. Transcriptome and multimodal models were developed based on simple multi- layer perceptron models. For high-throughput AI model development, the marker genes for transcriptome  modality were automatically identified in the pipeline by analyzing the trendline between expression  values and progression-free interval values. The multimodal approach was applied to cohorts of eleven  cancer types in the TCGA database, which reported neo-adjuvant treatment. The models trained with  multimodal data revealed superior performance in seven of the 11 cancer types compared to single- modality models. This study underscores the importance of multimodal approaches in advancing precision  oncology and provides a foundational framework for further exploration of multimodal data integration  in cancer research.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P25  Combining Real and Synthetic Data to Overcome Limited Training Datasets in  Multimodal Learning   Marini, N1; Liang, Z1; Rajaraman, S1; Xue, Z1; Antani, S1  1. National Library of Medicine, National Institutes of Health, Bethesda, MD    Biomedical data are inherently multimodal capturing complementary aspects of patient health, where,  for example, images provide “low-level” visual features, while associated textual reports summarize  “high-level” diagnostic findings. Artificial intelligence (AI) algorithms that integrate multiple modalities  into a single data representation can significantly improve clinical decision-making. For example,  multimodal foundation models, that are generally trained unsupervised, can integrate information from  multiple data types and effectively perform a wide variety of tasks.   However, the development of reliable multimodal AI requires use of large training datasets with samples  from modalities of interest. Biomedical datasets tend to be unimodal, skewed, often including just basic  class labels. Case in point, consider various publicly available skin lesion data sets that lack of annotated  reports paired with the images.   The goal of this work is to present a multimodal architecture that encodes fine-grained text  representations within image embeddings to build a robust representation of skin lesion data, exploiting  real and synthetized data. Large language models (LLMs) are used to synthesize textual descriptions that  are  paired  with  the  original  skin  lesion  images  and  used  for  model  development.  The architecture is evaluated on three tasks: skin lesion image classification, multi-modal data retrieval,  and the linkages between visual and textual concepts. The latter two tasks are a consequence of  architectural design and do not need supervised training.  The proposed multimodal representation outperforms the unimodal one on the classification of skin  lesion images and allows the extraction of knowledge from datasets without the need for additional  annotations.          NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P26  Towards a better CAR  through in vitro and in silico Perturbations  Cordes, S1  1. Laboratory of Immune Regulatory Genomics and Epigenomics, Translational Stem Cell Biology Branch,  National Heart, Lung and Blood Institute, National Institutes of Health, Bethesda, MD    The durability of gene and cellular therapies depends on the persistence of adoptively transferred cells,  while their efficacy relies on mature effector function. Physiologically, cells rarely exhibit indefinite self- renewal concurrently with mature functionality. Thus, optimizing cellular therapies is inherently a  dynamical challenge.  Genetic knockouts of transcription factors and epigenetic modifiers broadly alter cellular function. In  contrast, perturbations of cis-regulatory elements (CREs) generally exert localized effects, providing more  precise control of cellular states. However, systematically screening numerous candidate CREs remains  experimentally impractical.  We developed a machine learning model, trained on single-cell multi-omics data, to predict cellular state  dynamics following CRE perturbations. The model prioritizes therapeutic interventions by computing  metrics directly relevant to the persistence and efficacy of adoptive cells.  Model predictions indicate that knocking out the transcription factor TCF7 enhances transitions from  naïve and memory T cells to effector states. Conversely, disruption of a candidate CRE near KLF10  (chr8:101,790,788-101,791,270) significantly promotes transitions into naïve and memory subsets,  potentially enhancing persistence.  Our computational approach efficiently prioritizes an experimentally intractable number of candidate  perturbations using therapeutically relevant metrics. This targeted method enhances experimental  feasibility and accelerates discovery.  Future work includes integrating additional single-cell multi-omic modalities, such as single-cell Hi-C, to  further elucidate regulatory dynamics. This integration promises to refine predictions and facilitate the  design of more effective, persistent cellular therapies.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P27  Optimizing CAR costimulatory domains using contrastive learning and optimal  transport on high-throughput screening data  Kelly, C1; Bahr, R2; Zhu, W2; Keyvanfar, K3; Dagur, P3; Cordes, S2  1. School of Medicine, Georgetown University, Washington, DC  2. Laboratory of Immune Regulatory Genomics and Epigenomics, Translational Stem Cell Biology Branch,  National Heart Lung and Blood Institute, National Institutes of Health, Bethesda, MD  3. Flow Cytometry Core, National Heart Lung and Blood Institute, National Institutes of Health,  Bethesda, MD    Chimeric antigen receptor (CAR) T cells have established themselves as therapies for B cell malignancies,  where they induce prolonged remission in most patients, though fewer than half achieve durable control  of their disease.  CARs follow a modular design, with an extracellular portion conferring target-specificity  and two or more intracellular signaling domains.  One of the signaling components, called the  costimulatory domain, is crucial for ensuring proper and controlled immune responses.  Absence of this  domain results in T cell anergy, but the optimal costimulatory domains remain unknown.  To gain a predictive understanding of immunophenotypes resulting from different costimulatory domains,  we trained a contrastive learning model to learn a shared embedding between ESM-2 representations  and our experimentally determined immunophenotypes under different co-culture conditions.  We  trained two dense neural networks to project to a shared embedding using 90% of our data, we achieved  a moderate Fraction of Samples Closer than the True Match (FOSCTTM) of 0.45 in our validation sample.   We further refined this shared embedding with fused Gromov-Wasserstein optimal transport to achieve  a respectable FOSCTTM of 0.13.   Integration of cellular therapy experimental results into a pre-trained PLM via transfer learning permits  identification of latent dimensions representing the complex patterns governing cellular behavior that  depend on costimulatory domain sequence. This insight allows interpolation within that latent space to  locations of desired immunophenotypes and then use any decoder to generate optimal synthetic  costimulatory domain sequences which induce such phenotypes. We plan arrayed testing of synthetic  domains predicted to optimize CAR T cell immunophenotype proportions.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P28  Deep learning cellular dynamics from single-cell RNA sequencing  Duan, X1; Periwal, V1  1. Lab of Biological Modeling, National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK),  National Institutes of Health, Bethesda, MD    Single-cell RNA sequencing (scRNA-seq) provides a powerful framework for studying cellular  heterogeneity, transitions, and regulatory networks. However, reconstructing the underlying dynamical  processes governing these transitions remains a major challenge due to the high-dimensional nature of  gene expression data. To address this, we develop a variational autoencoder (VAE)-based approach that  learns a low-dimensional latent representation of cellular states and models their temporal evolution. We  apply our framework to gene expression data from Drosophila melanogaster blastoderm embryos,  compiled by Fowlkes et al., which includes measurements across multiple time points using a registration  technique. In this approach, gene expression profiles are encoded into a low-dimensional latent space,  where we train a neural stochastic differential equation (SDE) network to capture the continuous  dynamics of latent states over developmental time. The learned neural SDE models the progression of  cellular states, and a decoder subsequently maps these evolving latent representations back to the  original high-dimensional gene expression space, allowing for both accurate reconstruction of observed  transcriptional patterns and insight into the underlying dynamical processes. Future directions include the  use of symbolic regression to extract dynamical models from the inferred trajectories.              NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P29  Predicting Chemical Toxicity by Applying a Hierarchical Bayesian Approach with Priors  to the Tox21 Assay Data  Zeng, W1; Yadaw, AS1; Mehta, K1; Sanjak, J2; Nguyen, D-T3; Huang, R1; Mathé, EA1  1. Division of Pre-Clinical Innovation, National Center for Advancing Translational Sciences, National  Institutes of Health, Rockville, MD  2. Booz Allen Hamilton, Bethesda, MD  3. Digital R&D Solutions, Pfizer, Inc., Maryland    In vitro testing for drug activity and toxicity experimentally is an expensive and time-consuming process  yet critical for identifying candidate treatments that will translate into the clinic. In silico activity and  toxicity predictions using machine learning can speed up the drug development process by generating  accurate predictions. Resulting models can also highlight the most important factors that contribute to  predicting outcomes. The cell viability assay has been commonly used as a counter screen for compound  cytotoxicity in high throughput screening assays. In this paper, the cell viability counter-screen data  generated from screening 50 in vitro assays against the “Tox21 10K library” which consists of 8,947 unique  compounds, each of which can be represented using 208 chemical descriptors, was used for the first time  to develop predictive models for in vitro cytotoxicity of small molecules. This library was tested on 13  different cell lines, comprising tumor type (primary, metastatic, normal), 11 different tissue types, cell  types (epithelial, lymphoblast, fibroblast, or epithelial-like), sex and organism (animal cell and human cell).  Predictions were assessed using state-of-the-art machine learning approaches, including logistic  regression, a naïve Bayes and a hierarchical Bayesian model. In our approach, we evaluated a)  improvements in prediction capacity between models within individual assays and across assays with and  without the use of biological meta information (gender & organism); b) the chemical descriptors that  contributed most to predictions. The resulting balanced accuracies range from 0.66 to 0.81, depending on  the assay and model used. Logistic regression and hierarchical Bayesian models resulted in similar  balanced accuracies, and considering sex and organism in the models did not substantially improve model  predictions. Overall, our results demonstrate the utility of the Tox21 in vitro assay data in predicting in  vitro cytotoxicity.  These predictive models are critical for prioritizing chemicals as drug candidates for  further clinical evaluation.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P30  Interpretable Drug Response and Drug-Target Interaction Prediction Using Artificial  Intelligence  Inoue, YI1,2,3; Song, TS1; Fu, TF4; Luna, AL2,3  1. Computer Science, University of Minnesota, Minneapolis, MN  2. Computational Biology Branch, National Library of Medicine, National Institutes of Health, Bethesda,  MD  3. Developmental Therapeutics Branch, National Cancer Institute, National Institutes of Health,  Bethesda, MD  4. Department of Computer Science, Nanjing University Nanjing, Jiangsu, China    A challenge of using machine learning (ML) in biomedical research is a lack of interpretability, which limits  its support of data-driven decisions with explanations. We explore this topic here, focusing on cancer drug  response and mechanism prediction. We introduce two components: GraphPINE (Graph Propagating  Importance Network for Explanation) and DrugAgent. GraphPINE is a graph neural network (GNN) model  for drug response prediction using multi-omics data (e.g., gene expression) and interaction networks (e.g.,  protein-protein). The novelty of GraphPINE lies in its initialization of importance scores using biological  prior knowledge (drug-target interactions, DTIs) from literature and a dynamic updating mechanism. We  build on concepts from LSTM (Long Short-Term Memory), relying on previous predictions as hidden states  to advance GNNs such that GraphPINE initializes importance scores using prior knowledge and updates  these scores during model training. We apply GraphPINE to NCI60 data; GraphPINE achieves AUROC of  0.796 and AUPRC of 0.894 for 952 drugs. Separately, we developed DrugAgent, a multi-agent system  integrating knowledge graphs, internet searches, ML methods, and large language models (LLMs) to  improve DTI prediction. DrugAgent was evaluated using 178 kinase inhibitors against 300 kinases;  DrugAgent achieves superior performance (AUROC: 0.905 and AUPRC: 0.529). Interpretable subgraphs  accompany GraphPINE results, while DrugAgent results are enriched with prior knowledge. Multiple lines  of evidence must support conclusions in biomedical research. The bioinformatics efforts here build on this  fundamental notion to draw in additional data from heterogeneous sources uniformly and transparently  as part of ensemble results presented to users.               NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P31  Advancing antidepressant discovery through machine learning-based QSAR modelling  and insights from SHAP features  Oyinloye, P1; Wu, F1; Lee, KH1; Shi, L1  1. Computational Chemistry and Molecular Biophysics Section, Molecular Targets and Medications  Discovery Branch, National Institute on Drug Abuse, National Institutes of Health, Baltimore, MD    The serotonin transporter (SERT), a member of the neurotransmitter sodium symporter family, is essential  for regulating extracellular serotonin levels in the brain, influencing mood, emotion, motivation, and  memory. By regulating serotonin availability, SERT not only contributes to the therapeutic effects of  certain antidepressants but also holds potential for mitigating opioid abuse. In contrast, several inhibitors  of the dopamine transporter (DAT), which shares significant homology with SERT, are widely abused  psychostimulants. In this study, we conducted a comprehensive data extraction from ChEMBL and  DrugBank to retrieve and filter compounds with significant affinities for both SERT and DAT. We found  that antidepressant drugs are typically associated with higher affinities in SERT and lower affinities in DAT.  We further assembled datasets of both SERT and DAT ligands from ChEMBL to build machine learning- based Quantitative Structure-Activity Relationship (QSAR) models. Additionally, selectivity models were  developed for SERT in comparison to DAT. These models showed robust predictive performance in  predicting the affinity at SERT, DAT, and the selectivity for SERT. To further interpret the outputs of these  models, we applied SHapley Additive exPlanations (SHAP) to identify key chemical features that  distinguish SERT-selective from DAT-selective compounds. This research enhances our understanding of  the structure-activity relationships of SERT ligands and lays the foundation for the rational design of the  SERT selective ligands with reduced abuse liability.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P32  AI-driven development of ALDH3A1 selective inhibitors  Jain, SJ1; Yasgar, AY1; Nilova, AN1; Dalal, AD1; Rai, GR1; Zakharov, AZ1  1. National Center for Advancing Translational Sciences, National Institutes of Health,  Rockville, MD    Human Aldehyde Dehydrogenase 3A1 (ALDH3A1), a member of the ALDH enzyme family, plays a critical  role in metabolizing aliphatic and aromatic aldehydes, protecting cells from oxidative stress, and  maintaining homeostasis. Emerging evidence highlights its elevated expression in cancer stem cells, where  it may promote survival through oxidation of lipid-derived aldehydes and support the tumor  microenvironment. Therefore, pharmacological inhibition of ALDH3A1 represents a promising therapeutic  strategy in oncology. In this study, we present a computationally driven hit-to-lead workflow combining  reaction-based enumeration, molecular docking, and AI/ML models to identify and optimize potent  ALDH3A1 inhibitors. A structure-based virtual screen of our internal compound library yielded 47 active  compounds from 255 virtual hits (hit rate ~18%). Several chemotypes were further validated using the  AldeFluor cell assay, leading to the selection of two series for medicinal chemistry optimization. To expand  the chemical space around these hits, we performed reaction-based enumeration using the Enamine  building block collection (>1 million BBs), generating ~40,000 virtual analogs. These were prioritized using  a deep learning model trained on in-house ALDH3A1 data. Over 100 compounds were synthesized and  experimentally tested, resulting in several inhibitors with sub-100 nM potency. This study demonstrates  the utility of integrating in-silico reaction-based enumeration with AI-driven prioritization to accelerate  the discovery of potent ALDH3A1 inhibitors. The identified compounds serve as promising chemical  probes and potential leads for therapeutic development targeting cancer-related ALDH3A1 activity.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P33  In silico ADME models in drug discovery  Shah, P1; Weber, C1; Lim, G1; Zhao, T1; Sun, H1; Jain, S1; Zakharov, A1; Siramshetty, V1; Mathe, E1;  Xu, T1; Huang, R1; Xu, X1  1. Division of Preclinical Innovation, National Center for Advancing Translational Sciences,  Rockville, MD    Drug discovery and development is a long, expensive venture and the cost of bringing a new drug  candidate into the market has increased steadily in the past few decades. It costs between 1.6 and 2.8  billion US$ and takes between 10 and 15 years. While thousands of compounds are screened in preclinical  discovery, it is estimated that only 10 out of 1000 screened compounds ever become optimized leads that  progress into preclinical in vivo testing. Additionally, the clinical attrition rate is also extremely high with  <10% candidates reaching the market after entering clinical development. A central piece of the drug  discovery and development puzzle includes absorption, distribution, metabolism, and elimination (ADME)  studies. This is highlighted by the fact that in the past, >40% of drug candidates failed due to poor  pharmacokinetics.  While Pharma companies have large databases and computational models on the in  vitro ADME properties, these models are not readily available to all drug discovery research groups.  To  support drug discovery efforts at NCATS, we have collected in vitro ADME data on “drug-like” properties  for over 30K compounds synthesized by MedChem scientists.  These datasets cover a broad chemical  space from over 200 NCATS projects and are used to build the in silico ADME models which can be used  to guide the design of new molecules in drug discovery.  This work introduces ADME@NCATS, an in silico  ADME prediction platform developed at NCATS, along with a curated list of open-access ADME tools.          NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P34  ClinIQLink: A Neuro-Symbolic Pipeline for QA generation with Crowd-Sourced Human- in-the-Loop Verification  Colelough, BC1,2; Bartels, D1; Demner-Fushman, D1  1. National Library of Medicine, National Institutes of Health, Bethesda, MD  2. University of Maryland, College Park, MD    The rapid advancements in the area of generative large language models (LLMs) have introduced new  opportunities for the automation of question-answer (QA) dataset generation, particularly in high-risk  application domains such as medicine. However, the issue of maintaining factual accuracy and making the  process of knowledge recall transparent remains open. In this work, we present ClinIQLink: A Neuro- Symbolic Pipeline for QA Generation with Crowd-Sourced Human-in-the-Loop Verification, an automated  framework to generate a novel medical QA dataset from subject-matter expert source literature with  explicit linkages to the underlying references. We integrate an open-source LLM (LLaMA 3.3 - 70B) into a  neuro-symbolic pipeline to structure, extract, and generate atomic QA pairs from medical texts. To  enhance the validity of these generated QA pairs, we employed a crowd-sourced human validation  process using volunteer physicians from the NIH and US medical schools. The human annotators assessed  the factuality and relevance of the generated QA pairs through an interactive web-based interface. To  ensure maximum participation and maintain a high level of focus while evaluating, we incorporated  gamification elements in the interface design. Our results demonstrate that this hybrid neuro-symbolic  and human-in-the-loop approach effectively realizes automation effectiveness and expert validation with  the result of high-quality, transparent, and verifiable medical QA data. Our work advances the research  toward improved factual grounding of LLM-produced medical content to ensure that AI-based knowledge  retrieval complies with the standards of medical accuracy and credibility.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P35  Scientific Review NLP Conflict of Interest Identification  Mollerus, P1; Seideman, J1; Saraiya, D1; Meyer, A1; Footer, K1; Chang, R1; Nguyen, L1; Croghan,  J1; Rosenthal, A1; Klinkenberg, L2; Meyers, J2  1. Office of Cyber Infrastructure and Computational Biology, National Institute of Allergy and Infectious  Diseases, National Institutes of Health, Bethesda, MD   2. Scientific Review Program, Division of Extramural Activities, National Institute of Allergy and Infectious  Diseases, National Institutes of Health, Bethesda, MD    In the scientific grant review lifecycle, ensuring a transparent and unbiased evaluation is paramount.  Scientific Review Officers (SROs) must meticulously identify potential conflicts of interest (COI) between  grant applicants and reviewers, a task that typically necessitates manually sifting through all grant  applications to find relevant names. This process is both time-consuming and susceptible to human error.  To address this challenge, we developed a machine learning-based solution that leverages natural  language processing (NLP) to facilitate identification of potential COI in grant applications. The COI NLP  module employs named entity recognition (NER), entity resolution, and optical character recognition  (OCR) algorithms to process, identify, and extract names from machine-readable documents and scanned  images, with the capacity to process scanned supporting documents.  The solution was carefully tuned to  err on the side of more false positives rather than miss any actual entities. This COI NLP solution is  integrated into a custom, enterprise-wide web application – the Scientific Review Data Management  System (SRDMS) – with over 400 users across 23 different NIH ICs. By using NLP to identify potential COI  in each grant application, this solution saves SROs an average of 45 minutes of manual review time per  application and significantly enhances the efficiency, reliability, and fairness of the grant review process.          NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P36  Supervised Machine Learning for Scientific Coding Assistance  Seideman, J1; Do, W1; Tembo, M1; Opsahl-Ong, L1; Meyer, A1; Saraiya, D1; Footer, K1; Desai, A2;  Lee, L2; Nguyen, L1; Croghan, J1; Rosenthal, A1; Tartakovsky, M1  1. Office of Cyber Infrastructure and Computational Biology, National Institute of Allergy and Infectious  Diseases, National Institutes of Health, Bethesda, MD  2. Office of Strategic Planning, Initiative Development, and Analysis, National Institute of Allergy and  Infectious Diseases, National Institutes of Health, Bethesda, MD    The NIAID Office of Strategic Planning, Initiative Development, and Analysis (OSPIDA) assigns scientific  codes to extramural grants to enable detailed financial reporting of funding by pathogen, disease,  immunology, and other categories. This reporting is needed for congressional inquiries, data requests,  and interagency decision making. Assigning codes accurately is imperative to support these activities.  Currently, this process involves manual curation and classification of scientific content for thousands of  grants each year. To assist with this process, we developed the Coding Assistant Tool (CAT) – a supervised  machine learning solution that ingests grant application text and returns recommendations for scientific  codes. CAT leverages natural language processing (NLP) through a multi-layer perceptron neural network  trained on years of prior grant application text and OSPIDA-assigned scientific codes. Testing  demonstrated that, after model training, CAT predicts infectious disease codes with accuracy, recall, and  precision above 85%. CAT scientific code recommendations are currently being integrated into an existing  custom web application, for OSPIDA staff to leverage more seamlessly. The implementation of CAT is  saving OSPIDA staff time and effort and enhancing efficiency and consistency in the scientific coding  process.    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P37  AI helped me write this: using AI to analyze NIH's AI and data science grant portfolio  Piatkowski, GS1  1. Obstetric and Pediatric Pharmacology and Therapeutics Branch, Eunice Kennedy Shriver National  Institute of Child Health and Human Development, National Institutes of Health, Bethesda, MD    Artificial intelligence (AI) and data science are rapidly transforming biomedical research. NIH’s investment  in these areas has grown significantly, but identifying relevant grants across all Institutes and Centers  remains challenging due to inconsistent terminology and limitations in existing classification methods. In  this study, we evaluate and compare multiple approaches to identify data science and AI-related grants  funded by NIH from FY2007 to FY2024.  We first apply standard filters using RCDC categories (e.g., “Artificial Intelligence”) and keyword heuristics  (e.g., “machine learning,” “algorithm”) to extract baseline sets. To improve accuracy and capture projects  that may be missed or misclassified, we test large language models (LLMs) including GPT-4 and open- source LLaMA models. Each model classifies whether a project is AI/data science–related based on Title,  Abstract, and Project Terms, and optionally provides subcategories (e.g., NLP, imaging, predictive  modeling) and rationale.  Preliminary findings suggest LLMs improve classification precision over keyword-only methods, especially  in edge cases or when terminology is vague. The approach also enables flexible classification—for  example, distinguishing when AI is central to a project vs. peripherally mentioned. Aggregated results will  be used to analyze funding trends over time, by Institute, and by activity code and show how NIH’s data  science or AI portfolio has changed over time.  This work demonstrates how combining rule-based filters with generative AI can enhance portfolio  analysis and support more nuanced tracking of NIH investments in emerging technologies.                    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P38  Automating conversion of hand-drawn SBGN diagrams to SBGNML using large  language models  Balci, H1; Luna, A1,2  1. Computational Biology Branch, National Library of Medicine, National Institutes of Health, Bethesda,  MD  2. Developmental Therapeutics Branch, Center for Cancer Research, National Cancer Institute, National  Institutes of Health, Bethesda, MD    In systems biology, researchers often use diagrams to map how biological processes interact. These  diagrams can be created using visual languages such as Systems Biology Graphical Notation (SBGN), which  standardizes the representation of biological pathways, with Systems Biology Graphical Notation Markup  Language (SBGNML) as the file format for sharing them. Using machine-readable formats is key for  enabling data reuse and computational analysis. However, existing software tools for creating SBGN  diagrams are often difficult to use for first-time users, due to complex toolbars. Additionally, converting  hand-drawn SBGN diagrams into SBGNML after brainstorms is time-consuming. This research investigates  large language models (LLMs) to automate this conversion process, focusing on GPT-4o and Gemini 1.5  Pro models. We curated a crowdsourced dataset of 1,000 hand-drawn SBGN images, evenly split between  two common representations: Process Description (PD), detailing step-by-step biological events, and  Activity Flow (AF), showing the influence of one activity on another. We evaluate model performances in  extracting node types, edge connections, and labels. Preliminary results show strong performance in node  and label conversion, but edge conversion remains challenging. Gemini 1.5 Pro achieved up to 85.9%  accuracy in node conversion and 92% in label extraction for PD, with GPT-4o slightly behind. Edge  conversion remains less accurate, with 53% as the highest. We observed similar trends in AF diagrams.  This work can enhance SBGNML accessibility by automating diagram digitization and could extend to other  fields like engineering and software design, highlighting LLMs’ potential for converting conceptual  sketches into machine-readable formats.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P39  Cell phenotypes in the biomedical literature: First look at a new corpus  Rotenberg, NH1; Leaman, R1; Islamaj, R1; Fluharty, B1; Kuivaniemi, H1; Richardson, S1; Tromp,  G1,2; Lu, Z1; Scheuermann, RH1  1. Division of Intramural Research, National Library of Medicine (NLM), National Institutes of Health,  Bethesda, MD  2. Division of Immunology, Department of Biomedical Sciences, Biomedical Research Institute, Faculty of  Medicine and Health Sciences, Stellenbosch University, Cape Town, South Africa    Single-cell technologies are enabling the discovery of many novel cell phenotypes, but this growing body  of knowledge remains fragmented across the scientific literature. While natural language processing (NLP)  offers a promising approach to extract this information at scale, the current annotated datasets required  for NLP system development and evaluation do not reflect the complex assortment of cell phenotypes  described in recent studies.  We present a new corpus of excerpts from recent articles, manually annotated with mentions of human  and mouse cell phenotypes. The corpus distinguishes three types: (1) specific cell phenotypes (cell types  and states), (2) heterogenous cell populations, and (3) vague cell population descriptions. Mentions of the  first two types were linked to Cell Ontology identifiers where possible, using their meaning in context,  with matches labeled as exact or related. Annotation was performed by four cell biologists using a multi- round process, with automated pre-annotation and extensive quality control.  The corpus contains over 22,000 annotations across more than 3,000 passages selected from 2,700  articles, covering nearly half the concepts in the current Cell Ontology. Fine-tuning BiomedBERT in a  simplified named entity recognition task on this corpus resulted in substantially higher performance than  the same configuration fine-tuned on previously annotated datasets.  This corpus is a valuable resource for developing automated systems to identify cell phenotype mentions  in the biomedical literature and a foundation for the future extraction of relationships between cell types  and key biomedical entities, including genes, anatomical structures, and diseases.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P40  Automated Survey Collection with LLM-based Conversational Agents  Kaiyrbekov, K1; Dobbins, NJ2; Mooney, S1  1. Cyberinfrastructure and Artificial Intelligence Platforms Section, Center for Genomics and Data  Science Research, National Human Genome Research Institute, National Institutes of Health, Bethesda,  MD   2. Biomedical Informatics & Data Science, Department of Medicine, Johns Hopkins University, Baltimore,  MD    Objective: Phone surveys are crucial for collecting health data but are expensive, time-consuming, and  difficult to scale. To overcome these limitations, we propose a survey collection approach powered by  conversational Large Language Models (LLMs).  Materials and Methods: Our framework leverages an LLM-powered conversational agent to conduct  surveys and transcribe conversations, along with an LLM (GPT-4o) to extract responses from the  transcripts. We evaluated the framework’s performance by analyzing transcription errors, the accuracy of  inferred survey responses, and participant experiences across 40 surveys.  Results: GPT-4o extracted responses to survey questions with an average accuracy of 98%, despite an  average transcription word error rate of 7.7%. Participants reported occasional errors by the  conversational agent but praised its ability to demonstrate comprehension and maintain engaging  conversations.  Conclusions: Our study showcases the potential of LLM agents to enable scalable, AI-powered phone  surveys, reducing human effort and advancing healthcare data collection.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P41  Author and affiliated institution extraction from free-form letters using GenAI  Ornek, ME1; Zahnen, CR1; Chen, M-C1  1. Division of Planning, Analysis, and Information Management, Center for Scientific Review, National  Institutes of Health, Bethesda, MD    Efficient and effective processing of Letters of Support (LoS) in NIH grant applications is critical for  ensuring compliance and mitigating potential conflicts of interest during the peer review process. This  project introduces an AI-assisted approach that leverages GPT-4o to automate the identification and  extraction of authors and their institutional affiliations from LoS documents.  The workflow begins by preprocessing each combined PDF submission into image and text pages to enable  targeted input for GPT-4o. The automation then proceeds in two key stages. First, GPT-4o is prompted to  analyze the image pages to determine the start and end of each individual LoS, isolating them from multi- letter files. Next, it is prompted to interpret the corresponding letter text to extract author names and  institutional affiliations. Traditional Natural Language Processing (NLP) methods showed limited  effectiveness due to the unstructured nature and formatting variability of LoS documents. In contrast,  utilizing GPT-4o demonstrated significantly improved performance in handling these complexities.  By automating detection and extraction, this approach reduces manual effort required to screen LoS  submissions, minimizes human error, and improves consistency in data handling. The resulting accuracy  and repeatability enhance administrative efficiency and support a more transparent and reliable grant  review process. This project highlights the transformative potential of generative AI tools in biomedical  research administration, enabling streamlined workflows and reinforcing the integrity of peer review  operations.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P42  AI in action at the NICHD: Case studies and developmental pathways  Heymann, D1; Mykins, M1; Zhou, N1  1. Referral and Program Analysis Branch, National Institute of Child Health and Human Development,  National Institutes of Health, Bethesda, MD    The Referral and Program Analysis Branch (RPAB) at the NICHD is leveraging advanced data automation  solutions to solve complex problems, streamline processes, and enhance operational efficiency to benefit  the broader NICHD community. One of RPAB’s responsibilities is to triage incoming referrals and assign  them to the appropriate scientific branch. This process is challenging due to the time, and expertise  needed to navigate the complexity of overlapping scientific interests across branches.  To overcome these challenges, we developed and implemented an AI/ML Application Referral System   that augments and increases efficiency of the grant referral process. The versatility and adaptability of  our model also permitted us to repurpose our model framework for other use cases to 1) increase the  efficiency of NICHD’s study branch section assignment, and 2) handle mission critical items with short  turnarounds. To complement our AI/ML models, we are utilizing NIH LLMs, such as ChatBot for Intramural  Research Program (ChIRP), for additional refinement of our model predictions.   Throughout our development process we continuously integrate human-in-the-loop feedback from our  SMEs to refine and validate model outputs. Our approach of continuously integrating human feedback in  our DevOps allows us to deliver tangible benefits such as expedited turnaround, enhanced accuracy,  reduction in manual errors and administrative burden, and real-time insights for improved decision- making for our stakeholders. The system is designed to adapt to evolving scientific landscapes and  changes in NICHD research priorities. Our RPAB cloud environment structure ensures flexibility and  scalability to meet future requirements and expand as needed.    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P43  RARe-SOURCE Literature AI: Rare Disease Genotype-Phenotype Associations from  Biomedical Literature.  Alodadi, M1; Lyons, E1; Che, A1; Watson, D1; Tawa, GJ2; Porter, F3; Haugabook, SJ2; Ottinger, E2;  Mudunuri, U1  1. Advanced Biomedical Computational Science, Frederick National Laboratory for Cancer Research,  Frederick, MD  2. Therapeutic Development Branch, Division of Preclinical Innovation, National Center for Advancing  Translational Sciences, National Institutes of Health, Bethesda, MD  3. Division of Translational Research, Eunice Kennedy Shriver National Institute of Child Health and  Human Development, National Institutes of Health, Bethesda, MD    Rare diseases affect millions globally, but information and resources are often scarce [1]. To address this  need, the Therapeutic Development Branch in the intramural Division of Preclinical Innovation at NCATS  conceptualized Rare-SOURCETM and in collaboration with the Advanced Biomedical and Computational  Science developed and launched this user-centric bioinformatic resource platform for rare disease  information. The main objective is to facilitate data mining through a searchable interface that integrates  bioinformatics databases and enables users to navigate disease-gene-variant information quickly and  efficiently.   Determining genotype-phenotype connections is essential in preclinical and clinical research  environments, as it helps in fully understanding the relationship between genetic variations and disease  susceptibility. Biomedical literature is a knowledge source with valuable information. However, as the  data is largely unstructured, extracting relevant details is challenging, and mining for contextual  information such as variant pathogenicity [3], clinical and phenotypic details, and their relations to the  disease are not yet resolved.  Advancements in BioNLP and text-mining, powered by machine learning and transformer models like  BERT, have significantly improved information extraction from biomedical texts, advancing named-entity  recognition, relation extraction, and document classification [4-12]. RARe-SOURCETM aims to make rare  disease literature scalable, disease-agnostic and accessible through its LiteratureAI feature, which  leverages NLP to scan titles and abstracts for disease and gene mentions while also integrating other  resources for synonyms and aliases. Future enhancements will incorporate GenAI to extract genetic  variants along with their functional impact and clinical significance by accurately capturing clinical context,  potentially transforming variant pathogenicity predictions for the rare disease community.            NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P44  TrialGPT: matching patients to clinical trials with large language models  Jin, Q1; Wang, Z2; Floudas, CS3; Wan, N1; Chan, J1; Chen, F4; Gong, C5; Bracken-Clarke, D3; Xue,  E3; Fang, Y1; Tian, S1; Yang, Y1; Sun, J2; Lu, Z1  1. National Library of Medicine, National Institutes of Health, Bethesda, MD  2. University of Illinois Urbana-Champaign, Champaign, IL  3. National Cancer Institute, National Institutes of Health, Bethesda, MD   4. University of Pittsburgh, Pittsburgh, PA  5. Albert Einstein College of Medicine, Bronx, NY    Introduction  Trial recruitment is a persistent challenge in clinical research, hindered by increasingly complex patient  data and eligibility criteria. Traditionally, automatic trial matching relies either on embedding-based  techniques—demanding training data and often lacking interpretability—or structuring-based methods– –transforming criteria into structured queries. Such approaches can be resource-intensive, necessitating  a flexible, explainable solution that increases recruitment efficiency.  Methods  We present TrialGPT,1 a framework leveraging large language models (LLMs) for streamlined patient-to- trial matching. TrialGPT contains three modules: (1) TrialGPT-Retrieval, which uses LLM-generated  keywords and hybrid lexical-semantic search to filter trials from databases like ClinicalTrials.gov; (2)  TrialGPT-Matching, which analyzes shortlisted patient–trial pairs, providing explanations and citing  patient-level sentences on a criterion-by-criterion basis; and (3) TrialGPT-Ranking, which integrates  criterion-level predictions, ranking trials by the extent to which criteria are met. Notably, TrialGPT- Retrieval can be bypassed when trial search spaces are relatively small, such as hundreds of institution- specific trials.  Results and Conclusion  On three publicly available cohorts of 183 synthetic patients with over 75,000 trial eligibility annotations  from TREC challenges,2 TrialGPT-Retrieval recalls over 90% of relevant trials using less than 6% of the  initial collection. Manual evaluations on 1,015 patient-criterion pairs show that TrialGPT-Matching  achieves an accuracy of 87.3% with faithful explanations, near expert performance of 88.7%-90.0%. The  TrialGPT-Ranking scores are highly correlated with human judgments and outperform the best-competing  models by 29.8% in ranking trials. Furthermore, our pilot user study reveals that TrialGPT can reduce  screening time by 42.6% in recruitment. Overall, these results demonstrate promising opportunities for  patient-to-trial matching with TrialGPT.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P45  A Dataset for Grounded Question Answering from Electronic Health Records to Relieve  Clinician Burden  Soni, SS1; Demner-Fushman, DD1  1. Division of Intramural Research, National Library of Medicine, National Institutes of Health, Bethesda,  MD    Patient requests for medical information via patient portals are rising, contributing significantly to desktop  medicine and clinician burden. An approach to handling the increasing messaging burden is to assist  clinicians in formulating the responses. Thus, automatically generating answers to patients’ questions  considering their electronic health records (EHRs) is important.  We introduce a novel dataset to develop and evaluate systems that answer patients’ questions using  clinical evidence from EHRs. It comprises hand-curated patient questions (reflective of portal messages),  clinician-identified focus areas in questions, clinician-rewritten questions (to aid in formulating  responses), and clinical note excerpts providing context (from MIMIC-III and IV databases). Each sentence  in the note excerpt is manually annotated to mark its importance in answering the question as ""essential""  (must use), ""supplementary"" (may provide support), or ""not-relevant"". We evaluate system-generated  responses on “Factuality”—measured by F1 scores of cited evidence under strict (considers only  “essential” sentences as answers) and lenient (considers both essential and supplementary) criteria—and  “Relevance”—assessed by comparing answers to “essential” evidence and the original question.  The dataset contains 130 questions, with a mean note excerpt length of 18.2 (sd-12.1) sentences— including 5.7 (sd-4.0) essential, 1.9 (sd-2.9) supplementary, and 10.6 (sd-8.2) not-relevant sentences. The  baseline model, LLaMa 3.3 70B, achieved Factuality F1 of 55.6 (strict) and 57.3 (lenient), and Relevance  scores: ROUGE (19.0), SARI (53.5), BERTScore (83.8), AlignScore (52.2), and MEDCON (26.4). This  underscores the challenging nature of the dataset, offering a robust benchmark for automated patient  messaging systems leveraging EHR data.        NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P46  Forecasting from Clinical Textual Time Series: Adaptations of the BERT and Decoder  Families  Kumar, SK1; Noroozizadeh, SN1; Weiss, JCW1  1. Computational Health Research Branch, National Library of Medicine, National Institutes of Health,  Bethesda, MD    Clinical case reports document patient trajectories for communication of findings and best practices  across disciplines. While they contain a richer set of information than that of tabular data streams in  Electronic Health Records (EHRs), they lack the regularity of inputs and outputs that classical machine  learning algorithms typically use. To use these patient trajectories in their textual form, we propose the  forecasting task from textual time series, where the inputs are timestamped clinical findings. We define  evaluation measures appropriate for time-ordered text setup and test a large suite of large language  models from both the BERT and decoder model families. We find that finetuned decoder based models  perform the best at forecasting in the near-horizon. We further demonstrate the importance of time  ordering, which requires clinical time series construction, as compared to text ordering, the format of the  text inputs that LLMs are classically trained on. This highlights the additional benefit that can be  ascertained from time-ordered corpora, with implications for temporal tasks in the era of widespread LLM  use.         NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P47  Responsible Integration of Large Language Models in Biomedical Research  Aston, SA1; Cheng, H1  1. Data Science Core, National Institute on Deafness and Communication Disorders, National Institutes  of Health, Bethesda, MD    Large language models (LLMs), such as OpenAI’s ChatGPT, are transforming professional workflows,  including biomedical research. These AI models, trained on vast textual datasets, can assist in literature  review, hypothesis generation, and coding support. However, their integration into scientific research  requires careful consideration of key limitations, including limited access to recent findings due to fixed  training datasets, no direct access to databases or proprietary content, a lack of real understanding and  gaps in reasoning, biases inherited from training data, hallucinations (plausible but false information), and  potential plagiarism.  To address these concerns, the NIH and scientific journals have established guidelines to ensure  transparency and mitigate risks. Rapid advancements in LLM development also may help overcome some  of these challenges.  In this presentation, we explore the responsible use of LLMs in biomedical research by:  1. Presenting results from a survey of NIDCD researchers on their experiences, concerns, and questions  regarding LLMs.  2. Reviewing NIH and journal policies on AI-assisted research and publication.  3. Demonstrating examples of LLM applications in biomedical research.  By fostering discussion on best practices, we aim to equip researchers with strategies for leveraging LLMs  responsibly, maximizing their potential while minimizing risks in life-changing research.              NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P48  Multi-Agent Cross-Modal Large Language Model Framework for Chest X-ray Analysis  and Integrating COVID-19 Pneumonia Predictions  Liang, Z1; Rajaraman, S1; Marini, N1; Xue, Z1; Antani, S1  1. National Library of Medicine, National Institutes of Health, Bethesda, MD    We present a novel multi-agent large language model (LLM) framework to enhance the accuracy and  robustness of diagnosing COVID-19 pneumonia on chest X-ray (CXR) images. The framework leverages the  Generative Pre-trained Transformer (GPT)-o1 and integrates several specialized artificial intelligence (AI)  agents for classification and regression tasks. Using a chain-of-thought reasoning process, GPT-o1  analyzes both CXRs and expert annotations, synthesizes outputs from the agents and assigns confidence  weights based on performance metrics to ensure reliable predictions. Our evaluation of the model  confirms its capability to grade pneumonia severity while mitigating noise in regression tasks. Our  approach addresses challenges introduced due to the variety in the data and any modality-specific  limitations. The proposed LLM-based architecture is shown to outperform conventional AI models in  COVID-19 pneumonia detection and severity assessment. Our work highlights the potential benefits of  using multi-agent LLMs to enhance AI support for clinical decision-making through robust, efficient, and  comprehensive AI-assisted diagnostic tools.    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P49  RAG2SQL  Nolte, S1; Saddler, TO2; Reif, DM2;  Schmitt, CP1; Auerbach, SS2; Hsieh, J-H2  1. Office of Data Science, National Institute of Environmental Health Sciences, National Institutes of  Health, Durham, NC  2. Division of Translational Toxicology, National Institute of Environmental Health Sciences, National  Institutes of Health, Durham, NC    Toxicity-related databases can be challenging to navigate, requiring users to possess both domain-specific  knowledge and proficiency in query languages such as Structured Query Language (SQL). Writing effective  queries can therefore demand substantial time and effort. Large Language Models (LLMs) offer a  promising solution by enabling natural language interfaces to databases through Text-to-SQL, which  translates user questions into SQL queries. This capability can be further enhanced using a Retrieval- Augmented Generation (RAG) framework.  We evaluated a RAG-to-SQL approach using a SQLite database containing zebrafish larval behavior  developmental neurotoxicity data following chemical exposure. The LLM used was GPT-4o via Azure  OpenAI. Using 10 crafted prompts randomly selected from a prompt pool, repeated over three iterations,  we assessed the accuracy of generated queries under four conditions: (1) providing an image of the  database’s entity-relationship (ER) diagram, (2) supplying schema information derived solely from the  database’s Data Definition Language (DDL), (3) DDL schema plus supplementary database documentation,  and (4) DDL schema plus contextually relevant SQL examples.  Condition (1) substantially underperformed (mean accuracy = 0.46), while the other three conditions  produced comparable results, each achieving an average accuracy above 0.90. Errors in the top- performing conditions typically reflected correct logical reasoning but misinterpretation of field names in  the prompts—suggesting that additional prompt tuning could further enhance performance. Ongoing  work includes testing with more complex database schemas, applying alternative LLMs (e.g., Gemini), and  developing web-based applications for real-time data visualization.                  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P50  Leveraging Large Language Models (LLMs) for data extraction and quality assessment  in psychiatry systematic reviews: A comparison of inter-rater reliability between Elicit  and human coders  Erkan, CN1; Gu, G1; Tandilashvili, E1; Meigs, JM3; Lee, K1; Metcalf, O4; Livinski, A2; Pine, DS1;  Pereira, F1; Brotman, MA1; Henry, LA1    1. National Institute of Mental Health, National Institutes of Health, Bethesda, MD  2. National Institute of Health Library, Office of the Director, National Institutes of Health, Bethesda, MD  3. The Catholic University of America, Washington, DC  4. The University of Melbourne, Parkville, Melbourne, Australia    Background  Data extraction is a laborious and error-prone part of systematic reviews (Gartlehner et al., 2024). Large  language models (LLMs) may improve efficiency in reviews (Amirian et al., 2024), but their accuracy varies.  In our systematic review, we used an LLM, Elicit, as a secondary coder. We compared its accuracy in data  extraction and quality assessment to humans, hypothesizing that inter-rater reliability would be higher  for human-human than for human-Elicit coders.  Method  We reviewed 229 studies on ecological momentary assessment (EMA). Research assistants extracted 176  data points (e.g., demographics) and assessed 9 quality items (e.g., validity of measures). Articles were  double-coded: 99 by two human coders, 130 by one human coder and Elicit. Inter-rater reliability was  calculated for data extraction as [100 - ((# of discrepant data points/total # of extracted data points) *  100)] and for quality assessment as ((# of items agreed upon/total # of items)*100). Independent samples  t-tests compared reliabilities between groups.  Results  For data extraction, human-human coders showed higher inter-rater reliability (M=87.35, SD=5.97, range  = 72.73 – 97.16) than human-Elicit coders (M=82.29, SD=7.83, range = 55.68 - 94.89), t(226)=5.33, p<.001.  Quality assessment reliability was similar between groups (human-human: M=72.17, SD=14.97, range =  33.33 – 100.00; human-Elicit:  M=68.63, SD=16.22, range = 22.22 – 100.00, t(225)=1.68, p=0.094.  Conclusions  LLMs may reduce labor and errors in systematic reviews. Elicit’s quality assessment performance was  similar to human coders. While its data extraction performance is not yet at the level of human coders, it  shows promise for improving efficiency in evidence synthesis.          NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P51  Tuberculosis chest X-ray image retrieval system using deep learning based biomarker  predictions   Lowekamp, BC1; Gabrielian, A1; Hurt, DE1; Rosenthal, A1; Yaniv, Z1  1. National Institute of Allergy and Infectious Diseases, National Institutes of Health, Bethesda, MD    It is estimated that in 2023, 10.8 million people fell ill with Tuberculosis (TB) and 1.25 million died from it.  Additionally, there were about 400,000 new drug-resistant cases reported. These are especially  challenging, as treatment is complex, and outcomes are often poor. The NIAID TB Portals program is an  international consortium with a primary focus on patient centric data collection and analysis for drug  resistant TB. The data includes images, their associated radiological findings, clinical records, and  socioeconomic information. This work describes a TB Portals’ Chest X-Ray (CXR) based image retrieval  system which enables precision medicine. An input image is used to retrieve similar images and the  associated patient specific information, facilitating inspection of outcomes and treatment regimens from  comparable patients. Image similarity is defined using clinically relevant biomarkers: sex, age, body mass  index (BMI), and the percentage of lung affected per sextant. The biomarkers are predicted using  variations of the DenseNet169 convolutional neural network. A multi-task approach is used to predict sex,  age and BMI incorporating transfer learning from an initial training on the NIH Clinical Center CXR dataset  to the TB portals dataset. The resulting sex AUC, age and BMI mean absolute errors were 0.9854,  4.03years and 1.67kg/m2. For the percentage of sextant affected by lesions the mean absolute errors  ranged between 7% to 12% with higher error values in the middle and upper sextants which exhibit more  variability than the lower sextants. The retrieval system is currently deployed as part of the TB Portals  Radiomics Analysis Portal.                 NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P52  Artificial Intelligence-based Segmentation of Neurites in High-Resolution Microscopy  Images of iPSC-derived neurons  Arlova, A1; Weller, C1; Nalls, M1; Kelpsch, D1; Faghri, F1; Ryan, V1  1. Center for Alzheimer’s and Related Dementias, National Institute on Aging, National Institutes of  Health, Bethesda, MD    Dysfunction of mRNA transport and local translation in neurons, a potential factor involved in  frontotemporal dementia/amyotrophic lateral sclerosis (FTD/ALS), is well-suited to be assayed by high- resolution microscopy in cultured cells derived from induced pluripotent stem cells (iPSCs). Analysis of  sub-organellar particles can provide insight into effects of various mutations on these transcription and  translation processes. Therefore, accurate and fast segmentation of neurites in confocal fluorescent  microscopy images is a necessary step to determine boundaries of neurites and facilitate downstream  analysis of sub-organellar structures. Manual segmentation is time-consuming, subject to variability  among annotators, and does not allow for high-throughput image processing. Other methods, such as  automated thresholding with imaging software, do not always produce satisfactory results as image  quality can vary among images in a batch, and fluorescent signal produced by imaging markers is often  confounded by noise.  Artificial Intelligence-based solutions can help speed up neurite segmentation and  alleviate biases introduced by other segmentation methods. In this work, we investigate application of  convolutional neural networks (CNNs) to segment neurites in confocal fluorescent microscopy images. In  our preliminary findings, a UNet-based model achieved a Dice-Sorensen similarity coefficient score (Dice)  of 0.55, while NIS-Elements software thresholding workflow achieved a Dice of 0.43, and NIS-Elements  proprietary AI model achieved a Dice of 0.46. This experiment is a proof of concept that a well-performing  CNN can be trained on a relatively small manually annotated dataset. It is a viable neurite segmentation  method that can be implemented as the first step of an image analysis pipeline.                 NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P53  Tuberculosis Portals AI in Image Processing and Abnormality Detection  Lowekamp, B1; Yaniv, Z1; Cobean, R1; Hoppes, M1; Rosenfeld, G1; Grinev, A1; Gabrielian, A1;  Hurt, D1; Rosenthal, A1; Tartakovsky, M1  1. Office of Cyber Infrastructure and Computational Biology, National Institute of Allergy and Infectious  Diseases, National Institutes of Health, Bethesda, MD    The NIAID Tuberculosis (TB) Portals Program is a global initiative that enhances TB research through open- access, multi-domain data and tools. The TB Case Browser collects, stores, and provides deidentified TB  clinical, pathogen genomics, and imaging data for research. Researchers from over 20 countries upload  data including chest radiographs (CXR) and computed tomography (CT) studies.   Medical image analysis offers critical insights into disease diagnosis, progression, and treatment efficacy,  which led the TB Portals program to include radiologist/clinical reads in its image collection. To maximize  available information on disease pathology research, TB Portals partnered with Qure AI, CADDIETECH, and  UIIP, all providing researchers with AI-predicted abnormality labels and segmentations. These efforts are  essential in optimizing AI assistance for practicing radiologists in clinical settings.  The TB Portals program utilizes AI and advanced machine learning in its data quality pipelines to provide  high quality public datasets. To prevent ingestion of unwanted protected health information (PHI), TB  Portals uses AWS Rekognition with Optical Character Recognition (OCR) to screen images for text,  significantly reducing manual effort. Given the diverse quality of images from other countries, the TB  Portals program also implemented a “Chest-X-Ray Outlier” algorithm. This algorithm evaluates pixel data  and classifies images at the distribution tails as outliers, helping to identify low quality images and in  general any image that does not look like a frontal chest X-Ray.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P54  Machine Learning Classification of Clinical Edema   May, CM1; Kasi, KK1; Kobayashi, LK2; Conway, BC1; Pare, JP2  1. National Eye Institute, National Institutes of Health, Bethesda, MD  2. Brown University Emergency Department    Hyperspectral imaging (HSI) is a powerful technique that captures and analyzes data from across the  electromagnetic spectrum, providing significantly more detailed information than traditional RGB  imaging, which is limited to just three-color channels. By collecting hundreds of narrow, contiguous  spectral bands, HSI allows for the identification of unique spectral signatures based on how materials  interact with different wavelengths of light. In medicine, HSI has been applied to distinguish between  conditions such as kidney stones and cancers, though most studies focus on pixel-level classification (e.g.,  detecting cancer in specific pixels), rather than whole-image classification methods commonly used in  convolutional machine learning (e.g., determining if an entire image shows signs of disease). In this study,  we used a hyperspectral camera (SOC-710, Surface Optics Corporation) to capture images of patients  presenting with edema to the Brown University emergency department. Using principal components  analysis for dimensionality reduction and continuum removal for denoising, we achieved strong  classification accuracy. We applied support vector machine using a linear kernel, achieving a 100%  accuracy for cellulitis. We also performed 1D classification of wavelengths using four different algorithms,  again achieving perfect classification of cellulitis. We also determined relative melanin and hemoglobin  saturation maps within each patient, which has potential applications for classification of disease states  using hyperspectral imaging.                   NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P55  Leveraging an MRI-Based Foundation Model to Enhance Predictions of Survival in  Glioblastoma: A Multimodal Deep Learning Approach  Khanna, K1; Chen, Q1; Yan, C1; Meerzaman, D1  1.Computational Genomics and Bioinformatics, Center for Biomedical Informatics and Information  Technology, National Cancer Institute, National Institutes of Health, Bethesda, MD    Glioblastoma multiforme (GBM) is the most common primary brain tumor in adults and has very poor  survival outcomes. Accurate prediction of overall survival could significantly improve patient stratification  in clinical trials and treatment planning. In this study, we demonstrate that a pretrained 3D vision  transformer (SwinViT) can effectively predict overall survival in patients with GBM using preoperative  MRIs. Using the UPENN-GBM dataset (n=520), we adapted and finetuned the BrainSegFounder, a 3D  foundation model trained using MRI volumes from 41,400 healthy participants and 1,251 patients with  brain tumors, in order to predict overall survival. Our finetuned model significantly outperformed models  trained from scratch (C-Index: 0.672 ± 0.036 vs. 0.643 ± 0.431, Wilcoxon p=0.0488), with particularly  strong performance in stratifying the highest-risk patients. We further explored integration of diffusion  tensor imaging (DTI) and clinical variables with proven prognostic value, finding that multimodal  approaches combining imaging derived risk scores, age, MGMT methylation status, and extent of surgical  resection achieved the highest performance (C-Index: 0.714 ± 0.066). While imaging features performed  well in identifying high-risk patients, the addition of clinical markers provided the largest improvements  when distinguishing long-term (low risk) survivors. Our results demonstrate the immense potential of  foundation models to improve medical imaging analysis, particularly when labeled data is scarce, and  emphasizes the synergy between imaging and clinical data in GBM survival prediction.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P56  Deep learning approach to video-based behavioral classification through human pose  estimation.   MacLaren, CE1; Jackson, SN1; Fruchet, OE1; Volkman, RA1; Inati, SK2; Zaghloul, KA1  1. Functional Neurosurgery Section, Surgical Neurology Branch, National Institute of Neurological  Disorders and Stroke, National Institutes of Health, Bethesda, MD  2. Neurophysiology of Epilepsy Unit, Surgical Neurology Branch, National Institute of Neurological  Disorders and Stroke, National Institutes of Health, Bethesda, MD    There exist current methods for identifying human action in videos, but little advancements in behavior  in a hospital environment, where patients are monitored 24/7 via a live-stream camera. With new  advances in machine learning and computer vision, different deep learning models can now identify  objects and track their movement throughout a video. Through such, human movement – described as  human pose – can be extracted in videos, creating opportunity for tracking and classifying different  actions. We are interested in applying these methods for our own video data, where we record 24/7  clinical footage for epilepsy patients admitted at the NIH for seizure monitoring. Pre-trained human pose  models achieve very high mean average precision (mAP) and are useful for transferring to different  datasets. Utilizing a pre-trained network and fine-tuning for refined features, we can identify more  positional information to the standard pre-trained network for our non-uniform environments, where  patients may be in different settings with various obstructions, such as staff and family interruptions,  blankets, tables, etc. We are able to achieve a mAP of 0.78147, identifying 18 different points of interest  on the human body, and a precision of 0.99668 for identifying the proper boundaries of our patient. By  correctly identifying human movement in videos, we can cluster different behaviors to classify a patient’s  unique behaviors. With this annotated data, we can extract neural correlates for precise behaviors  throughout a patient’s entire stay.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P57  AI-based analysis of complex pigmentation phenotypes in zebrafish embryos  Shive, HR1  1. Laboratory of Cancer Biology and Genetics, National Cancer Institute, National Institutes of Health,  Bethesda, MD    Pigmentation is a complex process that can be dysregulated by diverse factors, including both external  (UV exposure, medications) and internal (genetic disorders, inflammation) variables. The zebrafish (Danio  rerio) is well-established as a facile comparative model for analyzing pigmentation disorders, and  melanophore development and melanin synthesis occur through conserved signaling pathways that are  comparable to humans. Phenotypic characterizations of pigmentation in zebrafish are frequently  conducted in embryonic or larval stages, which facilitates rapid, high-throughput screening of large animal  numbers and whole-animal imaging. However, downstream image-based analyses currently used to  assess pigmentation are cumbersome, labor-intensive, and difficult to use quantitatively. Here I describe  a method to analyze pigmentation patterns in zebrafish embryos using HALO (Indica Labs).  Although  HALO modules are designed as an image analysis platform for digital histologic images, I have developed  an AI-based approach to analyze digital stereomicroscopic images of zebrafish embryos. This approach  applies a convolutional neural network (CNN) algorithm to classify pigmented regions in embryos, which  subsequently enables accurate quantitative analyses of traits such as size of pigmented regions,  pigmentation pattern, and distance between zones of pigmentation. I applied this CNN to quantify  aberrant pigment patterns in zebrafish embryos with mutations in neurofibromin 1 (NF1). NF1 mutations  in humans cause neurofibromatosis type 1, a genetic disorder for which aberrant pigmentation is a key  diagnostic criterion. This work demonstrates the flexibility in creative use of CNN for analyzing digital  images and provides a novel approach that will be highly useful for assessing pigmentation phenotypes in  various zebrafish models.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P58  Privacy-preserving and communication-efficient prediction of ISUP grades from  prostate cancer histopathology images with foundation models  Lohmann, JJGL1,2; Witte, AW3; Maier, AM1; Saak, CCS1; Sauter, GS4; Zimmermann, MZ3; Bonn,  SB3,5; Baumbach, JB1,6  1. Institute for Computational Systems Biology, University of Hamburg, Hamburg, Germany  2. Cyberinfrastructure and Artificial Intelligence Platforms Section, National Human Genome Research  Institute, National Institutes of Health, Bethesda, MD   3. Institute of Medical Systems Bioinformatics, Center for Biomedical AI (bAIome), Center for Molecular  Neurobiology Hamburg (ZMNH), University Medical Center Hamburg-Eppendorf, Hamburg, Germany  4. Institute of Pathology, University Medical Center Hamburg-Eppendorf, Hamburg, Germany  5. Spearpoint Analytics AB, Stockholm, Sweden  6. Computational Biomedicine Lab, Department of Mathematics and Computer Science, University of  Southern Denmark, Odense, Denmark    Background: To provide the best possible treatment for prostate cancer patients, several machine  learning (ML) models have been developed to automate the classification of cancer severity using the  International Society of Urological Pathology (ISUP) grades based on histopathology images of prostate  biopsies. While ML models generally benefit from including more training data, aggregation of distributed  data, e.g. in a cloud, can be difficult due to privacy regulations.  Methods: To overcome data privacy issues, we utilize federated learning (FL), a privacy-preserving  technique to train ML models. Specifically, we have built a resource-efficient federated model for ISUP  grade prediction, utilizing foundation models. The performance of the FL-based model was evaluated by  comparing it to training on centralized cohorts and locally isolated cohorts. To test model robustness, the  change in performance was assessed on heterogeneous data distributions and additionally for an  increasing number of participating data providers.  Results: Compared to the centralized model the FL-based models perform equally well on the different  data distributions. Also, the model performance is robust against increasing numbers of participating  sites. By incorporating preprocessing by foundation models the model size used in training was reduced  by up to 98%, possibly leading to decreased communication overhead in real-world applications.  Conclusions: The presented FL approach competes with a centralized model when performing ISUP grade  prediction of prostate cancer without the need to aggregate the underlying data. Additionally, image  embedding using foundation models during preprocessing reduces the model size during federated  training.          NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P59  The effects of syndromic facial feature editing on AI and clinician diagnosis of genetic  conditions   Cheng, J1; Flaharty, KA1; Duong, D1; Waikel, RL1; Hu, P1; Ledgister Hanchard SE1; Solomon, BD1  1. Medical Genomics Unit, National Human Genome Research Institute, National Institutes of Health,  Bethesda, MD    Clinicians recognize genetic conditions based on the presence of distinct phenotypic features. In clinical  practice, they may use artificial intelligence tools to help them with this task. These phenotypic features  are catalogued in the Human Phenotype Ontology (HPO), but how each HPO feature contributes to the  recognition of a genetic condition remains unclear.    Advances in deep learning offer new possibilities for investigating clinical genetic datasets to better  understand how HPO features may contribute to condition recognition, for example, through super- resolution and inpainting techniques. By leveraging BrushNet, a deep learning-based inpainting diffusion  model, we can create high-resolution images of patients affected by genetic disorders with and without  selected HPO features.     After systematically editing HPO features from syndromic face images across a variety of genetic  conditions, we evaluated the diagnostic accuracy of clinicians and of state-of-the-art deep learning  classifiers such as Face2Gene and GestaltMatcher.     This study provides insight into the relative importance of specific syndromic facial features, highlights  the potential and implications of AI-driven image editing in clinical settings, and provides new ways of  creating high-quality counterfactual facial image datasets for clinical genetics studies.                 NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P60  Predicting tuberculosis from frontal chest X-rays: A Radiomics Analysis Portal research  service   Kantipudi, K1; Gabrielian, A1; Hurt, DE1; Rosenthal, A1; Yaniv, Z1  1. National Institute of Allergy and Infectious Diseases, National Institutes of Health, Bethesda, MD    According to the 2024 World Health Organization report, in 2023 10.8 million people were diagnosed with  Tuberculosis (TB). Chest X-rays have shown their utility both as a screening tool to identify individuals  exhibiting TB related abnormalities and as a triaging decision tool, guiding referral for additional testing.  The NIAID TB-Portals program is an international consortium focused on patient centric data collection  and analysis. The program’s Radiomics Analysis Portal enables interaction with the imaging data and  provides a web-based TB/not-TB classification service. The service offers three AI classification options.  All models were trained using publicly available datasets: Shenzhen and Montgomery from the NLM, the  TBX11K challenge and the TB-Portals program and derived PG-GAN images. Two lung segmentation  models were trained, one including the heart and one excluding it. Cropped images derived from lung  segmentation predictions were used to train DenseNet121 classification models. Classification training  was conducted using a five-fold cross validation framework with ImageNet weights initialization and the  cross-entropy loss function. The models with the highest area under the receiver operating characteristic  curve (AUC) were incorporated into the service. Additionally, all five DenseNet121 models were  incorporated into the service as an ensemble. The generalization performance of all three approaches  was evaluated using a private segregated TB screening dataset (279 TB/ 9287 not-TB) and a public dataset  (125 TB/ 153 not-TB). The AUC for the models on the private dataset ranged between 0.76 and 0.79. The  AUC for the models on the public dataset ranged between 0.74 and 0.84.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P61  Deep learning assisted matrix factorization improves cell recognition  in calcium  imaging analysis  von Buchholtz, LJ1  1. National Institute of Dental and Craniofacial Research, National Institutes of Health, Bethesda, MD    Most current methods in the analysis of calcium imaging data employ a form of matrix factorization that  models a calcium imaging movie as the sum of cellular components that are each defined by the  multiplication of a spatial footprint with an activity trace over time. While some algorithms have put  constraints on the temporal profile of cell activity, all current methods treat pixels as independent data  points and don't incorporate any prior knowledge about cell shape. As a result, these methods have  problems detecting cells with low temporal complexity and tend to artificially conflate cells that are  spatially connected and highly temporally correlated with each other.  I propose a model in which a Variational Autoencoder (VAE) is trained on cell image data to generate  realistic cell shapes from a low-dimensional latent vector. After training the VAE, the optimal  corresponding latent vector can then be calculated to match any given novel cell image by gradient  descent and backpropagation.  This VAE can also be used to generate spatial cell footprints that are then multiplied with randomly  initialized temporal activity traces. Simultaneous optimization for the latent vectors underlying the spatial  footprints and the temporal activity patterns by gradient descent backpropagation allows reliable analysis  of real-life calcium imaging data. Preliminary comparison with existing methods shows that the VAE- assisted matrix factorization is computationally more expensive, therefore slower but detects and  separates more true cells. It therefore also removes neighboring undetected cells as a major source of  signal contamination.                  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P62  Predicting Renal Tumor Pathology from Gross Appearance: An AI-based Pilot Study  Patel, MH1; Stecko, H2; Pramod, N3; Esengur, O2; Stevenson, E2; Saini, J1; Loebach, L1; Blachman- Braun, R1; Millan, B1; Nethala, D1; Gurram, S1; Linehan, WM1; Turkbey, B2; Ball, MW1  1. Urologic Oncology Branch, National Cancer Institute, National Institutes of Health, Bethesda, MD  2. Molecular Imaging Branch, National Cancer Institute, National Institutes of Health, Bethesda, MD  3. Genitourinary Malignancies Branch, National Cancer Institute, National Institutes of Health, Bethesda,  MD    Introduction: Renal tumor pathology can guide surgical management and treatment decisions, however,  conventional imaging is imperfect to predict histology subtype and renal biopsy is under-utilized. This  project aims to develop a convolutional neural network (CNN) model to predict renal tumor histologic  subtype from intra-operative gross imaging data, to create an objective tool to assist in real-time clinical  decision-making.  Methods: Images selected for AI model development contained encapsulated renal tumors captured from  live surgical recordings for patients undergoing partial nephrectomy between the years 2008-2024.  Selected images were assigned labels based upon tumor histology and randomized patient-wise into  training and testing groups. A CNN, based on DenseNet121 architecture from the MONAI framework, was  trained using the image sets.   Results: A total of 287 intraoperative images were utilized from 95 unique patients. A total of 48 patients  were randomized to the training dataset while 47 were randomized to the testing dataset. Patient  histology included clear cell (n = 123), chromophobe (n = 33), papillary (n = 48), angiomyolipoma (n = 23),  hybrid (n = 21), and oncocytoma (n = 39). AUC for each class was: 0.72 for clear cell carcinoma, 0.69 for  papillary type 1, 0.59 for oncocytoma, 0.61 for hybrid, 0.66 for chromophobe, and 0.72 for  angiomyolipoma.   Conclusion: An AI model based on DenseNet121 architecture can reasonably predict the histology of the  most common kidney tumor types based on intraoperative images. Further development of the AI model  with a larger training set may aid in expediting differential management based on tumor type.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P63  Weakly supervised learning for subcutaneous edema segmentation of abdominal CT  using pseudo-labels and multi-stage nnU-Nets  Bhadra, S1; Liu, J1; Summers, RM1  1. Clinical Center, National Institutes of Health, Bethesda, MD    Anasarca refers to the excessive accumulation of interstitial fluids within subcutaneous adipose tissue,  causing generalized edema commonly due to heart, kidney, or liver dysfunction. Automated volumetric  assessment of edema from abdominal CT scans offers valuable insight for monitoring disease progression,  particularly in ICU settings; however, manual annotation for supervised segmentation is impractical due  to edema's diffuse nature. While a recent unsupervised deep learning method leveraging intensity priors  was introduced, it resulted in frequent false positives or under-segmentation errors. To address these  limitations, we propose a weakly supervised segmentation framework utilizing multi-class pseudo-labels,  which combine edema intensity prior-based pseudo-labels with adipose tissue and muscle pseudo-labels  for additional anatomical context. Our two-stage approach employs nnU-Net as the segmentation  backbone. In Stage 1, muscle and fat pseudo-labels were generated from 101 contrast-enhanced CT scans  of patients without edema (52F, 49M, age: 66.6 ± 5.1 years) using existing body composition software  annotations. Stage 2 training involved 99 CT scans without edema (45F, 54M, age: 48.1 ± 17.7 years),  incorporating combined multi-class pseudo-labels from Stage 1 and the intensity prior method. Evaluation  on 16 edema-positive patient scans (10F, 6M, age: 52.4 ± 8.7 years), using five manually annotated slices  per scan supervised by an experienced radiologist, demonstrated significant improvements. Our method  enhanced the average Dice Similarity Coefficient and relative volume difference by 4–5% (p<0.05)  compared to intensity prior segmentation alone. Qualitatively, this weak supervision significantly reduced  false positives and under-segmentation errors, confirming its efficacy for accurate edema quantification  in monitoring anasarca.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P64  Staging Liver Fibrosis with Hepatic Perivascular Adipose Tissue as a CT Biomarker  Chan, S1; Mathai, TS1; Balamuralikrishna, PTS1; Batheja, V1; Liu, J1; Lubner, MG2; Pickhardt, PJ2;  Summers, RM1  1. Radiology and Imaging Sciences, National Institutes of Health Clinical Center, Bethesda, MD  2. Department of Radiology, University of Wisconsin School of Medicine & Public Health, Madison, WI    Cirrhosis is the 12th leading cause of death in the US. There are several CT imaging signs of late fibrosis,  such as redistribution of liver segment volume, increased liver nodularity, and periportal space widening.  Timely intervention can reverse the progression of early hepatic fibrosis, but later stages are irreversible.  We hypothesize that the perivascular adipose tissue (PVAT) around the portal vein arising from periportal  space widening may also be predictive of liver fibrosis. In this work, a fully automated pipeline was  developed to segment the liver, spleen, portal vein and its branches. The PVAT in the vicinity of the portal  vein was identified. From these structures, CT imaging biomarkers (volume, attenuation, fat fraction) were  computed. They were used to build uni- and multivariate logistic regression models for diagnosing  advanced fibrosis and cirrhosis. The best multivariate model for cirrhosis achieved 93.3% AUC, 78.9%  sensitivity, and 93.4% specificity. For advanced fibrosis, the multivariate model obtained 88.7% AUC,  84.2% sensitivity, and 73.7% specificity. The automated approach may be useful for population-based  studies of metabolic disease and opportunistic screening.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P65  Semantic segmentation of TB in chest X-rays: A new dataset and generalization  evaluation  Kantipudi, K1; Bui, V2; Yu, H2; Lure, YMF3; Jaeger, S2; Yaniv, Z1  1. National Institute of Allergy and Infectious Diseases, National Institutes of Health, Bethesda, MD  2. National Library of Medicine, National Institutes of Health, Bethesda, MD  3. MS Technologies Corp, Rockville, MD    According to the 2023 World Health Organization report, an estimated 7.5 million people were diagnosed  with tuberculosis (TB) in 2022. TB triaging is often performed using chest X-rays (CXRs), with significant  efforts invested in automating this task using deep learning. A key concern with algorithms that output  image-level labels, in our context TB/not-TB, is that they do not provide an explicit explanation with  respect to how the output was obtained, limiting the ability of user oversight. Semantic segmentation of  TB lesions can enable human supervision as part of the diagnosis process. This work presents a new  dataset, TB-Portals SIFT, which enables semantic segmentation of TB lesions in CXRs (6,328 images with  10,435 pseudo-label lesion instances). Using this data, ten semantic segmentation models from the UNet  and YOLOv8-seg architectures were evaluated in a five-fold cross validation study. The best performing  segmentation models from each architecture, nnUNet(ResEnc XL) and YOLOv8m-seg and their ensemble  were then evaluated for generalization on related classification and object detection tasks. Additionally,  several binary DenseNet121 classifiers were trained, and their classification generalization performance  was compared to that of the semantic segmentation-based classifier. Results show that the segmentation- based approach achieved better generalizability than the DenseNet121 classifiers and that the ensemble  of the models from the two architectures was the most stable, closely matching or exceeding the  performance of all other models across the tasks of segmentation, classification, and object detection.  The dataset is publicly available from the NIAID TB Portals program after signing a data usage agreement.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P66  Developing a deep learning algorithm to quantify pulmonary vascular remodeling in a  pre-clinical model of pulmonary arterial hypertension and comparing performance to  formal histopathological assessment  Joseph, TL1; Yu, ZX1; Siddique, MAH1; Chen, LY1; Elinoff, JM1  1. Critical Care Medicine and Pulmonary Branch, National Heart, Lung, and Blood Institute, National  Institutes of Health, Bethesda, MD    Rationale: Pulmonary arterial hypertension (PAH) is a rare, female-predominant disease characterized by  an inflammatory, proliferative arteriopathy that results in progressive narrowing of pre-capillary  pulmonary arterioles and eventually death from right heart failure. Pre-clinical PAH animal models are  critical to the development of novel therapeutics and the most common primary endpoint of these studies  is improvement in histopathological lung vessel remodeling. However, current methods for evaluating  pulmonary vascular histopathology are time-consuming and inconsistently applied across studies. To  address these limitations, our group sought to develop a deep learning algorithm with the ability to  perform rigorous and non-biased histopathological assessments in the rat SU-5416/Hypoxia (SuHx) model  of PAH.  Methods: A multi-layered feature-detecting neural network (Visiopharm tissue image analysis software)  was trained using manually annotated Masson Trichrome stained lung sections from SuHx and control  rats to develop a deep learning model that recognizes pulmonary arterioles and determines the extent of  vessel occlusion.   Results:  When evaluated on lung sections which were not used for training, the model achieved an F1  score of 0.54 (F1 score >0.5 is average, >0.7 is good). Model performance was better on lungs from SuHx  animals (F1=0.63) than from controls (F1=0.45). Notably, many of the “false positives” identified by the  model were indeed arterioles that were inadvertently or intentionally excluded from the validation set  due to pre-specified criteria.  Conclusion: Lung vessel segmentation using deep learning is feasible, however, further refinements are  required to improve model validity. A more exhaustive validation set may be more appropriate for  assessing model performance.                   NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P67  Scalable deep learning-based vessel segmentation and morphological quantification  Harouni, M1; Voss TC11  1. Division of Preclinical Innovation, National Center for Advancing Translational Sciences, National  Institutes of Health, Rockville, MD  Accurate segmentation of vascular structures is critical for understanding biological processes and disease  progression. Furthermore, automated vessel segmentation enables large-scale, quantitative analysis of  vascular features in biological imaging.  However, challenges such as image heterogeneity, dense vessel  networks, and large dataset sizes complicate traditional approaches. In this work, we present a scalable,  deep learning-based vessel analysis pipeline designed for high-throughput biological imaging studies. Our  approach begins with automated region-of-interest (ROI) selection, leveraging local contrast  normalization and statistical density estimation, followed by morphological validation through cluster- based analysis of vessel width and perimeter distributions.  The developed deep learning segmentation model, trained on a limited set of curated annotations,  achieved a Dice similarity coefficient of 0.964 across heterogeneous datasets comprising more than  54,000 images. Post-processing steps including skeletonization, hole-filling, and graph-based modeling  enabled detailed extraction of vascular topology, such as junction density, branch type distribution, and  vessel orientation. Spatial features were visualized through heatmaps and validated against original  imaging data to ensure interpretability.  This pipeline not only make more efficient vessel segmentation at scale but also provides rich quantitative  metrics supporting phenotype discovery, disease modeling, and therapeutic evaluation. Our integrated  framework highlights the power of combining advanced image analysis with deep learning to deliver  reproducible, biologically meaningful understandings in complex vascular growing structures.            NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P68  Empanada - a napari plugin with pre-packaged segmentation models for nuclei, lipid  droplets and mitochondria  Bhardwaj, A1; Narayan, K1  1. CCR Volume Electron Microscopy, Frederick National Laboratory, Frederick, MD  Understanding cellular structures hinges on the effective segmentation of organelles in volume electron  microscopy (vEM) images—a task traditionally marked by labor-intensive and time-consuming manual  efforts. The advent of deep learning has revolutionized this process, automating segmentation and  markedly enhancing efficiency. Convolutional neural networks (CNNs) have demonstrated particular  success in delineating organelles such as mitochondria and nuclei within vEM datasets. However, the  widespread adoption of these methods is often hindered by the substantial computational resources and  extensive annotated datasets they require.  Addressing these challenges, we have developed empanada as a user-friendly plugin for the napari image  viewer, designed to facilitate deep learning-based organelle segmentation. empanada now integrates pre- trained models NucleoNet, DropNet and MitoNet, and also offers users the flexibility to fine-tune existing  models or train new ones with their own data, thereby accommodating a diverse array of electron  microscopy images. The models within empanada are trained on large, heterogeneous, and meticulously  curated datasets, which are freely accessible to the scientific community.  Empanada streamlines the segmentation workflow by providing dedicated modules for training,  inference, and post-processing, opening deep learning applications to non-experts. By democratizing  access to advanced segmentation tools, empanada empowers researchers to efficiently analyze large EM  and vEM datasets, accelerating discoveries in cell biology through precise and high-throughput analysis of  organelle structures.            NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P69  Optical Coherence Tomography: A Reliable Imaging Modality for Detecting Age- Related Macular Degeneration Features  Elsawy, A1; Keenan, T2; Chew, EY2; Lu, Z1  1. National Library of Medicine, National Institutes of Health, Bethesda, MD  2. National Eye Institute, National Institutes of Health, Bethesda, MD    Age-related macular degeneration (AMD) is an irreversible and progressive retinal disease that affects the  macula. AMD is a leading cause of central vision loss for the elderly in developed countries. AMD is  predicted to affect 288 million people worldwide. Thus, early detection of AMD is important for slowing  its progression and preserving vision. AMD is categorized into early, intermediate, and late stages. The  early features of AMD include drusen, i.e., retinal deposits below the retinal pigment epithelium (RPE)  layer of the retina, the intermediate features include reticular pseudodrusen, i.e., subretinal deposits  above the RPE layer, and the late features include geographic atrophy (GA), i.e., the defining lesion of  atrophic AMD. Optical coherence tomography (OCT) is a non-contact non-invasive imaging technology  that provides high-resolution cross-sectional images in vivo. Hence, it is suitable for imaging the eye. To  study AMD on OCT, we used OCT datasets from the age-related eye diseases study 2 (AREDS2) and the  dark adaptation in AMD study (DAAMD). We developed Deep-GA-Net and Deep-RPD-Net for detecting  GA and RPD on OCT scans. We compared the developed models to retina specialists and visualized their  predictions. Results showed an area under the receiver operating characteristic curve of 0.94 for detecting  GA on AREDS2, 0.91 and 0.84 for detecting RPD on AREDS2 and DAAMD, respectively. The developed  models outperformed the retina specialists and could highlight interpretable features. These results  suggest that AMD features can be reliably identified on OCT. Thus, OCT can provide a valuable imaging  modality in managing the AMD progression.            NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P70  Deep Learning-based Contouring of Couinaud Segments on CT: Utility for Volumetric  Analysis of Future Liver Remnant  Mathai, TS1; Balamuralikrishna, PTS1; Batheja, V1; Kassin, M1; Hannah, C1; Ukeh, I1; Hernandez,  J1; Summers, RM 1  1. Radiology and Imaging Sciences, Clinical Center, National Institutes of Health, Bethesda, MD  2. Center for Cancer Research , National Cancer Institute , National Institutes of Health, Bethesda, MD    BACKGROUND:   Hepatocellular carcinoma (HCC) is a common primary liver cancer. High tumor burden, proximity to  hepatic vessels, and other comorbidities mean only 30% of patients are candidates for curative surgical  resection, which requires a 20% future liver remanent (FLR) to avoid post-operative complications. An  automated liver Couinaud segmentation tool was developed for FLR volumetric analysis and targeted  localization of tumors.   METHODS:  Three CT datasets were used: 1) public MSD Hepatic Vessels (161 patients), 2) NIH (43 patients with  cirrhosis, ascites and splenomegaly), and 3) public TCIA Colorectal Liver Metastasis (CRLM, 197 patients).  FLR was annotated by an expert radiologist and the Couinaud segments were manually annotated by two  physicians. MSD and NIH datasets were used for model training, while CRLM was reserved for testing. A  3D nnU-Net model outlined the Couinaud segments. Performance was compared to a prior 3D U-Net  model and evaluated with Dice Similarity Coefficient (DSC), Hausdorff Distance (HD) error (in mm), and  volume error (in cc).   RESULTS:   3D nnU-Net obtained a DSC of 0.99 ± 0.01 (IQR: 0.991, 0.998), HD error of 0.87 ± 1.83 mm (IQR: 0, 1.02),  and volume error of 13.7 ± 28.1 cc (IQR: 3.4, 15.3). Compared to U-Net, it was significantly different for  DSC (p < 0.001, effect size 0.86), HD error (p < 0.001, effect size 0.87), and volume error (p < 0.001, effect  size 0.91).   CONCLUSION:  The model generalized well to an external dataset and may be used for volumetric analysis on patients  undergoing portal vein embolization.            NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P71  Sensitivity based model agnostic scalable explanations of deep learning  Aggarwal, M1; Cogan, N2; Periwal, V1  1. Laboratory of Biological Modeling, National Institute of Diabetes and Digestive and Kidney Diseases,  National Institutes of Health, Bethesda, MD  2. Department of Mathematics, Florida State University, FL    Deep neural networks (DNNs) are powerful tools for data-driven predictive machine learning, but their  complex architecture obscures mechanistic relations that they have learned from data. This information  is critical to the scientific method of hypotheses development, experiment design, and model validation,  especially when DNNs are used for biological and clinical predictions that affect human health. We design  SensX, a model agnostic explainable AI (XAI) framework that outperformed current state-of-the-art XAI in  accuracy (up to 52% higher) and computation time (up to 158 times faster), with higher consistency in all  cases. It also determines an optimal subset of important input features, reducing dimensionality of further  analyses. SensX scaled to explain vision transformer (ViT) models with more than 150,000 features, which  is computationally infeasible for current state-of-the-art XAI. SensX validated that ViT models learned  justifiable features as important for different facial attributes of different human faces. SensX revealed  biases inherent to the ViT architecture, an observation possible only when importance of each feature is  explained. We trained DNNs to annotate biological cell types using single-cell RNA-seq data and SensX  determined the sets of genes that the DNNs learned to be important to different cell types.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P72  Raman-based machine-learning platform reveals unique metabolic differences  between IDHmut and IDHwt glioma  Lita, A1; Sjöberg, J2; Păcioianu, D3; Siminea, N3; Celiku, O1; Dowdy, T1; Păun, A4; Gilbert, MR1;  Noushmehr, H5; Petre, I2; Larion, M1  1. National Cancer Institute, Neuro-Oncology Branch, National Institutes of Health, Bethesda, MD  2. Department of Mathematics and Statistics, University of Turku, Turku, Finland  3. Faculty of Mathematics and Computer Science, University of Bucharest, Bucharest, Romania  4. Department of Bioinformatics, National Institute for Research and Development in Biological Sciences,  Bucharest, Romania  5. Department of Neurosurgery, Henry Ford Health System, Detroit, MI    BACKGROUND: Formalin-fixed, paraffin-embedded (FFPE) tissue slides are routinely used in cancer  diagnosis, clinical decision-making, and stored in biobanks, but their utilization in Raman spectroscopy- based studies has been limited due to the background coming from embedding media.  METHODS: Spontaneous Raman spectroscopy was used for molecular fingerprinting of FFPE tissue from  46 patient samples with known methylation subtypes. Spectra were used to construct tumor/non-tumor,  IDH1WT/IDH1mut, and methylation-subtype classifiers. Support vector machine and random forest were  used to identify the most discriminatory Raman frequencies. Stimulated Raman spectroscopy was used  to validate the frequencies identified. Mass spectrometry of glioma cell lines and TCGA were used to  validate the biological findings.  RESULTS: Here we develop APOLLO (rAman-based PathOLogy of maLignant gliOma)- a computational  workflow that predicts different subtypes of glioma from spontaneous Raman spectra of FFPE tissue  slides. Our novel APOLLO platform distinguishes tumors from nontumor tissue and identifies novel Raman  peaks corresponding to DNA and proteins that are more intense in the tumor. APOLLO differentiates  isocitrate dehydrogenase 1 mutant (IDH1mut) from wildtype (IDH1WT) tumors and identifies cholesterol  ester levels to be highly abundant in IDH1mut glioma. Moreover, APOLLO achieves high discriminative  power between finer, clinically relevant glioma methylation subtypes, distinguishing between the CpG  island hypermethylated phenotype (G-CIMP)-high and G-CIMP-low molecular phenotypes within the  IDH1mut types.  CONCLUSIONS: Our results demonstrate the potential of label-free Raman spectroscopy to classify glioma  subtypes from FFPE slides and to extract meaningful biological information thus opening the door for  future applications on these archived tissues in other cancers.              NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P73  Understanding and simulating membrane pore formation by piscidin1 using AI  informed enhanced sampling  Bodosa, J1,2; Pastor, R1  1. Membrane Biophysics, National Heart, Lung, and Blood Institute, National Institutes of Health,  Bethesda, MD  2. Biophysics, University of Maryland, College Park, MD    Antimicrobial peptides (AMPs) are short peptides found in organisms often as part of the innate immune  host defense. They are capable of disrupting the bacterial membrane thus leading to cell death. These  peptides aggregate on the membrane surface and form pores which allow leakage of the cell content.  Pisidin 1 is a fish AMP which has antimicrobial, antifungal properties. Rice et al. have used molecular  dynamics (MD) simulations to study defect formation by piscidin 1 in different membrane compositions.  Some challenges to the study of membrane pore formation by peptides are - time scale of peptide  aggregation, pore formation and pore persistence. MD is limited to studies which can be simulated within  reasonable time in all-atom resolution. If the event occurs over a long timescale (> 10s ) then it is unlikely  for one to observe statistically significant results using normal MD. We want to use an AI informed method  developed by Tiwari et al. to simulate peptide-mediated pore formation and obtain the free energy  associated with it.                     NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P74  AlphaFold2 screen reveals novel G1 cyclin docking modalities  Weaver, A1; Tuvikene, J1; Koivomagi, M1  1. Laboratory of Biochemistry and Molecular Biology, National Cancer Institute, National Institutes of  Health, Bethesda, MD    The cell cycle is a series of tightly regulated events that drive cellular growth, DNA replication, and division.  Regulation of this cycle is tightly controlled by cyclins and CDKs, which function together to phosphorylate  a specific set of substrates. Understanding these interactions involved in cell cycle transitions is crucial for  discovering the molecular mechanisms of cell cycle regulation. However, identifying specific docking  interfaces has been challenging due to the dynamic nature of these interactions. Traditional methods,  such as alanine scanning, are time-consuming and rely on low-throughput, one-by-one validation  approaches, making it difficult to systematically identify these interactions.  We present a computational pipeline leveraging AlphaFold2 to predict docking interfaces between yeast  cyclin, Cln3, and the full nuclear proteome of Saccharomyces cerevisiae. This allows for identification of  candidate docking interactions at a proteome-wide scale. Predicted AlphaFold2 complexes were analyzed  based on spatial and confidence features to define potential docking regions. To validate, we generated  yeast strains expressing mutant versions of Cln3 with targeted disruptions in the predicted interfaces.  Functional impact was assessed using a cell size assay as it’s a proxy for Cln3-activity during G1 phase  progression. Mutations in select docking regions resulted in altered cell size distributions, supporting their  role in mediating Cln3’s function in driving cell cycle progression.   Our results provide proof of principle for using AlphaFold2 to identify novel docking interfaces between  cyclins and their target proteins across model systems. This allows for the development of targeted  therapeutics that can disrupt specific docking interactions to halt cell division.                  NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P75  In silico evolution of globular protein folds from random sequences  Sahakyan, HK1; Babajanyan, SG1; Wolf, YI1; Koonin, EV1  1. Computational Biology Branch, Division of Intramural Research, National Library of Medicine, National  Institutes of Health, Bethesda, MD    The origin and evolution of protein folds are among the most challenging, long-standing problems in  biology. Although many plausible scenarios of early protein evolution leading to fold nucleation have been  proposed, realistic simulation of this process was not feasible because of the lack of efficient approaches  for protein structure prediction, a situation that changed with the advent of powerful AI-based tools for  fast and robust protein structure prediction. We developed a computational approach for protein fold  evolution simulations (PFES) with atomistic details that provide insights into the mechanisms of evolution  of globular folds. PFES introduces random mutations in a population of proteins, evaluates the effect of  mutations, and selects a new set of proteins for further evolution. Repeating this process iteratively allows  tracking the evolutionary trajectory of a changing protein fold that evolves under a selective pressure. We  employed PFES to show how stable, globular protein folds could evolve from random amino acid  sequences in various scenarios. The simulations reproduce the evolution of many simple folds of natural  proteins as well as the evolution of distinct folds not known to exist in nature. These findings could shed  light on the enigma of the rapid evolution of protein fold diversity at the earliest stages of life evolution.  PFES tracks the complete evolutionary history from simulations that describes intermediate states and  can be used to test versatile hypotheses on protein fold evolution.                NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P76  Computational modeling of Cyclin D1 protein-protein interactions  Tuvikene, J1; Esvald, EE1; Heidebrink G1; Koivomagi, M1  1. Laboratory of Biochemistry and Molecular Biology, National Cancer Institute, National Institutes of  Health, Bethesda, MD    Cyclin D1, forming complexes with cyclin-dependent kinases CDK4/6, is a key regulator of cell cycle  progression, with its dysregulation implicated in numerous cancers. However, only a few substrates of the  Cyclin D1-CDK4/6 complex are currently known, with the pocket proteins Rb, p107, and p130 being the  best characterized. To further explore Cyclin D1 interactions, we employed AlphaFold-Multimer to predict  interactions between Cyclin D1 and the entire human proteome. Next, we subjected the high confidence  models to a computational pipeline developed in our lab to identify interacting regions and hotspots on  Cyclin D1. To experimentally validate our predictions, we used a chemical-genetic approach combined  with mass spectrometry, labeling Cyclin D1-CDK4/6 substrates using different Cyclin D1 docking region  mutants. We estimate that approximately 30%-40% of the interactions predicted by AlphaFold-Multimer  could be validated experimentally.  Our research uncovers a wide range of potential novel substrates and docking sites, providing a detailed  map of Cyclin D1 interactions and enhancing our understanding of the molecular mechanisms controlling  cell cycle progression. Our study demonstrates the power of integrating computational predictions with  experimental validation to identify critical protein-protein interactions, offering new opportunities for  cancer research and therapeutic development.              NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P77  Integrating Network Analysis and Localization Prediction Using B-LEARN and ProtGPS  Kanno, T1; Kalchschmidt JS2; Brooks, SR1; Sun, H1  1. Biodata Mining and Discovery Section, National Institute of Arthritis and Musculoskeletal and Skin  Diseases, National Institutes of Health, Bethesda, MD  2. Genomics and Immunity section, National Institute of Arthritis and Musculoskeletal and Skin Diseases,  National Institutes of Health, Bethesda, MD    Investigating regulatory interactions and subcellular localization provides crucial insights into protein  function. While it would be ideal to have bespoke experimental data for each protein of interest, data  repositories from previous experiments, combined with state-of-the art AI prediction tools, present a fast  and computationally efficient way to predict outcomes and refine hypotheses. Here, we present two  recent tools, B-LEARN and ProtGPS that serve this purpose.   Our group has recently developed B-LEARN, an interactive online data portal designed for the intuitive  search and visualization of 4,400 B cell regulators and 17,638 regulatory connections identified in B cells.  The data consists of 47 genome-wide loss of function sgRNA screens, providing a high order view of  transcriptional networks across a wide range of genes.   ProtGPS, a neural network classifier recently published by the Whitehead Institute and MIT, can accurately  predict the subcellular compartmentalization of proteins from amino acid sequences. At BMDS, we have  converted ProtGPS from the original proof of concept Jupyter Notebook into an easy-to-use tool on our  internal Shiny Server, making data processing and sharing intuitive and accessible while maintaining data  security.   We explore a potential workflow integrating both tools on the well-studied coregulatory network  between RUNX1 and CBFB, alongside fusion candidates presented in fusionPDB. The predictions agree  well with published imaging and provide a promising pathway to narrow down other fusion proteins to  find candidates for further experimental study.              NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Poster Abstracts    P78  Efficient Computational Prioritization of Local Host Structures Mimicking Pathogen  Antibody Epitopes.  Nguyen, TH1,2; Ghedin, E1; Sormanni, P2  1. Laboratory of Parasitic Diseases, National Institute of Allergies and Infectious Diseases, National  Institutes of Health, Bethesda, MD  2. Centre for Misfolding Diseases, Department of Chemistry, University of Cambridge, Cambridge, UK    Molecular mimicry, where pathogen proteins resemble host structures, is a key hypothesis for hijacking  cellular pathways and triggering autoimmune responses. Proposed mechanisms involve promiscuous T  cells and antibodies cross-reacting due to sequence or structural similarity. However, detecting subtle,  local patch-level mimicry often missed by domain-level searches remains challenging. We present a  systematic computational method wherein proteins from known antibody-antigen complexes are  segmented and rapidly searched against host databases for local structural and chemical similarity.  Putative mimics are further evaluated using antibody docking and prioritized via AlphaFold3 co-folding to  assess complex formation feasibility. Promising candidates proceed to experimental validation from high- throughput enzyme-linked immunosorbent assays (ELISA) to biolayer interferometry (BLI). Preliminary  computational hits to the SARS-CoV-2 proteome with solved antibody structures yielded fewer than 30  candidate mimic complexes. This stringent approach aims to reduce the high false-positive rates  associated with simpler sequence or structure comparisons, potentially uncovering novel antibody- mediated mechanisms underlying conditions like Post-Acute Sequelae of COVID-19 (PASC).            NIH Campus Directions   NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Directions      Public Transportation  Visitor parking is extremely difficult to find at NIH, so if at all possible, we recommend taking public  transportation. There is a Metro station on the NIH campus with shuttle bus service to the main  buildings. If you plan to take the metro, please take the Red Line and get off at “Medical Center  Metro Station”. The NIH Gateway Visitor center is located directly across from the Metro Station.  Please proceed to the Gateway Center to get a visitor badge and either walk or take one of the  free campus shuttles to Building 10 South Entrance (see Campus Shuttles below).    Driving Directions Website (http://www.nih.gov/about-nih/visitor-information/driving- directions)    Directions for Parking OFF Campus in Visitor Parking Garage (MLP-11):  All visitors must enter through the NIH Gateway Center (Bldg. 66). If you are planning to drive to  campus and park outside of campus, please drive to the Main Visitor Entrance at NIH Gateway  Center (Bldg. 66) and park at the Multi-level Parking Garage (MLP-11). This garage is outside of  the perimeter security and, thus, vehicles will not need to go through vehicle inspection, reducing  the amount of time it takes to get on campus. The cost to park in MLP-11 is $2 per hour for the  first three hours, $12 maximum for the entire day. Once you have parked, please proceed to the  Gateway Center (Bldg. 66) to get a visitor badge and either walk or take one of the free campus  shuttles to Building 10 South Entrance (see Campus Shuttles below).    Directions for Parking ON Campus:  Visitor parking on campus is limited, so we recommend parking off campus in MLP-11. If you are  planning to drive and parking on campus, please allow at least 30 minutes to pass through campus  security (Gateway Inspection Station, Bldg. 66A). You will be required to submit to a vehicle  inspection. Visitors over 15 years of age must provide a form of government-issued ID such as a  driver's license or passport. Once you are through the vehicle inspection, you may park in the  green lot next to Bldg. 53 along South Drive and walk to Bldg. 10 (see NIH Visitor Map).     Campus Shuttles  NIH provides a number of free shuttle services throughout the day on the NIH Campus for  employees, patients, and visitors. The following shuttles run from the Visitor Center to Building  10 South Entrance:  •  “Metro/Building 10 South Express Shuttle – Light Green Line” – get on at Visitor Center  (“Metro”) and get off at “Bldg. 10 (South)” stop (1 stop).  o  Departs NIH Visitor Center roughly every 20 minutes.  •  “Campus Limited – Purple Line” – get on at Visitor Center (“Metro”) and get off at “Bldg.  10 (South)” stop (4 stops).  o  Departs NIH Visitor Center roughly every 25 minutes.  •  “Campus Route – Red Line” – get on at Visitor Center (“Metro”) and get off at “Bldg. 10  (South)” stop (6 stops).  o  Departs NIH Visitor Center roughly every 15 minutes.    NIH Artificial Intelligence Symposium – May 16th, 2025 – Masur Auditorium, Building 10  Directions      Directions to Masur Auditorium in Building 10:   o From the North lobby entrance: From the lobby, go down the right side, passing  Admissions on your right. Continue straight through the sliding glass doors, following  posted signs to the Masur. Continue following the “Detour” signs to the Masur. The  auditorium is just past the main elevators.  o From the South lobby entrance: From the lobby, take either the left or right hallway up a  slight incline until you come to the entrance of the Masur Auditorium. When the two  hallways converge, you are standing in front of Masur Auditorium.    NIH Campus Map Visitor Parking * https://ors.od.nih.gov/maps/Pages/default.aspx Building 10 Map Poster Area is  Right past the FAES  Book Store https://clinicalcenter.nih.gov/about/visitor1.html https://ors.od.nih.gov/pes/emb/Documents/CC_MasurAudMap.pdf * Masur Auditorium * Cafeteria is  down the  elevators/stairs* Meow"|1|1|0|1|1
NIST.SP.1270.txt|NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence Reva Schwartz Apostol Vassilev Kristen Greene Lori Perine Andrew Burt Patrick Hall This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.1270 NIST Special Publication 1270 Reva Schwartz National Institute of Standards and Technology Information Technology Laboratory Apostol Vassilev National Institute of Standards and Technology Information Technology Laboratory Computer Security Division Kristen Greene National Institute of Standards and Technology Information Technology Laboratory Information Access Division Lori Perine National Institute of Standards and Technology Information Technology Laboratory & The University of Maryland Andrew Burt Patrick Hall BNH.AI This publication is available free of charge from:  https://doi.org/10.6028/NIST.SP.1270 U.S. Department of Commerce Gina M. Raimondo, Secretary National Institute of Standards and Technology James K. Olthoff, Performing the Non-Exclusive Functions and Duties of the Under Secretary of Commerce for Standards and Technology & Director, National Institute of Standards and Technology Towards a Standard for Identifying and Managing Bias in Artificial Intelligence March 2022 Certain commercial entities, equipment, or materials may be identified in this document in order to describe an experimental procedure or concept adequately. Such identification is not intended to imply recommendation or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the entities, materials, or equipment are necessarily the best available for the purpose. National Institute of Standards and Technology Special Publication 1270  Natl. Inst. Stand. Technol. Spec. Publ. 1270, 86 pages (March 2022)  CODEN: NSPUE2 This publication is available free of charge from: https://doi.org/10.6028/NIST.SP.1270 Executive Summary As individuals and communities interact in and with an environment that is increasingly virtual, they are often vulnerable to the commodification of their digital footprint. Concepts and behavior that are ambiguous in nature are captured in this environment, quantified, and used to categorize, sort, recommend, or make decisions about people’s lives. While many organizations seek to utilize this information in a responsible manner, biases remain endemic across technology processes and can lead to harmful impacts regardless of intent. These harmful outcomes, even if inadvertent, create significant challenges for cultivating public trust in artificial intelligence (AI). While there are many approaches for ensuring the technology we use every day is safe and secure, there are factors specific to AI that require new perspectives. AI sys- tems are often placed in contexts where they can have the most impact. Whether that impact is helpful or harmful is a fundamental question in the area of Trustworthy and Responsible AI. Harmful impacts stemming from AI are not just at the individual or en- terprise level, but are able to ripple into the broader society. The scale of damage, and the speed at which it can be perpetrated by AI applications or through the extension of large machine learning MODELs across domains and industries requires concerted effort. Fig. 1. The challenge of managing AI bias Current attempts for addressing the harmful effects of AI bias remain focused on computational factors such as rep- resentativeness of datasets and fairness of machine learning algorithms. These remedies are vital for mitigating bias, and more work remains. Yet, as illus- trated in Fig. 1, human and systemic in- stitutional and societal factors are sig- nificant sources of AI bias as well, and are currently overlooked. Successfully meeting this challenge will require tak- ing all forms of bias into account. This means expanding our perspective beyond the machine learning pipeline to recog- nize and investigate how this technology is both created within and impacts our so- ciety. Trustworthy and Responsible AI is not just about whether a given AI system is biased, fair or ethical, but whether it does what is claimed. Many practices exist for responsibly producing AI. The importance of transparency, datasets, and test, evaluation, validation, and verification (TEVV) cannot be overstated. Human factors such as participatory design techniques and multi-stakeholder approaches, and a human-in-the-loop are also important for mitigating risks related to AI bias. However none of these practices individually or in i/77 concert are a panacea against bias and each brings its own set of pitfalls. What is miss- ing from current remedies is guidance from a broader SOCIO-TECHNICAL perspective that connects these practices to societal values. Experts in the area of Trustworthy and Respon- sible AI counsel that to successfully manage the risks of AI bias we must operationalize these values and create new norms around how AI is built and deployed. This document, and work by the National Institute of Standards and Technology (NIST) in the area of AI bias, is based on a socio-technical perspective. The intent of this document is to surface the salient issues in the challenging area of AI bias, and to provide a first step on the roadmap for developing detailed socio-technical guidance for identifying and managing AI bias. Specifically, this special publication: • describes the stakes and challenge of bias in artificial intelligence and provides ex- amples of how and why it can chip away at public trust; • identifies three categories of bias in AI — systemic, statistical, and human — and describes how and where they contribute to harms; • describes three broad challenges for mitigating bias — datasets, testing and eval- uation, and human factors — and introduces preliminary guidance for addressing them. Bias is neither new nor unique to AI and it is not possible to achieve zero risk of bias in an AI system. NIST intends to develop methods for increasing assurance, GOVERNANCE and practice improvements for identifying, understanding, measuring, managing, and reducing bias. To reach this goal, techniques are needed that are flexible, can be applied across con- texts regardless of industry, and are easily communicated to different stakeholder groups. To contribute to the growth of this burgeoning topic area, NIST will continue its work in measuring and evaluating computational biases, and seeks to create a hub for evaluating socio-technical factors. This will include development of formal guidance and standards, supporting standards development activities such as workshops and public comment pe- riods for draft documents, and ongoing discussion of these topics with the stakeholder community. Key words bias, trustworthiness, AI safety, AI lifecycle, AI development ii/77 Acknowledgments The authors wish to thank everyone who responded to our call and submitted comments to the draft version of this paper. The received comments and suggested references were essential for improving the paper and the future direction of this work. We also want to thank the many people who assisted with the updating of the document, including our NIST colleagues, and other reviewers who took the time to provide their constructive feedback. We thank Kyle Fox for his insightful comments, discussions, and invaluable input. Audience The intended primary audience for this document includes individuals and groups who are responsible for designing, developing, deploying, evaluating, and governing AI systems. The document is informed and motivated by segments of the public who experience poten- tial harm or inequities due to bias in AI systems, or are affected by biases that are newly introduced or amplified by AI systems. Background This document is a result of an extensive literature review, conversations with experts from the areas of AI bias, fairness, and socio-technical systems, a workshop on AI bias,1 and public comments on the draft version.2 Insights derived from the public comments have been integrated throughout this document. An overview and analysis of themes from the public comments will be posted.3 Intermediate follow-on work to this publication will include development of formal guidance for assessing and managing the risks of AI bias, and a series of public workshops to discuss these topics with the stakeholder community and build consensus. Trademark Information All trademarks and registered trademarks belong to their respective organizations. NIST Special Publications The National Institute of Standards and Technology (NIST) promotes U.S. innovation and industrial competitiveness by advancing measurement science, standards, and technology in ways that enhance economic security and improve our quality of life. Among its broad range of activities, NIST contributes to the research, standards, evaluations, and data re- quired to advance the development, use, and assurance of trustworthy artificial intelligence (AI). 1For more information about this workshop see https://www.nist.gov/news-events/events/2020/08/bias-ai- workshop. 2Public comments are available at https://www.nist.gov/artificial-intelligence/comments-received-proposal- identifying-and-managing-bias-artificial. 3Updated information for all of these resources can be found on the NIST AI Bias webpage, located at https://www.nist.gov/artificial-intelligence/ai-fundamental-research-free-bias. iii/77 The Information Technology Laboratory (ITL) at NIST develops tests, test methods, reference data, proof of concept implementations, and technical analyses to advance the de- velopment and productive use of information technology. ITL’s responsibilities include the development of management, administrative, technical, and physical standards and guide- lines. This special publication focuses on addressing and managing risks associated with bias in the design, development, and use of AI. It is one of a series of documents and workshops related to the NIST AI Risk Management Framework (AI RMF) and is intended to advance the trustworthiness of AI technologies. As with other documents in the AI RMF series, this publication provides reference information and technical guidance on terminology, processes and procedures, and test and evaluation, validation, and verification (TEVV). While practical guidance4 published by NIST may serve as an informative reference, this guidance remains voluntary. The content of this document reflects recommended practices. This document is not intended to serve as or supersede existing regulations, laws, or other mandatory guidance. 4The term ’practice guide,’ ’guide,’ ’guidance’ or the like, in the context of this paper, is a consensus-created, informative reference intended for voluntary use; it should not be interpreted as equal to the use of the term ’guidance’ in a legal or regulatory context.” This document does not establish any legal standard or any other legal requirement or defense under any law, nor have the force or effect of law. iv/77 How to read this document Section 1 lays out the purpose and scope of NIST’s work in AI bias. Section 2 describes three categories of bias and how they may occur in the commission, design, development, and deployment of AI technologies that can be used to generate predictions, recommenda- tions, or decisions (such as the use of algorithmic decision systems), and how AI systems may impact individuals and communities or create broader societal harms. Section 3 de- scribes the challenge of bias related to three core areas: datasets; test, evaluation, validation and verification; and human factors, and provides general guidance for managing AI bias in each of those areas. This document uses terms such as AI technology, AI system, and AI applications inter- changeably. Terms related to the machine learning pipeline, such as AI model or algorithm are also used in this document interchangeably. Depending on context, when the term “system” is used it may refer to the broader organizational and/or social ecosystem within which the technology was designed, developed, deployed, and used, instead of the more traditional use related to computational hardware or software. Important reading notes: • The document includes a series of vignettes, shown in red callout boxes, to help exemplify how and why AI bias can reduce public trust. Interesting nuances/aspects are highlighted in blue callout boxes, important takeaways are shown as framed text. • Terms that are displayed as small caps in the text are defined in the GLOSSARY. Clicking on a word shown in small caps, e.g. MODEL, takes the reader directly to the definition of that term in the Glossary. From there, one may click on a page number shown at the end of the definition to return. • March 24, 2022 update: the following changes are introduced with respect to the original version of this document published on March 15, 2022: – Fixed typos in the text of Fig. 5 and Fig. 7. – Removed duplicates and fixed poorly formatted entries in the References. – Corrected a statement in the text of VIGNETTE on p.7 regarding the work cited in [36]. v/77 Contents 1 Purpose and Scope 1 2 AI Bias: Context and Terminology 3 2.1 Characterizing AI bias 3 2.1.1 Contexts for addressing AI bias 3 2.1.2 Categories of AI bias 6 2.2 How AI bias contributes to harms 9 2.3 A Socio-technical Systems Approach 10 2.4 An Updated AI Lifecycle 12 3 AI Bias: Challenges and Guidance 14 3.1 Who is Counted? Datasets in AI Bias 15 3.1.1 Dataset Challenges 15 3.1.2 Dataset Guidance 17 3.2 How do we know what is right? TEVV Considerations for AI Bias 20 3.2.1 TEVV Challenges 20 3.2.2 TEVV Guidance 27 3.3 Who makes decisions and how do they make them? Human Factors in AI Bias 32 3.3.1 Human Factors Challenges 32 3.3.2 Human Factors Guidance 35 3.4 How do we manage and provide oversight? Governance and AI Bias 42 3.4.1 Governance Guidance 42 4 Conclusions 47 5 Glossary 49 List of Figures Fig. 1 The challenge of managing AI bias i Fig. 2 Categories of AI Bias. The leaf node terms in each subcategory in the picture are hyperlinked to the GLOSSARY. Clicking them will bring up the definition in the Glossary. To return, click on the current page number (8) printed right after the glossary definition. 8 Fig. 3 The AI Development Lifecycle 13 Fig. 4 The output of an AI system altered by background content. 24 Fig. 5 How biases contribute to harms 27 Fig. 6 Human-centered Design Process [ISO 9241-210:2019] 40 Fig. 7 Human-centered Design Process for AI Systems 41 vi 1. Purpose and Scope In August 2019, fulfilling an assignment in an Executive Order on AI,5 NIST released “A Plan for Federal Engagement in Developing Technical Standards and Related Tools” [1]. Based on broad public and private sector input, this plan recommended a deeper, more consistent, and long-term engagement in AI standards “to help the United States to speed the pace of reliable, robust, and trustworthy AI technology development.” NIST research in AI continues along this path to focus on how to measure, evaluate, and enhance the trustworthiness of AI systems and the responsible practices for designing, developing, and deploying such systems. Working with the AI community, NIST has identified the follow- ing technical and socio-technical characteristics needed to cultivate trust in AI systems: accuracy, explainability and interpretability, privacy, reliability, robustness, safety, and se- curity resilience—and that harmful biases are mitigated or controlled. While AI has significant potential as a transformative technology, it also poses inher- ent risks. Since trust and risk are closely related, NIST’s work in the area of trustworthy and responsible AI centers around development of a voluntary Risk Management Frame- work (RMF). The unique challenges of AI require a deeper understanding of how AI risks differ from other domains. The NIST AI RMF is intended to address risks in the de- sign, development, use, and evaluation of AI products, services, and systems for such tasks as recommendation, diagnosis, pattern recognition, and automated planning and decision- making. The framework is intended to enable the development and use of AI in ways that will increase trustworthiness, advance usefulness, and address potential harms. NIST is leveraging a multi-stakeholder approach to creating and maintaining actionable practice guides via the RMF that is broadly adoptable. AI risk management AI risk management seeks to minimize anticipated and emergent negative impacts of AI systems, including threats to civil liberties and rights. One of those risks is bias. Bias exists in many forms, is omnipresent in society, and can become ingrained in the automated systems that help make decisions about our lives. While bias is not always a negative phenomenon, certain biases exhibited in AI models and systems can perpetuate and amplify negative impacts on individuals, organizations, and society. These biases can also indirectly reduce public trust in AI. There is no shortage of examples where bias in some aspect of AI technology and its use has caused harm and negatively impacted lives, such as in hiring, [2–7] health care, [8–17] and criminal justice [18–30]. Indeed, there are many instances in which the deployment of AI technologies have been accompanied by concerns about whether and how societal biases are being perpetuated or amplified [31–46]. Public perspectives Depending on the application, most Americans are likely to be unaware of when they are 5Exec. Order No. 13,859, 84 Fed. Reg. 3,967 (Feb. 11, 2019), https://www.federalregister.gov/documents/ 2019/02/14/2019-02544/maitaining-american-leadership-in-artificial-intelligence. 1/77 interacting with AI enabled technology [47]. However, there is a general view that there needs to be a “higher ethical standard” for AI than for other forms of technology [48]. This mainly stems from the perceptions and fears about loss of control and privacy [46, 49–51]. Bias is tightly associated with the concepts of transparency and fairness in society. For much of the public, the assumptions underlying algorithms are rarely transparent. The com- plex web of code and decisions that went into the design, development, and deployment of AI rarely is easily accessible or understandable to non-technical audiences. Nevertheless, many people are affected by—or their data is used as inputs for—AI technologies and sys- tems without their consent, such as when they apply to college, [52] for a new apartment, [53] or search the internet. When individuals feel that they are not being fairly judged when applying for jobs [2–5, 7, 54–56] or loans [57–59] it can reduce public trust in AI technology [60, 61]. When an end user is presented with information online that stigmatizes them based on their race, age, or gender, or doesn’t accurately perceive their identity, it causes harm [34, 36, 37, 41]. Consumers can be impacted by price gouging practices resulting from an AI application, even when it is not used to make decisions directly affecting that individual [43]. 2/77 2. AI Bias: Context and Terminology For purposes of this publication, the term Artificial Intelligence (AI) refers to a large class of software-based systems that receive signals from the environment and take actions that affect that environment by generating outputs such as content, predictions, recommenda- tions, classifications, or decisions influencing the environments they interact with, among other outputs [62]. Machine learning (ML) refers more specifically to the “field of study that gives computers the ability to learn without being explicitly programmed,” [63] or to computer programs that utilize data to learn and apply patterns or discern statistical rela- tionships. Common ML approaches include, but are not limited to, regression, random forests, support vector machines, and artificial neural networks. ML programs may or may not be used to make predictions of future events. ML programs also may be used to create input for additional ML programs. AI includes ML within its scope. While AI holds great promise, the convenience of automated classification and discov- ery within large datasets can come with significant downsides to individuals and society through the amplification of existing biases. Bias can be introduced purposefully or inad- vertently into an AI system, or it can emerge as the AI is used in an application. Some types of AI bias are purposeful and beneficial. For example, the ML systems that underlie AI applications often model our implicit biases with the intent of creating positive expe- riences for online shopping or identifying content of interest [64, 65]. The proliferation of recommender systems and other modeling and predictive approaches has also helped to expose the many negative social biases baked into these processes, which can reduce public trust [66–69]. AI is neither built nor deployed in a vacuum, sealed off from societal realities of dis- crimination or unfair practices. Understanding AI as a socio-technical system acknowl- edges that the processes used to develop technology are more than their mathematical and computational constructs. A socio-technical approach to AI takes into account the val- ues and behavior modeled from the datasets, the humans who interact with them, and the complex organizational factors that go into their commission, design, development, and ultimate deployment. 2.1 Characterizing AI bias 2.1.1 Contexts for addressing AI bias Statistical context In technical systems, bias is most commonly understood and treated as a statistical phe- nomenon. Bias is an effect that deprives a statistical result of representativeness by system- atically distorting it, as distinct from a random error, which may distort on any one occasion but balances out on the average [70]. The International Organization for Standardization (ISO) defines bias more generally as: “the degree to which a reference value deviates from the truth”[71]. In this context, an AI system is said to be biased when it exhibits system- atically inaccurate behavior. This statistical perspective does not sufficiently encompass or 3/77 communicate the full spectrum of risks posed by bias in AI systems. Legal context This section was developed in response to public comments. Stakeholder feedback noted that the discussion of bias in AI could not be divorced from the treatment of bias in the U.S. legal system and how it relates to laws and regulations addressing discrimination and fairness, especially in the areas of consumer finance, housing, and employment.6,7 There currently is no uniformly applied approach among the regulators and courts to measuring impermissible bias in all such areas. Impermissible discriminatory bias generally is defined by the courts as either consisting of disparate treatment, broadly defined as a decision that treats an individual less favorably than similarly situated individuals because of a protected characteristic such as race, sex, or other trait, or as disparate impact, which is broadly defined as a facially neutral policy or practice that disproportionately harms a group based on a protected trait.8 This section is presented not as legal guidance, rather as a reminder for developers, deployers, and users of AI that they must be cognizant of legal considerations in their work, par- ticularly with regard to bias testing. This section provides basic background understanding of some of the many ways bias is treated in some federal laws. As it relates to disparate impact, courts and regulators have utilized or considered as acceptable various statistical tests to evaluate evidence of disparate impact. Traditional methods of statistical bias testing look at differences in predictions across protected classes, such as race or sex. In particular, courts have looked to statistical significance testing to assess whether the challenged practice likely caused the disparity and was not the result of chance or a nondiscriminatory factor.9 6Many laws, at the federal, state and even municipal levels focus on preventing discrimination in a host of areas. See e.g. Title VII of the Civil Rights Act, regarding discrimination on the basis of sex, religion, race, color, or national origin in employment, the Equal Credit Opportunity Act, focused, broadly, on dis- crimination in finance, the Fair Housing Act, focused on discrimination in housing, and the Americans with Disabilities Act, focused on discrimination related to disabilities, among others. Other federal agencies, including the U.S. Equal Employment Opportunity Commission, the Federal Trade Commission, the U.S. Department of Justice, and the Office Federal Contract Compliance Programs are responsible for enforce- ment and interpretation of these laws. 7Note that the analysis in this section is not intended to serve as a fully comprehensive discussion of the law, how it has been interpreted by the courts, or how it is enforced by regulatory agencies, but rather to provide an initial high-level overview. 8See 42 U.S.C. 2000e-2(a) (2018) and 42 U.S.C. 2000e-2(k) (2018), respectively. 9The Uniform Guidelines on Employment Selection Procedures (UGESP) state “[a] selection rate for any race, sex, or ethnic group which is less than four-fifths ( 4/5ths) (or eighty percent) of the rate for the group 4/77 It is important to note, however, that the tests used to measure bias are not applied uniformly within the legal context. In particular, federal circuit courts are split on whether to require a plaintiff to demonstrate both statistical and practical significance to make out a case of disparate impact. Some decisions have expressly rejected practical significance tests in recent years while others have continued to endorse their utility. This split illustrates that while the legal context provides several examples of how bias and fairness has been quantified and adjudicated over the last several decades, the relevant standards are still evolving. It is also important to note that critical differences exist between traditional disparate impact analyses described above and illegal discrimination as it relates to people with dis- abilities, particularly under the Americans with Disabilities Act (ADA). Claims under the ADA are frequently construed as “screen out” rather than as “disparate impact” claims. ”Screen out” may occur when an individual with a disability performs poorly on an evalua- tion or assessment, or is otherwise unable to meet an employer’s job requirements, because of a disability and the individual loses a job opportunity as a result. In addition, the ADA’s prohibition against denial of reasonable accommodation, for example, may require an em- ployer to change processes or procedures to enable a particular individual with a disability to apply for a job, perform a job, or enjoy the benefits and privileges of employment. Such disability-related protections are particularly important to AI systems because testing an algorithm for bias by determining whether such groups perform equally well may fail to detect certain kinds of bias. Likewise, eliminating group discrepancies will not necessarily prevent screen out or the need for reasonable accommodation in such systems. Cognitive and societal context The teams involved in AI system design and development bring their cognitive biases, both individual and group, into the process [72]. Bias is prevalent in the assumptions about which data should be used, what AI models should be developed, where the AI system should be placed — or if AI is required at all. There are systemic biases at the institu- tional level that affect how organizations and teams are structured and who controls the decision making processes, and individual and group heuristics and cognitive/perceptual biases throughout the AI lifecycle (as described in Section 2.4). Decisions made by end users, downstream decision makers, and policy makers are also impacted by these biases, can reflect limited points of view and lead to biased outcomes [73–78]. Biases impacting human decision making are usually implicit and unconscious, and therefore unable to be easily controlled or mitigated [79]. Any assumption that biases can be remedied by human control or awareness is not a recipe for success. with the highest rate will generally be regarded by the Federal enforcement agencies as evidence of adverse impact.” 29 C.F.R. § 1607.4(D) 5/77 2.1.2 Categories of AI bias Based on previous academic work to classify AI bias [80–90] and discussions with thought leaders in the field, it is possible to identify three dominant categories of AI bias. This three- way categorization helps to expand our understanding of AI bias beyond the computational realm. By defining and describing how systemic and human biases present within AI, we can build new approaches for analyzing, managing, and mitigating bias and begin to understand how these biases interact with each other. Correspondingly, Fig. 2 presents three categories of AI bias. Definitions for these terms are found in the GLOSSARY.10 This list of biases, while not exhaustive, constitutes prominent risks and vulnerabilities to consider when designing, developing, deploying, evaluating, using, or auditing AI applications. Systemic Systemic biases result from procedures and practices of particular institutions that operate in ways which result in certain social groups being advantaged or favored and others be- ing disadvantaged or devalued. This need not be the result of any conscious prejudice or discrimination but rather of the majority following existing rules or norms. Institutional racism and sexism are the most common examples [91]. Other systemic bias occurs when infrastructures for daily living are not developed using universal design principles, thus limiting or hindering accessibility for persons with disabilities. Systemic bias is also re- ferred to as institutional or historical bias. These biases are present in the datasets used in AI, and the institutional norms, practices, and processes across the AI lifecycle and in broader culture and society. See VIGNETTE for more examples. 10Definitions for each category of bias were often selected based on either recently published papers on the topic, or seminal work within the domain the term is most associated with. When multiple definitions were identified, the most relevant definition was selected or adapted. The references provided are not intended to indicate specific endorsement or to assign originator credit. 6/77 Systemic bias in gender identification Beyond personal identity, human faces encode a number of conspicuous traits such as nonverbal expression, indicators of sexual attraction and selection, and emotion. Facial recognition technology (FRT) is used in many types of appli- cations including gender identification, which compares morphological distances between faces to classify human faces by gender. The degree of sexual dimor- phism between men and women appears to vary with age and ethnic group. As a consequence, accuracy of FRT gender identification can vary with respect to the age and ethnic group [92]. Prepubescent male faces are frequently misclassified as female, and older female faces are progressively misclassified as male [92]. Stud- ies have highlighted that human preferences for sexually dimorphic faces may be evolutionarily novel [93, 94]. One study found differing levels of facial sexual di- morphism in samples taken from countries located in Europe, South America, and Africa [95]. Buolamwini and Gebru examined the suitability of using skin types as a proxy for demographic classifications of ethnicity or race and found that skin type is not an adequate proxy for such classifications. Multiple ethnicities can be represented by a given skin type, and skin type can vary widely within a racial or ethnic category. For example, the skin types of individuals identifying as Black in the U.S. can represent many hues, which also can be represented in ethnic His- panic, Asian, Pacific Islander and American indigenous groups. Moreover, racial and ethnic categories tend to vary across geographies and over time [36]. While training data based on a limited or non-representative sample of a group results in lower accuracy in categorizing members of that group, the degree of sexual monomorphism or dimorphism within that group also affects accuracy. Additional biases can occur due to a lack of awareness about the multiplicity of gender [96]. 7/77 SYSTEMIC BIAS HUMAN BIAS STATISTICAL/ COMPUTATIONAL BIAS historical societal institutional SELECTION AND SAMPLING USE AND INTERPRETATION PROCESSING/VALIDATATION INDIVIDUAL INDIVIDUAL GROUP data generation; detection; ecological fallacy; evaluation; exclusion; measurement; popularity; population; representation; Simpson’s Paradox; temporal; uncertainty. activity; concept drift; emergent; content production; data dredging; feedback loop; linking. amplification; inherited; error propagation; model selection; survivorship. groupthink; funding; deployment; sunk cost fallacy. behavioral; interpretation; Rashomon effect or principle; selective adherence; streetlight effect; annotator reporting; human reporting; presentation; ranking. automation complacency; consumer; mode confusion; cognitive; anchoring; availability heuristic; confirmation; Dunning–Kruger effect; implicit; loss of situational awareness; user interaction. Fig. 2. Categories of AI Bias. The leaf node terms in each subcategory in the picture are hyperlinked to the GLOSSARY. Clicking them will bring up the definition in the Glossary. To return, click on the current page number (8) printed right after the glossary definition. 8/77 Statistical and Computational Statistical and computational biases stem from errors that result when the sample is not representative of the population. These biases arise from systematic as opposed to random error and can occur in the absence of prejudice, partiality, or discriminatory intent [97]. In AI systems, these biases are present in the datasets and algorithmic processes used in the development of AI applications, and often arise when algorithms are trained on one type of data and cannot extrapolate beyond those data. The error may be due to heterogeneous data, representation of complex data in simpler mathematical representations, wrong data, and algorithmic biases such as over- and under-fitting, the treatment of outliers, and data cleaning and imputation factors. Human Human biases reflect systematic errors in human thought based on a limited number of heuristic principles and predicting values to simpler judgmental operations [98]. These biases are often implicit and tend to relate to how an individual or group perceives infor- mation (such as automated AI output) to make a decision or fill in missing or unknown information. These biases are omnipresent in the institutional, group, and individual de- cision making processes across the AI lifecycle, and in the use of AI applications once deployed. There is a wide variety of human biases. Cognitive and perceptual biases show themselves in all domains and are not unique to human interactions with AI. Rather, they are a fundamental part of the human mind. There is an entire field of study centered around biases and heuristics in thinking, decision-making, and behavioral economics for example [98]. Such research investigates phenomena such as ANCHORING BIAS, availability heuris- tic or bias, CONFIRMATION BIAS, and framing effects, among many others. It should be noted that heuristics are adaptive mental shortcuts that can be helpful, allowing complexity reduction in tasks of judgement and choice, yet can also lead to cognitive biases [98]. Hu- man heuristics and biases are implicit; as such, simply increasing awareness of bias does not ensure control over it. Here we focus on broader examples of human bias in the AI space. 2.2 How AI bias contributes to harms Technology based on AI has tighter connections to and broader impacts on society than traditional software. Applications that utilize AI are often deployed across sectors and contexts for decision-support and decision-making. In this role, they can replace humans and human processes for high-impact decisions. For example, AI-based hiring technolo- gies and the models that underlie them replace people-oriented hiring processes and are implemented in any sector that seeks to automate their recruiting and employment pipeline [99–101]. Yet, ML models tend to exhibit “unexpectedly poor behavior when deployed in real world domains” without domain-specific constraints supplied by human operators [102]. These contradictions are a cause for considerable concern with large language mod- els (or so-called foundation models) due to their considerable EPISTEMIC and ALEATORIC 9/77 uncertainty[103] (as described in Section 3.2.1)—among other factors. Methods for cap- turing the poor performance, harmful impacts and other results of these models currently are imprecise and non-comprehensive. Values While ML systems are able to model complex phenomena, whether they are capable of learning and operating in line with our societal values remains an area of considerable re- search and concern [55, 60, 104–109]. Systemic and implicit biases such as racism and other forms of discrimination can inadvertently manifest in AI through the data used in training, as well as through the institutional policies and practices underlying how AI is commissioned, developed, deployed, and used. Statistical/algorithmic and human cogni- tive and perceptual biases enter the engineering and modeling processes themselves, and an inability to properly validate model performance leaves these biases exposed during de- ployment [61, 102, 110, 111]. These biases collide with the cognitive biases of the individ- uals interacting with the AI systems as users, experts in the loop, or other decision makers. Teams that develop and deploy AI often have inaccurate expectations of how the technol- ogy will be used and what human oversight can accomplish, especially when deployed outside of its original intent [112, 113]. Left unaddressed, these biases and accompanying contextual factors can combine into a complex and pernicious mixture. These biases can negatively impact individuals and society by amplifying and reinforcing discrimination at a speed and scale far beyond the traditional discriminatory practices that can result from implicit human or institutional biases such as racism, sexism, ageism or ableism. 2.3 A Socio-technical Systems Approach Likely due to expectations based on techno-solutionism and a lack of mature AI process governance, organizations often default to overly technical solutions for AI bias issues. Yet, these mathematical and computational approaches do not adequately capture the societal impact of AI systems [61, 73, 75, 111]. The limitations of a computational-only perspective for addressing bias have become evident as AI systems increasingly expand into our lives. The reviewed literature suggests that the expansion of AI into many aspects of public life requires extending our view from a mainly technical perspective to one that is socio- technical in nature, and considers AI within the larger social system in which it operates [7, 19, 31, 37, 74, 75, 78, 114–119]. Using a socio-technical approach to AI bias makes it possible to evaluate dynamic systems of bias and understand how they impact each other and under what conditions these biases are attenuated or amplified. Adopting a socio- technical perspective can enable a broader understanding of AI impacts and the key de- cisions that happen throughout, and beyond, the AI lifecycle–such as whether technology is even a solution to a given task or problem [3, 108]. Reframing AI-related factors such as datasets, TEVV, participatory design, and human-in-the-loop practices through a socio- technical lens means understanding how they are both functions of society and, through the power of AI, can impact society. A socio-technical approach also enables analytic 10/77 approaches that take into account the needs of individuals, groups and society. Techno-solutionism As computational technologies have evolved, there has been an increasing tendency to believe that technical solutions alone are sufficient for addressing complex problems that may have social, political, ecological, economic, and/or ethical dimensions. This approach to problem-solving, often termed techno- solutionism,[120] assumes that the “right” code or algorithm can be applied to any problem and ignores or minimizes the relevance of human, organizational, and so- cietal values and behaviors that inform design, deployment, and use of technology. In the context of socio-technical AI systems, techno-solutionism promotes a view- point that is too narrow to effectively address bias risks. One control, for exam- ple, used in model risk management to mitigate against techno-solutionism and other anti-patterns, is to establish, document, and review the anticipated real-world value of an AI system. Socio-technical approaches in AI are an emerging area, and identifying measurement tech- niques to take these factors into consideration will require a broad set of disciplines and stakeholders. Identifying contextual requirements for evaluating socio-technical systems is necessary. Developing scientifically supportable guidelines to meet socio-technical re- quirements will be a core focus. 11/77 AI bias extends beyond computational algorithms and models, and the datasets upon which they are built. The assumptions and decisions made within the pro- cesses used to develop technology are key factors, as well as how AI technology is used and interpreted once deployed. The idea that quantitative measures are better and more objective than other observations is known as the MCNAMARA FALLACY. This fallacy, and the related concept TECHNOCHAUVINISM [35], are at the center of many of the issues related to algorithmic bias. Traditional ML approaches attempt to turn ambiguity, context, human subjectivity, and cate- gorical observations into objectively measurable quantities based on numerical mathematical models of their representations. This well-intentioned process enables data-driven modeling but it also inadvertently creates new challenges for socio-technical systems. Representing these complex human phenomena with mathematical models comes at the cost of disentangling the context necessary for understanding individual and societal impact and contributes to a fallacy of objectivity [121]. Science has made great strides in understanding the limitations of human cognition, including how humans perceive, learn, and store visual, aural, and textual information, and make decisions under risk. Yet, significant gaps remain. Thus, any mathematical attempt to model such human traits is limited and incomplete. This is a key challenge in model causality and predicting human interpretation of model output. And without proper governance, excising context and flattening the categories into numerical constructs makes traceability more difficult [122]. Finding approaches in TEVV to compensate for these limitations in the un- derlying modeling technology and bringing back the necessary context is an important area of study. 2.4 An Updated AI Lifecycle Improving trust in AI by mitigating and managing bias starts with identifying a structure for how it presents within AI systems and uses. Organizations that design and develop AI technology use the AI lifecycle to keep track of their processes and ensure delivery of high-performing functional technology—but not necessarily to identify harms or manage them. This document has adapted a four-stage AI lifecycle from other stakeholder ver- sions.11 The intent is to enable AI designers, developers, evaluators and deployers to relate 11AI lifecycles utilized as key guidance in the development of the four-stage approach are: Centers of Ex- cellence (CoE) at the U.S. General Services Administration [70] [IT Modernization CoE. (n.d.)], the Or- ganisation for Economic Co-operation and Development [106] [Organisation for Economic Co-operation and Development. (2019).]. Another model of the AI lifecycle is currently under development with the Joint Technical Committee of the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC). See Information technology — Artificial intelligence — AI system life cycle processes, ISO/IEC CD 5338 (under development, 1st ed.), https://www.iso.org/standard/81118.html. 12/77 lifecycle processes with AI bias categories and effectively facilitate its identification and management. The academic literature and best practice guidelines strongly encourage a multi-stakeholder approach to developing AI applications using a lifecycle. Guidance for how organizations can enable this approach is described in Section 3.3.2 and focuses on participatory design methods such as human-centered design. Test &  Evaluation Pre-Design Design &  Development Deployment Fig. 3. The AI Development Lifecycle AI Lifecycles are iterative, and begin in the Pre-Design stage, where plan- ning, problem specification, background research, and identification of data take place. Decisions here include how to frame the problem, the purpose of the AI component, and the general notion that there is a problem requiring or benefit- ing from AI. Central to these decisions is who (individuals or groups) makes them and which individuals or teams have the most power or control over them. These early decisions and who makes them can reflect systemic biases within orga- nizational settings, individual and group heuristics, and limited points of view. Systemic biases are also reflected in the datasets selected within pre-design. All of these biases can affect later stages and decisions in complex ways, and lead to biased outcomes [3, 74–78]. The Design and Development stage typically starts with analysis of the requirements and the available data. Based on this, a model is designed or selected. A compatibility anal- ysis should be performed to ensure that potential sources of bias are identified and plans for mitigation are put into place. As model implementation progresses and is trained on selected data, the effectiveness of bias mitigation should be evaluated and adjusted.During development the organization should periodically assess the completeness of bias iden- tification processes as well as the effectiveness of mitigation. Finally, at the end of the development stage, and before deployment, a thorough assessment of bias mitigation is necessary to ensure the system stays within pre-specified limits. The overall model speci- fication must include the identified sources of bias, the implemented mitigation techniques and related performance assessments before the model can be released for deployment. The Deployment stage is when the AI system is released and used. Once humans begin to interact with the AI system the performance of the system must be monitored and reassessed to ensure proper function. Teams should engage in continuous monitoring and have detailed policies and procedures for how to handle system output and behavior. System retraining may be necessary to correct adverse events, or decommission may be necessary. Since the lifecycle is iterative there are numerous opportunities for technology development teams to carry out multi-stakeholder consultation and ensure their applications 13/77 are not causing unintended effects or harms. Specific guidance for governing systems under these conditions is the subject of Section 3.4.1. The Test and Evaluation stage is continuous throughout the entire AI Development Lifecycle. Organizations are encouraged to perform continuous testing and evaluation of all AI system components and features where bias can contribute to harmful impacts. For example, if during deployment the model is retrained with new data for a specific context, the model deployer should work with the model producer to assess actual performance for bias evaluation. Multi-stakeholder engagement is encouraged to ensure that the assessment is balanced and comprehensive. If deviations from desired goals are observed, the findings should feed into the model Pre-Design stage to ensure appropriate adjustments are made in data curation and problem formulation. Any proposed changes to the design of the model should then be evaluated together with the new data and requirements to ensure compatibility and identification of any potential new sources of bias. Then another round of design and implementation commences to formulate corresponding requirements for the new model capabilities and features and for additional datasets. During this stage, the model developer should perform continuous testing and evaluation to ensure that bias mitigation maintains effectiveness in the new setting, as the model is optimized and tested for performance. Once released, the deploying organization should use documented model specifications to test and evaluate bias characteristics during deployment in the specific context. Ideally, this evaluation should be performed together with other stakeholders to ensure all previously identified problems are resolved to everyone’s satisfaction. The most accurate model is not necessarily the one with the least harmful impact [123]. 3. AI Bias: Challenges and Guidance Through a review of the literature, and various multi-stakeholder processes, including pub- lic comments, workshops, and listening sessions, NIST has identified three broad areas that present challenges for addressing AI bias. The first challenge relates to dataset factors such as availability, representativeness, and baked-in societal biases. The second relates to issues of measurement and metrics to support testing and evaluation, validation, and verification (TEVV). The third area broadly comprises issues related to human factors, including so- cietal and historic biases within individuals and organizations, as well as challenges related to implementing human-in-the-loop. This section outlines some key challenges associated with each of these three areas, along with recommended guidance. It must be noted that TEVV does not amount to a full application of the scientific method. TEVV is an engineering construct that seeks to detect and remediate problems in a post-hoc fashion. The scientific method compels more holistic design thinking through 14/77 rigorous experimental design, hypothesis generation, and hypothesis testing. In particular, anecdotal evidence and the frequency of publicly-recorded AI bias incidents indicate that solid experimental design techniques that focus on structured data collection and selection and minimization of CONFIRMATION BIAS are being downplayed in many AI projects. CONSTRUCT VALIDITY is particularly important in AI system development. AI develop- ment teams should be able to demonstrate that the application is measuring the concept it intends to measure. It is important for all stakeholders, including AI development teams, to know how to evaluate scientific claims. That said, all the bias mitigants and gover- nance processes outlined in this document do show promise. Interestingly, they are often borrowed from practices outside of core AI and ML — even technical guidance related to improved experimental design and more rigorous application of the scientific method. None are a panacea. All have pitfalls. NIST plans to work with the trustworthy and re- sponsible AI communities to explore the proposed mitigants and governance processes, and build associated formal technical guidance over the coming years in concert with these communities. The challenge of bias in AI is complex and multi-faceted. While there are many approaches for mitigating this chal- lenge there is no quick fix.The recommendations in this document include a sampling of potentially promising tech- niques. These approaches, individually or in concert, are not a panacea against bias and each brings its own strengths and weaknesses. 3.1 Who is Counted? Datasets in AI Bias 3.1.1 Dataset Challenges AI design and development practices rely on large scale datasets to drive ML processes. This ever-present need can lead researchers, developers, and practitioners to first “go where the data is,” and adapt their questions accordingly [124]. This creates a culture focused more on which datasets are available or accessible, rather than what dataset might be most suitable [108]. As a result, the data used in these processes may not be fully representa- tive of populations or the phenomena that are being modeled. The data that is collected can differ significantly from what occurs in the real world [76, 77, 117]. For example, sampling bias occurs when data is collected from responses to online questionnaires or is scraped from social media. The datasets which result are based on samples that are neither randomized nor representative of a population other than the users of a particular online platform. Such datasets are not generalizable, yet frequently are used to train ML appli- cations which are deployed for use in broader socio-technical contexts, even though data representing certain societal groups may be excluded [116]. Systemic biases may also be manifested in the form of availability bias when datasets that are readily available but not 15/77 fully representative of the target population (including proxy data) are used and reused as training data. Disadvantaged groups including indigenous populations, women, and dis- abled people are consistently underrepresented [37, 116, 125, 126]. Similarly, datasets used in natural language processing (NLP) often differ significantly from their real-world applications, [127] which can lead to discrimination [128] and systematic gaps in perfor- mance. Other issues arise due to the common ML practice of reusing datasets. Under such practices, datasets may become disconnected from the social contexts and time periods of their creation. Scholars are beginning to examine the ethical and adverse impact impli- cations of using data collected at a specific time for a specific purpose for uses that were not originally intended. Decontextualizing data raises questions related to privacy, consent, and internal validity of ML model results [129]. Even when datasets are representative, they may still exhibit entrenched historical and systemic biases, improperly utilize protected attributes, or utilize culturally or contextually unsuitable attributes. Developers sometimes exclude protected attributes, associated with social groups which have historically been discriminated against. However, this does not remedy the problem, since the information can be inadvertently inferred in other ways through proxy or latent variables. Latent variables such as gender can be inferred through browsing history, and race can be inferred through zip code. So models based on such variables can still negatively impact individuals or classes of individuals [73]. Thus, the proxies used in development may be both a poor fit for the concept or characteristic seeking to be measured, and reveal unintended information about persons and groups. There is also sensitivity related to attributes and inferences that do not receive protection under civil rights laws, but which may enable discrimination when inferred and used by an ML model, such as low income status. Alternately, when there is not sufficient knowledge or awareness of the socio-technical context of a process or phenomenon, the attributes that are collected for use in an ML application may not be universally applicable for modeling the different social groups or cultures who are analyzed using the application. For example, using (past) medical costs to predict the need for future health interventions leads to severe under-prediction of healthcare needs in groups that do not have sufficient access to health care, such as African Americans [14]. Protected attributes: A host of laws and regulations have been established to prohibit discrimination based on grounds such as race, sex, age, religious affiliation, national origin, and disability status, among others. Local laws can apply protections across a wide variety of groups and activities. Once end users start to interact with an AI system, any early design and development decisions that were poorly or incompletely specified or based on narrow perspectives can be exposed, leaving the process vulnerable to additive statistical or human biases [77]. By not designing to compensate for activity biases, algorithmic models may be built on data from 16/77 only the most active users, likely creating downstream system activity that does not reflect the intended or real user population [130, 131] resulting in potentially harmful impacts. In one example, by considering that ads for jobs in Science, Technology, Engineering and Mathematics (STEM) might be seen most often by men due to how marketing algorithms optimize for cost in ad placement, the women who were the intended audience of the ads never saw them [132] cf., VIGNETTE for details. Furthermore, feedback loops can result in disparity amplification in which marginalized individuals or groups are less likely to use an AI system and the subsequent training data are based on the most frequent users. For example, non-native English speakers are less likely to use a voice-enabled personal assistant and people living in transit deserts are often dependent on ride-hailing services. So, the experiences of these groups do not match the intended purpose or operation of the AI system. 3.1.2 Dataset Guidance A key question that must be asked for the development and deployment of an AI system is: do datasets exist that are fit or suitable for the purpose of the various applications, do- mains and tasks for which the AI system is being developed and deployed? Not only is the predictive behavior of the ML system determined by the data, but the data also largely de- fines the machine learning task itself [61]. The question of dataset fit or suitability requires attention to three factors: statistical methods for mitigating representation issues; processes to account for the socio-technical context in which the application is being deployed; and awareness of the interaction of human factors with the AI technical system at all stages of the AI lifecycle. When datasets are available, the set of metrics for demonstrating fairness are many, context-specific, and unable to be reduced to a concise mathematical definition [133]. Statistical Factors AI bias problems are exacerbated by the variety of statistical biases that are prevalent in the large scale datasets used in ML modeling. When these models are deployed for decision-based applications, often in high-risk settings and off-label uses, harms can be perpetuated and amplified. A major trend for addressing AI bias is to focus on balanced statistical representation in the datasets used in modeling processes. Simple but effective techniques, such as class imbalance measures or label imbalance measures, or analysis using statistical phenomena such as SIMPSON’S PARADOX,[134] can be used to detect bias in datasets, and sometimes help mitigate it [85, 135–138]. Numerous studies and software libraries invoke data rebal- ancing processes (e.g., [139]). Causal models and graphs may also be used to detect direct discrimination in the data [61, 85]. Generalized linear models require that variables are independent with little multicollinear- ity and that residuals are normally distributed and homoscedastic. Furthermore, common algorithmic techniques such as L1 and L2 regularization in ML cost functions assume that the variables are unimodal. However, data is often heterogeneous and multimodal espe- 17/77 cially when populations are not disaggregated by gender, age, race, or income. Thus, it is important to document and communicate the limitations of the applicability of AI outputs, whether a model is used for benchmarking, prediction, or classification. In many cases, practitioners train models on benchmark datasets and use them on real data in specific applications. However, it may not be possible to fully address mathematically the imbalances in representation and the heterogeneous nature of real-world heterogeneous datasets. A recent study highlighted serious errors in commonly used benchmark dataset [140]. Consequently, a model trained on biased and erroneous data may lead to biased and inaccurate predictions. Moreover, training a model on one dataset and using it to operate on another requires special care to account for potential differences in the distributions of the datasets that may further exacerbate the unfairness and errors of the model. Accounting for Socio-technical Factors While statistical methods are indeed necessary, they are not sufficient for addressing the AI bias challenges associated with datasets. Modeling processes have the intent of making contextual concepts measurable. Once the context has been removed, however, it is difficult to get it back, leading AI models to learn from inexact representations. Just as building codes are designed based on general principles, but designed to incorporate the specific geographic characteristics of a region, so too must the use of datasets in ML applications be adapted to take into the full spectrum of socio-technical factors of the context in which they are deployed. Word embeddings represent text data as positions in a high-dimensional mathe- matical space. Such a representation allows arithmetic (measurable comparisons) to be performed on words [141]. However, when text data are simplified as math- ematical objects, contextual information including homographs or idioms that do not fit neatly into the model may be lost. When asked to compute “doctor” - “fa- ther” + “mother” using this arithmetic, an AI system might respond with “nurse.” Is the AI system’s answer due to historical gender stereotypes in professions or due to the natural, close association of the gender-specific verb “nurse” with mother? In other scenarios, even when attempts are made to explicitly remove bias from training data, biases may still exist because of deep, complex connections within the text data [80, 142]. Attention to the socio-technical factors for an AI system is essential at all phases of the lifecycle, most importantly in design, development, and deployment. In the design phase, socio-technical analysis provides insights into social variations in the dynamics or charac- teristics of a phenomenon. This can help better frame questions for analysis and enable assessment of dataset fit. A socio-technical perspective in the development phase facili- tates selection of data sources and attributes, and explicitly integrates impact assessment as a complement to algorithmic accuracy. Studies have shown how it is possible to mathemat- ically address statistical bias in a dataset, then develop an algorithm which performs with 18/77 high accuracy, yet produce outcomes that are harmful to a social class and diametrically opposed to the intended purpose of the AI system [14]. The need for new ways to mea- sure the impact of AI systems is a current theme in the literature and the trustworthy and responsible AI research community. The practice of deploying AI in off-label uses, that is AI systems being applied to a task or within a social or organizational context for which it was not designed, must be approached with caution, especially in high-risk settings. Socio- technical analysis can help determine if such use, with modification, is both ethically and technically feasible. In all cases, a socio-technical perspective implicates adopting pro- cesses that include involving stakeholders, examining cultural dynamics and norms, and assessing societal impacts. AI technologies can be perfectly accurate and still contribute to harmful outcomes. Interaction of human factors and datasets Systemic institutional biases are captured in the datasets used to build the models underlying AI applications. These biases are com- pounded by the decisions and assumptions made by AI design and development teams about which datasets to use [129]. These decisions affect who and what gets counted, and who and what does not get counted. The issue of “flattening” the societal and behavioral factors within the datasets themselves is problematic, but often overlooked [66, 129, 143, 144]. The problem is further exacerbated by the variety of statistical biases that are preva- lent in the large scale datasets used in ML modeling. Human biases, whether conditioned socially or unconscious cognitive bias, are factors in data selection, curation, preparation and analysis processes. A person who annotates training data (for example, for gesture recognition and sentiment analysis) may impart their own perception biases. A person who chooses which data sources and variables to leave in or take out may do so in a way that aligns with a held belief. Data typically needs to be cleaned in some way, removing outliers and spurious data. Missing data may be imputed (replacing the missing values with nearest neighbors or extrapolated values) or removed entirely. Missing data may be more frequent in marginalized populations. Furthermore, because of compounding collection biases, missing and spurious data is often not random. Data analysis decisions such as the cardinal treatment of ordinal data in a Likert-scale or rating-scale data may lead to a biased estimator [145]. Processes for documenting poten- tial sources of human bias are essential but often overlooked elements for characterizing AI model transparency and explainability, in addition to addressing AI bias and fairness. As with statistical factors and socio-technical analysis, incorporating awareness and docu- mentation in the AI lifecycle helps to define limitations and ensure ethically and socially appropriate uses that do not perpetuate or amplify harms. See Section 3.3 for a more thor- ough discussion of challenges and guidance related to human factors and AI bias. 19/77 3.2 How do we know what is right? TEVV Considerations for AI Bias 3.2.1 TEVV Challenges Delegating decision-making to algorithms is appealing because ML systems produce more consistent decisions compared to humans [146]. However, AI systems do not work in a vacuum. Operational context, such as the jurisdiction and industry vertical in which a system operates, serves to frame fairness goals. Even the algorithm itself relies on data for training and performance tuning, which in turn can be assessed by a fairness metric. Therefore, when we consider the computational approaches to mitigating bias, we must take into consideration these three components together: algorithms, data, and fairness metrics. AI systems regularly model concepts that are—at best—only partially observable or capturable by data. Without direct measures for these highly complex considerations, AI development teams use proxies, which can create many risks [147]. For example, for “criminality,” a measurable index or construct, might be created from other information, such as arrests and convictions, which are used as PROXY variables for predicting a certain outcome—in this case, whether a certain individual is likely to be a repeat offender. In algorithmic hiring, an AI system might be developed using input variables such as “length of time in prior employment,” “productivity,” and “number of lost hours” as measurable proxies in lieu of the not directly measurable concept of “employment suitability.” The al- gorithm might also include a predictor variable such as distance from the employment site [148] because it might correlate with employees quitting their job due to long commutes or bad traffic. However, since “distance from the employment site” might disadvantage candidates from certain neighborhoods, and “length of time in prior employment” might disadvantage candidates who are unable to find stable transportation (or relate to other socio-economic factors) the AI system will contribute to biased outcomes. Epistemic and aleatoric uncertainty ML distinguishes two types of predictive uncertainty: EPISTEMIC and ALEATORIC [149]. For example, models produced by deep learning ML systems exhibit epistemic uncertainty in the parameters of the computed model. The model parameters are typically computed as the result of a nonconvex minimization of an appropriately chosen cost function. It is well known from mathematics that such a formulation of the problem does not have a unique solution [150, 151]. While epistemic uncertainty can be reduced by increasing the amount of representative training data, it cannot be fully eliminated. This can impact the behav- ior of a deep learning system in deployment when used with real-world data, especially when there is a mismatch in the distributions of the real and training data [102]. This can lead to undesirable effects on many of the AI system’s critical attributes (e.g., robustness, resilience), including inducing harmful bias. Even convex problems (e.g., multiple linear regression) may suffer from epistemic uncertainty when a decision variable is not included in the model. Another inherent type of uncertainty associated with machine learning is ALEATORIC. 20/77 It represents the uncertainty inherent in the data, e.g., the uncertainty in the label assigning process of the training dataset. Aleatoric uncertainty is the irreducible part of the predictive uncertainty. Since these two types of uncertainties (EPISTEMIC and ALEATORIC) are highly context-dependent, changing the context may blur the difference between them or even cause one to turn into the other. Thus, their characterization as reducible and irreducible is not absolute. For example, datasets containing overlapping samples with different attributes could be embedded into higher dimensions so that the samples are clearly separated, thus reducing aleatoric uncertainty at the expense of epistemic uncertainty - because the model would likely overfit the existing data in the larger space. Some of the difficulty in distin- guishing epistemic and aleatoric uncertainty is that ML models are (implicit) mathematical representations of the data on which they are trained [152]. The growth of Large Language Models Large LANGUAGE MODELs (LLMs) have become the dominant trend in deep learning to- day and are expected to continue to grow in importance [103, 153]. Although LLMs have been able to achieve impressive advances in performance on a number of important tasks, they come with significant risks that could potentially undermine public trust in the technol- ogy. LLMs create significant challenges for both EPISTEMIC and ALEATORIC uncertainty. Relying on large amounts of uncurated web data increases aleatoric uncertainty [154]. In- depth knowledge of the data and its statistical properties is critically important for detecting bias in the predictive output of ML models. Identifying sources of bias is the first step in any bias mitiga- tion strategy. 21/77 Epistemic uncertainty and large-scale AI models With the availability of large and fast computing resources, massive artificial neu- ral networks are becoming increasingly common. In particular, some language models now consist of trillion-dimensional parameter spaces trained on hundreds of gigabytes of data. The training data, often scraped from internet sources, com- monly has known gender, racial, cultural, and socio-economic biases [154, 155]. Alternative approaches to large-sized language datasets have been proposed to mitigate harmful bias, but such an approach may introduce other human biases in the selection of values-targeted datasets. Beyond the systemic and selection bi- ases, large language models also highlight EPISTEMIC UNCERTAINTY. Stochastic gradient descent (or other accelerated methods) methods [151] are used to find a set of parameters that minimize a cost function associated with the model, but deep neural networks exhibit complicated nonlinearities which result in many potential local minima. A trillion-dimensional manifold may have a huge, unknown num- ber of minima [156]. Furthermore, to fit these parameters into computer memory, it is often necessary to use half-precision floating-point numbers [157], introduc- ing rounding error which may undermine stability in the numerical methods [158]. As a result, the model may demonstrate unknown and erratic behavior and chal- lenges for reproducibility and explainability [159]. In the quest for fitting larger and larger models into existing finite computational re- sources, LLMs rely on techniques, e.g., reduced-precision numerical representations of models, that further increase the epistemic uncertainty of deep learning models, [160] cf., VIGNETTE. Early practice has shown that concerns about the use of LLMs are in- deed valid, with preliminary experimental results showing LLMs exhibit significant bias [154, 161, 162]. To reduce risks from the use of LLMs, future work in this area should move towards efforts to fully understand and characterize their behavior, and to devise effective mitigation measures against the biases they bring. Processes While datasets exhibit numerous biases that lead to harmful impacts, they feed directly into other system level processes that determine what is important to model. For AI systems to determine this importance, and effectively categorize and sort the firehose of data for downstream recommendations and decisions, contextual information is flattened and unob- servable phenomena are quantified through the development of indices and use of proxies. The use of data attributes with names like “criminality,” “hireability,” “creditworthiness,” or similar can be indicative of experimental design problems that give rise to harmful bias. The software designers and data scientists working in design and development are of- ten highly focused on system performance and optimization. This focus can inadvertently be a source of bias in AI systems. For example, during model development and selection, modelers will almost always select the most accurate models. Yet, as Forde et al describe in their paper, [163] selecting models based solely on accuracy is not necessarily the best 22/77 approach for bias reduction. Furthermore, the choice of the model’s objective function, upon which a model’s definition of accuracy is based, can reflect bias. Not taking context into consideration during model selection can lead to biased results for sub-populations (for example, disparities in health care delivery). Relatedly, systems that are designed to use aggregated data about groups to make predictions about individual behavior—a prac- tice initially meant to be a remedy for non-representative datasets[18]—can lead to biased outcomes. This bias, known as ECOLOGICAL FALLACY, occurs when an inference is made about an individual based on their membership within a group (for example, predicting college performance risk based on an individual’s race [52]). These unintentional weight- ings of certain factors can cause algorithmic results that exacerbate and reinforce societal inequities. Natural language processing (NLP) is a powerful computational approach to al- low machines to meaningfully understand human spoken and written languages. Powering activities such as algorithmic search, speech translation, and even con- versational text generation, NLP is able to help us communicate with computer systems to carry out a variety of tasks. The set of harms that can arise from the use of NLP however has become a recent concern in the area of trustworthy AI [80, 90, 154, 164, 165]. Hovy and Prabhumoye describe five sources of bias in NLP and potential ways to counteract it [166]. 23/77 Spurious Correlations The speed and scope of machine learning processes can unfortunately ex- pand the development of systems based on questionable scientific underpin- nings that learn spurious correlations related to human characteristics. For ex- ample, the German public radio outlet BR24 examined a system that purport- edly assessed tone of voice, language, gestures, and facial expressions to cre- ate a personality profile for use in hiring processes [6]. The analysis showed the AI system was easily manipulated by superficial changes to its inputs, Fig. 4. The output of an AI system altered by background content. awarding candidates higher scores when they wore glasses or when a bookshelf was in the background, diminishing claims that the sys- tem analyzed human expressions, and raising concerns about shortcut learning [167]. In- deed, many AI systems now attempt to make inferences about individuals based on their fa- cial characteristics that are not scientifically supportable, such as their propensity for com- mitting crimes or even their sexual orienta- tion [121, 168–172]. The basis for draw- ing conclusions about emotional state from facial characteristics ranges from unscientific and debunked theories to emerging experimen- tal studies [173], presenting concerning chal- lenges to AI systems that claim to make such judgements. By mechanizing human charac- teristics these systems can obfuscate significant uncertainty and result in harmful biases. AI-based hiring systems that claim to glean information about candidates from audio and video have been shown to increase bias in outcome decisions and may present untenable trade-offs between bias mitigation and prediction accuracy [174]. AI systems marketed as making predictions based on facial expressions often generate decisions based on biased experimental design premises [168] or spurious patterns learned by the system (e.g., shortcut learning). These cases illus- trate the risks associated with using AI systems for tasks like sentiment or affect analysis, along with using systems to infer spurious correlations more broadly, which can perpetuate biases across groups and, in several instances can be scien- tifically unsound [175]. AI systems in consequential or sensitive areas should not be built on the basis of spurious correlations. They can provide faux-objective jus- tification for biased outcomes. A socio-technical perspective broadens awareness of these risky computational approaches. The rise of predictive analytics as a mechanism for identifying patterns in human be- havior is a recent example of a process that can produce biased outcomes and therefore 24/77 should be used carefully. These applications can be highly effective at identifying key in- sights in data that are unable to be gleaned by humans [176]. This technology is also often presented and perceived as a way to reduce human cognitive biases and make decisions more fair and objective [27, 177, 178]. In well defined and constrained settings these tech- nologies can result in accurate and fair outcomes. However, the assumption that AI-based systems are more objective, especially in high stakes decision making, remains unclear. Categorizing unobservable behavior and phenomena leads to increased uncertainty in sys- tem performance. Measuring whether the patterns identified by these applications are real or a result of spurious correlations is difficult. Adding to the challenge is the reality that these systems are built and placed within organizational settings along with their accompa- nying — often unstated — policies and priorities, and used by subject matter experts and decision makers who have their own implicit heuristics and biases [179]. A fallacy of ob- jectivity can often surround these processes, and may create conditions where technology’s capacity and capabilities are oversold [121]. See VIGNETTE for an example. Algorithmic effects Algorithmic complexity can vary greatly across AI models. The number of parameters, which mathematically encode the training data, may be as few as one and as many as one trillion. Simple models with fewer parameters are often used because they tend to be less expensive to build, more explainable and more transparent, and easier to implement. However, such models can exacerbate statistical biases because restrictive assumptions on the training data often do not hold with nuanced demographics. Furthermore, designers who must make decisions on what variables to include or exclude can impart their own cognitive biases into the model [110, 180]. Complex models are often used on nonlinear, multimodal data such as text and images. Such models may capture latent systemic bias in ways that are difficult to recognize and predict. Expert systems, another AI paradigm, may encode cognitive and perceptual biases in the knowledge accumulated by practitioners from which the system is designed to emulate. Validity Ultimately, AI systems should demonstrate that they perform accurately, but how do we know what constitutes a “right answer”? Validating performance is a difficult but nec- essary endeavor for any system being deployed to the public and effective management and mitigation of AI bias. Many difficulties and flaws can arise in system validation. A common challenge in system testing is a lack of ground truth, or noisy labeling and other annotation factors which make it difficult to know what is accurate. The use of proxy vari- ables compounds this difficulty, since what is being measured isn’t directly observable. Performing system tests under optimal conditions — or conditions that are not close to the deployed state — is another challenging design flaw. System performance metrics are also difficult to generalize and can lead to issues with unintended use. Due to these challenges, subject matter experts should be relied upon during validation to create and oversee the most realistic possible validation processes [102]. Also the practice of “stratified perfor- 25/77 mance evaluations,” [102] where system performance is analyzed across segments in the training or test data, whether demographic segments or otherwise, is a basic consideration for understanding system validity across a population of users. Validation and deployment Validation also means ensuring that the system is not being used in unintended ways. DE- PLOYMENT BIAS happens when an AI model is used in ways not intended by developers. Emergent bias happens where the model is used in unanticipated contexts. Developers of an algorithm used by major U.S. cities to assist in coordinating housing to homeless people began phasing it out after several cities inappropriately used the algorithm as an assessment tool rather than as the presecreening tool as it was designed [181]. In another instance, the Chicago Police Department decommissioned an algorithm designed to predict the risk that an individual might be involved in future gun violence, citing unintended use and misap- plication of the model [182]. It is not uncommon for deployment to be used as system testing. Depending on the context, institutional review may not be required to carry out this type of testing [183]. Without system validation, an AI system could be released that is technically flawed or fails to establish appropriate underlying mechanisms for proper functioning [184–186]. A system could be deployed in a negligent manner, be based on pseudoscience or spurious correlations, prey on the user, or generally exaggerate claims. In such cases, the goal should not be to ensure applications are bias-free, but to reject the development outright in order to prevent disappointment or harm to the user as well as to the reputation of the provider. Such systems may also run afoul of existing legal frameworks that proscribe unfair, deceptive, and predatory practices (UDAP).12 This type of scenario may reinforce public distrust of AI technology since untested or technically flawed systems can contribute to bias and other harmful outcomes. AI systems as magic A further validation challenge of AI systems stems from their accessibility and hype. Physi- cist Richard Feynman referred to practices that superficially resemble science but do not follow the scientific method as cargo cult science. A core tenet of the scientific method is that hypotheses should be testable, experiments should be interpretable, and models should be falsifiable or at least verifiable. Commentators have drawn similarities between AI and cargo cult science citing its black box interpretability, reproducibility problem, and trial- and-error processes [187, 188]. High-level machine learning libraries and reduced costs of cloud computing have made AI more affordable and easier to develop. As a result, AI development is becoming increasingly democratized. Still, AI itself remains largely opaque—deep neural networks and Bayesian inference require advanced mathematics to understand. The DUNNING–KRUGER EFFECT is a cognitive bias in which a person with limited knowledge in a domain may vastly overestimate their understanding of that domain. 12See, e.g., Federal Trade Commission Act, Section 5. 26/77 Even among experts, data-driven technologies can exacer- bate CONFIRMATION BIAS, particularly when they are im- plicitly guided by expected outcomes. An analysis that ex- amined hundreds of AI algorithms for identifying COVID found that few of them were effective [189]. The danger is that with enough tweaking of hyperparameters across many candidate AI models, one of them may appear to be highly accurate even when measured against standard performance datasets. DATA DREDGING (also known as p-hacking) is a statistical bias in which testing huge numbers of hypotheses of a dataset may appear to yield statistical significance even when the results are statistically nonsignificant. Fig. 5 provides examples of how the three categories of bias — systemic, statistical and computational, and human - interact and contribute to harms within the data and processes used in AI applications, and the validation procedures for determining performance. Systemic Biases Statistical and  Computational Biases Human Biases Datasets Processes and  Human Factors TEVV Who is counted, and  who is not counted? What is important? How do we know  what is right? Issues with latent variables Underrepresentation of marginalized  groups Automation of inequalities Underrepresentation in determining  utility function Processes that favor the majority/minority Cultural bias in the objective function  (best for individuals vs best for the  group) Reinforcement of inequalities (groups  are impacted more with higher use of  AI) Predictive policing more negatively  impacted Widespread adoption of  ridesharing/self-driving cars/etc.  may change policies that impact  population based on use Sampling and selection bias Using proxy variables because they  are easier to measure Automation bias Likert scale (categorical to ordinal to  cardinal) Nonlinear vs linear Ecological fallacy Minimizing the L1 vs. L2 norm General difficulty in quantifying  contextual phenomena Lack of adequate cross-validation Survivorship bias Difficulty with fairness Observational bias (streetlight  effect) Availability bias (anchoring) McNamara fallacy Groupthink leads to narrow choices Rashomon effect leads to subjective  advocacy Difficulty in quantifying objectives  may lead to McNamara fallacy Confirmation bias Automation bias Fig. 5. How biases contribute to harms 3.2.2 TEVV Guidance To mitigate the risks stemming from epistemic and aleatoric uncertainties, model devel- opers should work closely with the organizations deploying them. Teams should work to ensure periodic model updates, and test and recalibrate model parameters on updated repre- sentative datasets to meet the business objectives while staying within desired performance targets and acceptable levels of bias. From a Bayesian inference perspective, this can be seen as updating the prior of the model to help avoid issues that may arise from using stale priors. Organizations are recommended to employ appropriate governance procedures to 27/77 adequately capture this cross-organizational need and ensure no negative impacts from us- ing the AI technology. Algorithms In ML, it is not meaningful to assign bias to the model or algorithm itself without con- textual information about the specific tasks on which they may be used. This links the model and algorithm to the dataset on which they are trained and tested (see VIGNETTE for how contextual factors can play a role in bias). The catchphrase “bias in, bias out” is widely used to describe the heavy dependence of the algorithmic behavior on the data. For example, in a natural language processing context, hate speech detection models use di- alect markers as toxicity predictors, which can result in bias against minority groups [190]. In another context, an algorithm designed to deliver gender-neutral advertisements about jobs in STEM resulted in gender bias due to younger women being considered a valuable subgroup and more expensive as the targets for advertisements [85, 132]. Methods that help to reduce algorithmic bias are another helpful construct for under- standing it. Specific methods for algorithmic mitigation of bias for many different machine learning tasks have been delineated or surveyed in recent studies [85, 191–194]. When considering approaches to mitigating algorithmic bias in a specific task context, recent lit- erature categorizes debiasing methods into one of three categories [61, 85, 191, 194]: 1. Pre-processing: transforming the data so that the underlying discrimination is mit- igated. This method can be used if a modeling pipeline is allowed to modify the training data. 2. In-processing: techniques that modify the algorithms in order to mitigate bias during model training. Model training processes could incorporate changes to the objective (cost) function or impose a new optimization constraint. 3. Post-processing: typically performed with the help of a holdout dataset (data not used in the training of the model). Here, the learned model is treated as a black box and its predictions are altered by a function during the post-processing phase. The function is deduced from the performance of the black box model on the holdout dataset. This technique may be useful in adapting a pre-trained large language model to a dataset and task of interest. 28/77 The limits of algorithmic transparency in eliminating bias Automated decision-making is appealing but comes with risks that can result in discriminatory outcomes. Researchers investigated settings where ads are allo- cated by algorithm and found instances where historically–discriminated–against– groups are less likely to see desirable ads [132]. In this setting, a field test was performed with an ad that was intended to promote job opportunities and train- ing in STEM. The STEM career ad campaign was motivated by widespread con- cern about a shortage of underrepresented groups in the STEM sector, particularly women. The assumption is that disseminating information about STEM careers to women and encouraging women to enter this field helps to address this problem. However, since women are far more likely to make decisions about household purchases, they are more valuable targets for advertising, creating pricing differ- entials for ad displays. The result of the ad campaign was that 20%+ more men than women viewed the ad, with the largest difference in the 25-54 year old age group. The findings in this study help demonstrate the difficulty of evaluating algorithms for preventing discrimination, and the need for a socio-technical lens on the chal- lenge. It is insufficient to look for bias in the algorithm alone. Relatedly, according to Lambrecht [132]: “One popular policy prescription has been a focus on algorithmic trans- parency where algorithmic codes are made public. Such policies are gaining increasing momentum - for example, the Federal Trade Commission (FTC) launched a new unit focused on algorithmic transparency, ... however, that algorithmic transparency would not have helped regulators to foresee uneven outcomes. The reason is that an examination of the algorithmic code would likely have revealed an algorithm focused on minimizing ad costs for advertisers. Without appropriate knowledge about the economic context and how such cost– minimization might affect the distribution of advertising, such ‘transparency’ would not have been particularly helpful.” While transparency into AI system mechanisms is rarely a direct bias mitigant, as explained above, transparency enables many critical AI governance functions. Transparency is very important, but should not be mistaken for fairness. In sectors of the U.S. economy where the Equal Credit Opportunity Act,13 influential court cases,14 or other legal and regulatory matters invoke the legal doctrine of Disparate Treatment, debiasing efforts may be less likely to explicitly include pre-, in-, and post- processing approaches, and instead rely on alternative modeling approaches. In consumer 13CFPB Supervision & Examination Manual, pt. II, § C, Equal Credit Opportunity Act (Oct. 2015). 14e.g., Ricci v. DeStefano, 557 U.S. 557 (2009). 29/77 finance and employment litigation, where the practice of bias remediation, e.g., debias- ing, has been pursued for decades, practitioners are more likely to consider adjustments to input variables or model hyperparameters to improve bias testing results or real-world outcomes. Demographic group membership, necessary for bias testing purposes, is often inferred using the Bayesian improved surname geocoding (BISG) process (see [195]). Modeling algorithms or debiasing techniques that rely on demographic information, as most pre-, in-, and post- processing methods do, may pose higher risks in regulated environments where disparate treatment must be avoided [196]. Fairness metrics From a computational standpoint, defining a fairness metric for ML requires developing a formal mathematical model to achieve desired predictive goals on a given dataset and associated task. Numerous fairness metrics are proposed in the literature [85, 191, 194, 197–199]. Much of the work in determining fairness criteria involves supervised learning, but the labeled data required for these tasks may not be readily available. This is particularly true for large language models, where the sheer scale of the datasets used for training is prohibitive for proper data labeling. This has a direct impact on both representativeness of the training data and, in turn, its impact on the representativeness of the generated model might exacerbate discriminatory outcomes, as large language models are adapted to specific datasets and tasks. Moreover, even if datasets are representative they may still exhibit biases or improperly utilize protected attributes, which in turn may lead to discrimination. Proxies may be used for hiding protected attributes and care should be taken to avoid discrimination resulting from badly chosen proxies [59, 136, 147, 200, 201]. And, even if proxies are used to hide protected attributes, they may still reveal sensitive information about individuals or groups [195, 202]. Recent literature [203] considers alternative learning tasks, e.g. unsupervised learning and reinforcement learning where only intermediate feedback is provided to the model, and tries to balance the effects of short- and long-term rewards. Several open questions still remain about the use and representativeness of synthetically generated data, in applications where little data is available. An emerging related line of research is to use simulations to evaluate the long-term impact of machine learning systems by incorporating elements of system level dynamics, feedback loops, and other long-term effects to make fair decisions in dynamic environments [204]. Another challenge, with serious social ramifications, is how to measure fairness in the emergent class of deployed generative models, such as large language models, computer vision systems, or deep fakes, whose outputs are free form text, audio or video [205]. 30/77 While academic research into mathematical notions of fairness has blossomed in recent years, procedures for testing fairness in regulatory and litigation settings such as employ- ment and consumer finance have been operational for decades, and reached a level of ma- turity before the recent increase in interest on the topic. In these areas, statistical tests can be applied to determine whether some automated decision-making system is acting out- side the bounds of applicable law. t-tests, χ2-tests, analysis of regression coefficients, and other traditional statistical tests can be used to show a statistically significant difference be- tween ML system outcomes across demographic groups. In some cases, measurements of differential validity are also used to ensure that applicants and employees receive roughly equal service from systems in employment, where system performance quality is evaluated across demographic groups.15 Credible attempts at bias mitigation should maintain align- ment with acknowledged legal standards. Generally, the majority of fairness metrics are observational as they can be expressed using probability statements involving the available random variables [61]. These metrics can be classified into many categories: fairness through unawareness, individual fairness, demographic parity, disparate impact, differential validity, proxy discrimination, equality of opportunity, etc. However, not all critically important lines of inquiry can be answered through observations alone. Moreover, depending on the relationship between a protected attribute and the data, certain observational definitions of fairness can increase discrimina- tion. Hence, research to improve fairness metrics continues. For instance, a counterfactual fairness definition has been developed [199] to capture the intuition that a decision is fair towards an individual if it is the same in both the actual world and a counterfactual world— where the individual belongs to a different demographic group. Simulations can also be used to gain counterfactual information about how the data would have varied if a different data collection or decision-making policy had been in place [204]. As algorithmic discrim- ination can arise from the encoding of spurious correlations and noisy local dependencies into ML systems during training, there is currently great focus on causal tools [206] and how they can formally incorporate effects of hypothetical actions to solve a wide range of fairness modeling problems. Until causal methods are more widely available and adopted, minimizing the number of input variables, and ensuring that there is no strong correlation amongst them and a logical relationship to the prediction target, is a mitigation tactic for proxy discrimination and other AI risks. 15See, e.g., U.S. v. Ga. Power Co., 474 F.2d 906 (5th Cir. 1973). 31/77 When deciding which fairness metric to adopt, it is important to recognize the impossibility of satisfying certain mathemat- ical fairness constraints at once except in highly constrained special cases [207]. For example, there is an inherent incom- patibility between two conditions: calibration and balancing the positive and negative classes. These conditions cannot be satisfied simultaneously unless under certain constraints [78]. While not all mathematical fairness desiderata can be achieved simultaneously, it is important to note that mitigated bias and good performance can be achieved simultaneously [208]. The plethora of fairness metric definitions illustrates that fairness cannot be reduced to a concise mathematical definition. Fairness is dynamic, social in nature, application and context specific, and not just an abstract or universal statistical problem. Therefore, it is important to adopt a socio-technical approach to fairness in order to have realistic fair- ness definitions for different contexts as well as task-specific datasets for machine learning model development and evaluation. 3.3 Who makes decisions and how do they make them? Human Factors in AI Bias 3.3.1 Human Factors Challenges As ML algorithms have evolved in accuracy and precision, computational systems have moved from being used purely for decision support—or for explicit use by and under the control of a human operator—to automated decision making with limited input from hu- mans. Computational decision support systems augment another, typically human, system in making decisions. Comparatively, for algorithmic decision systems there is less human involvement, with the AI system itself more in the “driver’s seat,” and able to produce out- comes with little human involvement to govern the impact. The growth and prevalence of algorithmic decision systems has helped to drive a decreased sense of trust in AI among the public [209]. This distrust is exacerbated by the reality that historical and social biases are baked-in to the data and assumptions used in the algorithmic models generating automated decisions. As a result, these algorithmic models have a higher probability of producing and amplifying unjust outcomes (e.g. for racial and ethnic minorities in areas such as criminal justice) [18–30, 210]. The systemic biases embedded in algorithmic models can also be exploited and used as a weapon at scale, causing catastrophic harm [211–214]. Organiza- tions that deploy AI models and systems without assessing and managing these risks can not only harm their users but jeopardize their reputations. Deployment Context of Use AI systems are designed and developed to be used in specific real world settings, but are 32/77 often tested in idealized scenarios. Once deployed, the original intent, idea, or impact assessment can drift as the application is repurposed or used in unforeseen ways, and in settings or contexts for which it was not originally intended. Different deployment con- texts means a new set of risks to be considered. Engaging with the broad set of stakeholder communities that may be impacted by the deployment of these technologies—before the decision is made to build the AI system—is an important consideration and strongly rec- ommended. For more on context of use and what it encompasses from a human-centered design perspective, see subsequent Section 3.3.2. One major purpose, and a significant benefit, of automated technology is that it can make sense of information more quickly and consistently than humans. AI systems are also often perceived as a way to make public interest decisions more fair, or to reduce (or eliminate) biased human decision making and bring about a more equitable society [27]. These perspectives have led to the deployment of au- tomated and predictive modeling tools within trusted institutions and high-stakes settings such as hiring or criminal justice. In such settings, automated decisions that incorporate negative biases can perpetuate harms more quickly, extensively, and systematically than human and societal biases on their own. Human-in-the-loop Most algorithmic decision systems are socio-technical systems. They are inextricably tied to human social behavior, from the datasets used by ML processes and the decisions made by those who build them, to the interactions with the humans who provide the insight and oversight to make such systems actionable. The default assumption is that placing a human “in-the-loop” of such systems can ensure that adverse events do not occur. Current perceptions about the role and responsibility of the human-in-the-loop with AI are often implicit, and expectations about level of performance for these systems are often based on untested or outdated hypotheses. The bulk of academic literature available in this domain often relates to humans working with automated systems that pre-date the broad scale use of ML. Some human-in-the-loop systems are deployed for use by subject matter experts. In this expert-driven scenario, professionals with expertise in a specific domain work in con- junction with an automated system towards a specific end goal—usually a consequential decision about another individual(s). Depending on the purpose of the system, the expert may interact with the ML model but is rarely part of the design or development of the system itself. These experts are not necessarily familiar with ML, data science, computer science, or other fields traditionally associated with AI design or development. For ex- ample, for AI systems that are deployed in the domain of medicine, the experts are the physicians and bring their expertise about medicine—not data science, data modeling and engineering, or other computational factors. 33/77 The perception that a human (expert or otherwise) can effectively and objectively oversee the use of algorithmic decision systems is a problematic assumption. More work needs to be done to understand the complex institutional and soci- etal structures where these systems are developed and placed. Humans carry their own significant cognitive biases and HEURISTICS into the operation of AI systems and exactly how they can assist remains an understudied area. One challenge with human-in-the-loop scenarios is finding a configuration that enables a system to be used in a way that optimally leverages, instead of replaces, the subject matter ex- pertise of the human. This is difficult since subject matter experts and AI develop- ers often lack a common vernacular, which can contribute to miscommunication and misunderstood expectations and capabilities on both sides of the human-AI system. Expert-system configurations are complex, even without the aid of a highly advanced AI. Experts and operators can often be placed into AI-based system settings without explicit declarations for governing authority over the specific task and outcome. With the promise of approaches that are more quantitative, subject matter experts may inadvertently activate the McNamara fallacy and leverage the AI system to take the pressure off of their often more subjective processes for the presumed objectivity of automation (this bias is often referred to as automation complacency). Expert users may also subconsciously find ways to leverage this perceived objectivity as cover, or even justification, for their implicit biases [215–217] and inadvertently make decisions that are inaccurate and harmful. Relatedly, AI developer communities may subconsciously presume that experts’ methods have been validated to a greater degree than is the case. These kinds of implicit individual and group actions may create conditions that indirectly encourage the use of technology that is not quite ready for use, especially in high-stakes settings [3, 78, 218]. Researchers recommend that AI development teams work in tighter conjunction with subject matter experts and practitioner end users, who in turn, must “consider a deliberate and modest approach” when utilizing automated output [219]. Expert-driven ML and human-in-the-loop practices are not intended to serve as a form of oversight on AI systems and accompanying results. Experts bring their particular sub- ject matter knowledge to the process, and are not necessarily trained to govern the use of an AI system they played no role in developing. But current legal and governance struc- tures actively rely on humans—either expert or otherwise—to serve as a mechanism for protecting society from faulty, mistaken, and/or dangerous algorithmic decisions. The fun- damental assumption of such structures is that a human overseer, simply by virtue of being human, will be able to provide adequate governance for systems.16 The reality however is 16This is most frequently emphasized in governance frameworks that associate human-in-the-loop decisions as posing less risk, as opposed to fully automated decision making. See, for exam- ple, the role of general human intervention in minimizing risks for AI systems in the FDA’s “Good Machine Learning Practice for Medical Device Development: Guiding Principles,” https://www.fda.gov/medical-devices/software-medical-device-samd/good-machine-learning-practice- 34/77 that without significant procedural and cultural support, optimistic expectations about how humans are able to serve in this administrative capacity are not borne out in practice. The literature provides a thorough review of the flaws of human oversight policies [112]. General public The challenge of interpretable systems is also a factor for consumer or citizen use of AI applications. It is presumed that trust can improve if the public is able to interrogate and engage with AI systems in a more transparent manner. In their ar- ticle on public trust in AI, Knowles and Richards state “...members of the public do not need to trust individual AIs at all; what they need instead is the sanction of authority provided by suitably expert auditors that AI can be trusted” [220]. De- veloping such an authority requires standard practices, metrics, and norms from a socio-technical perspective. The NIST AI Risk Management Framework will help create standard practices, metrics and norms in consensus with the AI community. Reliance on various downstream professionals to act as a governor on automated processes in complex societal sys- tems is not a viable approach. 3.3.2 Human Factors Guidance Impact assessments The decision to deploy AI technology is a function of organizational incentives. AI is designed and developed within a set of organizational norms and policies. One recent proposed approach for ensuring that technology is developed in an ethical and respon- sible manner is the algorithmic impact assessment. Identifying and addressing potential biases is an important step in the assessment process. There is currently momentum for AI researchers to include statements about potential societal impacts [221] when submitting their work to journals or conferences. Similar to privacy impact assessments, which are re- lied upon by data protection and privacy frameworks to gauge and respond to data privacy risks, such impact assessments provide a high-level structure that enables organizations to frame the risks of each algorithm or deployment while also accounting for the specifics of each use case. Engaging in impact assessment can also serve as a forcing mechanism for medical-device-development-guiding-principles; NHTSA’s “Automated Driving Systems 2.0 Voluntary Guidance,” https://www.nhtsa.gov/sites/nhtsa.gov/files/documents/13069a-ads2.0 090617 v9a tag.pdf. In the military context, even more emphasis has been placed on human intervention, such as in “AI Principles: Recommendations on the Ethical Use of Artificial Intelligence,” Department of Defense Defense Innovation Board, https://media.defense.gov/2019/Oct/31/2002204458/-1/- 1/0/DIB AI PRINCIPLES PRIMARY DOCUMENT.PDF; see also Brig. Gen. (ret.) Jean Michel Verney et al., “Human-On-the-Loop,” Joint Air & Space Power Conference 2021, https://www.japcc.org/human- on-the-loop/. 35/77 organizations to articulate any risks, and then to generate documentation of any mitigation activities in the event that any harms—and associated oversight—do arise17[222–226]. A misstep with impact assessments is to only apply them once at the beginning of a long and iterative process in which goals and outcomes can change over time. To overcome the challenge of the point-in-time nature of impact assessments, impact assessments must be applied at some reasonable cadence when used with iterative and evolving AI systems. Another concern with impact assessments is that the technology groups, or others who will be assessed, may have undue influence on building or using the assessment. Multi-stakeholder engagement The practice of technology development is also complicated by the role of power and de- cision making within the organizational structure [227]. A consistent theme from the lit- erature is the benefit of engaging a variety of stakeholders and maintaining diversity along social lines where bias is a concern (racial diversity, gender diversity, age diversity, di- versity of physical ability) [228, 229]. These kinds of practices can lead to broadening perspectives, and in turn, more thorough evaluation of the societal impacts of technology- based applications. Using the demographic traits of organizational personnel to identify problematic aspects within development culture and practice is not sufficient and may not be fair. Identifying downstream impacts may take time and require the involvement of end- users, practitioners, subject matter experts, and interdisciplinary professionals from the law and social science. Expertise matters, and these stakeholders can bring their varied expe- riences to bear on the core challenge of identifying harmful outcomes and context shifts within the specific setting the AI system will be deployed. Technology or datasets that seem non-problematic to one group may be deemed disas- trous by others. The manner in which different user groups can game certain applications may also not be so obvious to the teams charged with bringing an AI-based technology to market. These kinds of impacts can sometimes be identified in early testing stages, but are usually very specific to the contextual end-use and will change over time. Acquiring these types of resources for risk and associated impacts does not necessarily require a huge allocation, but it does require deliberate planning and guidance. This is also a place where innovation in approaching bias could improve practice. These factors are part of changing norms and creating an organizational risk culture where teams improve capacity for con- sidering the impact of the technology they design and develop, and communicating about these impacts more broadly. Diversity, Equity & Inclusion Without prioritizing diversity, equity, and inclusion in the teams involved in training and deploying AI systems it is difficult to move beyond a focus on system optimization or to address design considerations and risks beyond a narrow subset of users. Consider for example how character limits impact some languages and cultures more so than others; in 17H.R. 2231, 116th Cong. (2019), https://www.congress.gov/bill/116th-congress/house-bill/2231/text. 36/77 recognition of this effect, Twitter increased its character limit from 140 to 280 characters [230]. In another example, a recent exercise by the same social media company found that AI used to filter image content disfavored people with white hair and memes written in non-latin scripts [231, 232]. As recent research has shown that developers with similar demographic backgrounds make similar misjudgements, [72] ensuring that individuals involved in training, testing, and deploying the system have a diversity of experience, expertise and backgrounds is a critical risk mitigant that can help organizations manage the potential harms of AI. The human heuristics and biases that lead to examples such as these are implicit; as such, simply increasing awareness of bias does not ensure control over it. As previously described in Section 3.3, heuristics are adaptive mental shortcuts than can often be beneficial to reduce complexity in tasks of judgement and choice, yet also lead to cognitive biases. The concepts and reasoning behind diversity, equity, and inclusion in the workplace are closely tied to the need for broad multi-stakeholder engagement during all aspects of the AI lifecycle. Numerous studies have touted the benefits of increased diversity, equity, and inclusion in the workplace [233–236]. Yet, the AI field noticeably lacks diversity [237]. To extend the benefits of diversity, equity, and inclusion to both the users and develop- ers of AI systems, commentators and experts now recommend that bias mitigation efforts should be multifaceted, empowering a diverse group of individuals who reflect a range of backgrounds, perspectives and expertise, which in turn can help to broaden the views of AI system designers and engineers [238, 239]. In particular, diversity, equity and inclusion efforts can help organizations better understand: how the system is likely to impact a wide variety of users, how such users might interact with the system in practice, the potential harms and benefits of systems across users and groups, whether troubleshooting efforts— such as the recourse channels described below—are likely to be effective in practice, as well as how the system might impact broader populations beyond direct users of the sys- tem, among others. Practice Improvements By taking a lifecycle approach it is possible to identify junctures where well-developed guidance, assurance, and governance processes can assist business units and data and social scientists to collaboratively integrate processes to reduce bias without being cumbersome or blocking progress. Several technology companies are developing or utilizing guidance to improve organizational decision making and make the practice of AI development more responsible by implementing processes such as striving to identify potential bias impacts of algorithmic models. One approach is to enumerate institutional assumptions when de- veloping algorithmic decision systems and map these assumptions to the expectations of the groups impacted by the technology–which requires deliberate multi-stakeholder and community engagement. “Cultural effective challenge” is a practice that seeks to create an environment where technology developers can actively challenge and question steps in modeling and engineering to help root out statistical biases and the biases inherent in hu- man decision making [240]. Requiring AI practitioners to defend their techniques, within 37/77 a demographically and professionally diverse setting, can incentivize new ways of think- ing, stimulate improved practices, and help create change in approaches by individuals and organizations [227]. Human–AI configuration AI systems are often deliberately placed into high-risk settings to counteract the known subjectivity and bias of humans. Yet considerable questions remain about how to optimally configure humans and automation. An approach to human-in-the-loop that takes into con- sideration the broad set of socio-technical factors is necessary, especially in the context of AI bias. The list of relevant sub-topics span fields such as human factors, psychology, orga- nizational behavior, and human-AI interaction, and building bridges between these and the technology communities is still necessary. NIST seeks to develop formal guidance about how to implement human-in-the-loop processes that do not amplify or perpetuate the many human, systemic and computational biases that can degrade outcomes in this complex set- ting. Identifying system configurations and necessary qualifications for their components that result in outcomes that are accurate and trustworthy will be a key focus. System and procedural transparency A consistent finding in the literature is that AI systems need to be more explainable and in- terpretable. The proliferation of tools such as datasheets and model cards are intended to fill that gap [241, 242]. Bias intersects with transparency in complex ways. Groups who invent and produce technology have specific intentions for its use and are unlikely to be aware of all the ways a given application will be used and repurposed once deployed. Transparency tools are especially helpful for addressing the problem of unintended use, but even when AI systems are used as intended there are significant individual differences in how humans in- terpret AI model output. This issue becomes particularly relevant when deploying systems for use by subject matter experts, who are less interested in how a system works and more concerned with why a system provided a given output. When system designers do not take these perceptual differences into consideration it can lead to misinterpretation of output, which is especially problematic in high-risk settings [243, 244]. Coordinated guidance is necessary to ensure that transparency tools are effectively supporting the professionals who use them and not indirectly contributing to processes that could amplify bias. There are techniques to flag factors in datasets and modeling processes that can produce biased outcomes or cause noncompliance with legal requirements. The intent here is that flagging information for somebody along the AI lifecycle or the end user will serve as a system check. Yet, flagging such information for downstream users does not always result in a directly positive outcome, and can in fact create the opposite[181, 245]. Developing guidance in this area will require more information about the settings under which human biases may amplify harmful outcomes, and where humans can work optimally with and complement an AI-based system. These questions, like those related to AI system design, are notably dependent on setting (e.g., aircraft, cyber-physical systems, public safety and forensics, manufacturing), operator (e.g., expert, trained, naive), and task (e.g., recognition, 38/77 event detection, forecasting, reasoning). Keeping humans at the center of AI design Human-centered design (HCD) is an approach to the design and development of a system or technology that aims to improve the ability of users to effectively and efficiently use a product. HCD seeks to improve the user experience of an entire system, involving all aspects of a technology, from hardware design to software design. HCD is a methodology that has been successfully applied to a myriad of important domains, and NIST itself has authored several HCD handbooks tailored for particular domains, e.g.:, biometrics and public safety [246, 247]. HCD is an ongoing, iterative process in which project teams design, test, and contin- ually refine a system, placing users at the core of the process. Humans and their needs drive the process, rather than having a techno-centric focus. HCD works as part of other development lifecycles, including waterfall, spiral and agile models. User-centered design, HCD, participatory design, co-design, and value-sensitive design all have key similarities; at the highest level, they seek to provide humans with designs that are ultimately beneficial to their lives. Furthermore, by placing humans at the center of such approaches, they nat- urally lend themselves to a deeper focus on larger societal considerations such as fairness, bias, values, and ethics. HCD works to create more usable products that meet the needs of its users. This, in turn, reduces the risk that the resulting system will under-deliver, pose risks to users, result in user harms, or fail. The HCD process is illustrated in Fig. 6 below. 39/77 Designed soluons meets  user requirements  Evaluate the design   against requirements Understand and specify the  context of use Plan the human‐centered  design process Specify the user  requirements Evaluate the design   against requirements Iterate  where  appropriate Fig. 6. Human-centered Design Process [ISO 9241-210:2019] As defined in International Organization for Standardization (ISO) standard 9241-210:2019 [248], HCD involves: • an explicit understanding of users, tasks and environments–the context of use; • the involvement of users throughout design and development; • a design driven and refined by human-centered evaluation; • an iterative process whereby a prototype is designed, tested and modified; • addressing the whole user experience; • a design team including multidisciplinary skills and perspectives. Based on the ISO standard, a HCD methodology for the development of AI systems could iteratively comprise the following, as shown in Fig. 7: • Defining the Context of Use, including operational environment, user characteristics, tasks, and social environment; 40/77 • Determining the User & Organizational Requirements, including business require- ments, user requirements, and technical requirements; • Developing the Design Solution, including the system design, user interface, and training materials; and • Conducting the Evaluation, including usability and conformance testing. Although all components of HCD are critical, the context of use has key socio-technical considerations for AI systems. The socio-technical dynamics and conditions under which an AI system is used must be considered at the front end of any project to ensure that the design of the system will meet the needs of users, the objectives of the organization, and larger societal needs once the system is implemented in a real-world environment. Evaluaon Context of  Use Design  Soluon User &  Organizaonal Requirements  USERS Fig. 7. Human-centered Design Process for AI Systems A deep understanding of contextual fac- tors is important throughout the AI life- cycle. Context of use does not simply involve the users’ context of use, it in- volves a much broader view of context: the organizational environment in which the AI system is being developed (in- cluding existing systems and products); the operational environment in which the system will be used; and the larger so- cietal environment in which the system will be implemented. For example, some intended users of AI systems may not have consistent or reliable access to fun- damental internet technologies (a phe- nomenon widely described as the “digital divide” [249, 250]), leading to biases in how different communities access a sys- tem. Similarly, those with disabilities may experience difficulties interacting with AI sys- tems. Crucially, such difficulties often cannot be mitigated by mathematical or software de-biasing approaches, and failure to address these important design issues may pose legal risks, for example in employment related activities affecting persons with disabilities.18 18Congress has recognized that objects, systems, and processes often are not designed with individuals with disabilities in mind. By ensuring that these protections apply at the individual rather than group level, Congress further recognized that the means of placing an individual with disabilities on equal footing with others may require an individualized solution—one person with a disability may require a reasonable accommodation, and a different individual with a disability may require a different accommodation or no accommodation at all. Some disabilities are so heterogeneous that even two individuals with the same disability may need different accommodations. In the employment context, an algorithm may screen out a particular individual, and therefore may violate the Americans with Disabilities Act, regardless of whether broadly defined groups of individuals with disabilities tend to be assessed highly by a given algorithm. 41/77 A growing number of researchers have pointed out the benefits of socio-technical ap- proaches. For example, Ferrer et al [251] note: “This challenge could be addressed through a socio-technical approach which can consider both the technical dimensions and the com- plex social contexts in which these systems are deployed. Building public confidence and greater democratic participation in AI systems requires ongoing development of not just explainable AI but of better Human-AI interaction methods and socio-technical platforms, tools and public engagement to increase critical public understanding and agency.” Research to integrate HCD with the standard design, development, evaluation, and de- ployment processes of today’s AI systems is relatively recent. In their chapter on HCD of AI in the Handbook of Human Factors and Ergonomics, Margetis et al state that “A core concept of HCD is that of actively involving end-users and appropriate stakeholders in the process. In the context of AI, this means placing humans in the loop, not only through meaningful human control [252], but also through their active participation in the prepa- ration, learning, and decision-making phases of AI [253].” Human-centered AI (HCAI) is an emerging area of scholarship that reconceptualizes HCD in the context of AI, providing human-centered AI design metaphors and suggested governance structures to develop reli- able, safe, and trustworthy AI systems [254]. Schneiderman envisages HCAI as ”bridg[ing] the gap between ethics and practice with specific recommendations for making successful technologies that augment, amplify, empower, and enhance humans rather than replace them. This shift in thinking could lead to a safer, more understandable, and more manage- able future. An HCAI approach will reduce the prospects for out-of-control technologies, calm fears of robot-driven unemployment, and diminish the threats to privacy and security. A human-centered future will also support human values, respect human dignity, and raise appreciation for the human capacities that bring creative discoveries and inventions.” 3.4 How do we manage and provide oversight? Governance and AI Bias Governance processes impact nearly every aspect of managing AI bias. For that reason, it is essential to view governance as a holistic implementation tier, socio-technical in nature, and informing each phase of the bias management process. It is also important to note that governance does not simply focus on technical artifacts, such as AI systems alone, but also on organizational processes and cultural competencies that directly impact the individuals involved in training, deploying and monitoring such systems. While there are a number of components to effective governance for managing bias in AI systems, we focus here on organizational measures and culture. 3.4.1 Governance Guidance Monitoring AI systems may perform differently than expected once deployed, which can lead to dif- ferential treatment of individuals from different groups. A key measure to control this risk is to deploy additional systems that monitor for potential bias issues, which can alert the proper personnel when potential problems are detected. Without such monitoring in place, 42/77 it can be difficult to know if deployed system performance in the real world matches up to the measurements conducted in a laboratory environment, or whether newly collected data match the distribution of the training data. A key consideration for the success of live monitoring for bias is the collection of data from the active user population, especially data related to user demographics such as age and gender, to enable calculation of assessment measures. These type of data can have a variety of privacy implications and may be subject to legal restrictions on what types of data can be collected and under what conditions. Recourse Channels Availability of feedback channels allow system end users to flag incorrect or potentially harmful results, and seek recourse for errors or harms. A number of legal frameworks pri- oritize the ability of users to appeal and override unfavorable decisions, and are applied in a subset of algorithmic systems deployed in areas like consumer finance. Because appeal and override recourse often requires a logical description of the questionable ML deci- sion, these processes are tightly connected to AI system explainability and interpretability. Though not without criticism [255], adverse action notices for negative consumer credit decisions, as mandated by the Equal Credit Opportunity Act and the Fair Credit Reporting Act, are an example of an explanation and appeal process19[256]. Additional appeal and override processes could include options for customers to interact with a human instead of an AI system or options to avoid similar AI-generated content in the future. Embedding such processes and technologies into AI systems allows users to appeal wrong decisions (or even suggestions) while also empowering technology development teams to remediate potential incidents at or near their inception point. Policies and Procedures In the context of AI systems, ensuring that written policies and procedures address key roles, responsibilities, and processes at all stages of the AI model lifecycle is critical to managing and detecting potential overall issues of AI system performance.20 Policies and procedures can enable consistent development and testing practices, which in turn can help to ensure that results from AI systems are repeatable and that related risks are consistently mapped, measured and managed. Without such policies, the management of AI bias can easily become subjective and inconsistent across organizations, which can exacerbate risks over time rather than minimize them—if, for example, irreconcilably different metrics are used across systems. Policies may: • define the key terms and concepts related to AI systems and the scope of their in- tended impact; • address the use of sensitive or otherwise potentially risky data; 19See 15 U.S.C., § 1691(d). 20Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011). 43/77 • detail standards for experimental design, data quality, and model training; • outline how the risks of bias should be mapped and measured, and according to what standards; • detail processes for model testing and validation; • detail the process of review by legal or risk functions; • set forth the periodicity and depth of ongoing auditing and review; • outline requirements for change management; and • detail any plans related to incident response for such systems, in the event that any significant risks do materialize during deployment. Documentation Clear documentation practices can help to systematically implement policies and proce- dures, standardizing how an organization’s bias management processes are implemented and recorded at each stage. Standardized documentation can, in turn, help to ensure ac- countability, as described in further detail below. Model documents should contain in- terpretable descriptions of system mechanisms, enabling oversight personnel to make in- formed, risk-based decisions about the system’s potential to perpetuate bias. Documen- tation also serves as a single repository for important information, supporting not only internal oversight of AI systems and related business processes, but also enhancing system maintenance, and serving as a valuable resource for any necessary corrective or debugging activities.21 Model documentation is especially important in the context of accountability. The use of documentation templates with specific requirements enables practitioners to walk through workflows as they are prescribed in written policies and procedures, or by other best practices. Omission of key documentation elements can indicate a lack of adherence to written policies and procedures on the part of system developers or testers. Some model documentation templates also include contact information for developers and stakeholders [241, 242]. The act of adding contact information to a document describing a work product can enable more efficient oversight and communications. This type of practice should also lead to greater concern and responsibility for the quality of the product, which in turn, can impact bias management efforts within an organization. 21Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021), https://ww w.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management /index-model-risk-management.html. 44/77 Accountability Accountability plays a critical role in governance efforts [257]. Governance without ac- countability is, in practice, unlikely to be effective. Ensuring that a specific team, and often, a specific individual – such as a Chief Model Risk Officer, as is now common in large consumer finance organizations – is responsible for bias management in AI systems is a fundamental accountability mechanism.22 Ensuring individuals or teams bear respon- sibility for risks and associated harms provides a direct incentive for their mitigation. Put simply, when someone’s boss is accountable for bias issues, they too are accountable for bias issues—and this phenomenon promulgates down to front-line practitioners. Account- ability for AI bias cannot lie on the shoulders of a single individual, which is why account- ability mandates should also be embedded within and across the various teams involved in the training and deployment of AI systems. Existing technical and procedural frameworks for accountability related to AI include general governance procedures, and application of system monitoring, data quality measures, computer security countermeasures, and nondis- crimination mechanisms, among others [258, 259]. Fundamentally, accountability requires a clear assessment of the role of the AI system itself. For example, decision-support systems, which may be claimed not to result in direct decision-making and therefore pose less risks, can easily become overly relied upon by users, or misused or abused. In these cases, the AI system would generate similar harms as if it were engaging in decision-making directly. Model or algorithmic audits [260] can be used to assess and document such crucial accountability considerations. There are several notions of audits commonly discussed in the responsible and trustworthy AI communities. Audit may refer to a traditional internal audit function employed to track issues of model risk, as in traditional model governance. Audit may refer to a structured and principled application of lessons learned in financial audit practices to AI systems [261]. Alternatively, audit may refer to some general documentation and transparency approach. Audits can be an effective accountability, bias, and general risk mitigation mechanism. Indeed, laws are being passed that demand bias audits of AI-based systems used in employment [262]. However, audits currently exist in a wide range of forms with varying levels of quality and consensus [263]. Audits will be addressed in future NIST documents related to the AI risk management framework. Culture & Practice For AI governance to be effective, it needs to be embedded throughout the culture of an organization. While organizational culture and practice can be defined in a variety of ways, the central theme of most such definitions emphasize beliefs, norms and values - or, in other words, the behavior an organization prioritizes in practice, even if such behavior is not codified or written down [264]. Risk management culture and practices can be a powerful technique for identifying biases across the AI lifecycle and from a socio-technical system perspective. 22Bd. Governors Fed. Rsrv. Sys., supra note 20. 45/77 Effective challenge The principal of effective challenge is a central component of model risk management frameworks. This practice is heavily relied on by the financial sector to mitigate algorithmic risk, and mandates that important model design and im- plementation decisions be questioned by experts with the authority and stature to make changes in design and implementation.23 Fostering a culture of effective challenge encour- ages actively challenging and questioning steps in the development of AI systems, and can help to raise issues of AI bias before they materialize in deployed systems. An organiza- tional culture that encourages serious questioning of AI system designs will be more likely to identify problems before they turn into harmful incidents. Relatedly, while individuals who are part of the development of AI systems may be knowledgeable about the potential harmful impacts of the technology they build, impact assessments should not be exclu- sively developed by these teams due to increased likelihood of confirmation bias and other incentives that may cause conflicts of interest. Three lines of defense Because culture can be difficult to map or measure directly, one way to encourage this approach is to incentivize critical thinking and review at an or- ganizational and procedural level. Model risk management frameworks, for example, are often systematically implemented through the so-called “three lines of defense,” which creates separate teams that are held accountable for different aspects of the model lifecy- cle. Typically, the first line of defense focuses on model development, the second on risk management, and the third on auditing.24 While a traditional three-lines approach may be impractical for smaller organizations, ensuring that a culture of effective challenge is encouraged and sustained can help organizations to anticipate, and therefore to effectively mitigate, risks of bias before they materialize. Risk Mitigation, Risk Tiering & Incentive Structures Some applications of AI are high-risk.25 A central cultural component of effective risk management for AI bias lies in a clear acknowledgment that risk mitigation, rather than risk avoidance, is often the most effective factor in managing such risks.26 Developing a risk mitigation mindset, meaning a clear acceptance that incidents can and will occur, and emphasizing practical detection and mitigation once they do, can help ensure that any risks of bias are quickly mitigated in practice. This acknowledgement enables a clear triag- ing of risks which can enable organizations to focus finite resources on the risks of bias that are most material, and therefore most likely to cause real-world harm. An additional component of effective organizational culture includes aligning pay and promotion incen- tives across teams to AI risk mitigation efforts, such that participants in the risk mitigation 23Id. 24Off. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for Deposit-Taking Institutions, E-23 (Sept. 2017). 25Eur. Comm’n, Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts (pro- posed Apr. 21, 2021), https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A52021PC0206. 26Bd. Governors Fed. Rsrv. Sys., supra note 20 46/77 mechanisms—like the three lines of defense—are truly motivated to use sound develop- ment approaches, test rigorously and audit thoroughly.27 Information Sharing As described in a NIST special publication [265], sharing cyber threat information helps organizations improve both their own security postures, and those of other organizations. Identifying internal mechanisms for teams to share information about bias incidents or other harmful impacts from AI helps to elevate the importance of AI risks and provides information for teams to avoid past failed designs. Some initial efforts are already under- way [266]. As teams begin to create norms for tracking such incidents, it can potentially transform AI practices and the organizational culture. Improving awareness of how bias presents in deployed AI and its related impacts can enhance knowledge and capabilities, and prevent incidents. Fostering a culture of information sharing can also serve as a new area for community engagement. 4. Conclusions This document has provided a broad overview of the complex challenge of addressing and managing risks associated with AI bias. It is clear that developing detailed technical guid- ance to address this challenging area will take time and input from diverse stakeholders, within and beyond those groups who design, develop, and deploy AI applications, and in- cluding members of communities that may be impacted by the deployment of AI systems. Since AI is neither built nor deployed in a vacuum, we approach AI as a socio-technical system, acknowledging that AI systems and associated bias extend beyond the computa- tional level. Bias can be introduced purposefully or inadvertently, or it can emerge as the AI system is used, impacting society at large through perpetuating and amplifying biased and discriminatory outcomes. Adopting a socio-technical perspective brings new requirements, many of which are contextual in nature, to the processes that comprise the AI lifecycle. It is important to gain understanding in how computational and statistical factors interact with systemic and human biases. NIST has provided an initial socio-technical framing for AI bias in this document, in- cluding key context and terminology, highlights of the main challenges, and foundational directions for future guidance. This information is classified and discussed through the document according to three key areas: 1. dataset availability, representativeness, and suitability in socio-technical contexts; 2. TEVV considerations for measurement and metrics to support testing and evaluation; 3. human factors, including societal and historic biases within individuals and organiza- tions, participatory approaches such as human-centered design, and human–in–the– loop practices. 27Id. 47/77 Identifying the key requirements for improving our knowledge in this area is a neces- sary first step. To ensure broad input, engagement, and consensus, NIST will carry out supporting standards development activities such as workshops and public comment peri- ods for draft documents. NIST intends to develop further consensus socio-technical guidance in collaboration with the research community and a broad set of other stakeholders, including those who are directly impacted by AI bias. The intent is for this guidance to be of specific assistance for organizations who commission, design, develop, deploy, use, or evaluate AI for a variety of use cases. By providing these entities with clear, explicit, and technically valid guidance NIST intends to improve the state of practice for AI bias and assure system trustworthiness. 48/77 5. Glossary activity bias A type of selection bias that occurs when systems/platforms get their training data from their most active users, rather than those less active (or inactive) [131]. 8 aleatoric uncertainty Aleatoric uncertainty, also known as statistical uncertainty, refers to unknowns that differ each time we run the same experiment. It refers to the vari- ability in the outcome of an experiment which is due to inherently random effects. For example, in machine learning context, the data-generating process may have a stochastic component that cannot be reduced by any additional source of information. Consequently, even the best model trained on this data will not be able to provide a definite answer. 9, 20, 21 amplification bias Arises when the distribution over prediction outputs is skewed in com- parison to the prior distribution of the prediction target [267]. 8 anchoring bias A cognitive bias, the influence of a particular reference point or anchor on people’s decisions. Often more fully referred to as anchoring-and-adjustment, or anchoring-and-adjusting: after an anchor is set, people adjust insufficiently from that anchor point to arrive at a final answer. Decision makers are biased towards an initially presented value [79]. 8, 9 annotator reporting bias When users rely on automation as a heuristic replacement for their own information seeking and processing [268]. A form of individual bias but often discussed as a group bias, or the larger effects on natural language processing models. 8 automation complacency When humans over-rely on automated systems or have their skills attenuated by such over-reliance (e.g., spelling and autocorrect or spellcheck- ers). 8 availability heuristic Also referred to as availability bias. A mental shortcut whereby people tend to overweight what comes easily or quickly to mind, meaning that what is easier to recall—e.g., more “available”—receives greater emphasis in judgement and decision-making. 8 behavioral bias Systematic distortions in user behavior across platforms or contexts, or across users represented in different datasets [144, 269]. 8 cognitive bias A broad term referring generally to a systematic pattern of deviation from rational judgement and decision-making. A large variety of cognitive biases have been identified over many decades of research in judgement and decision-making, some of which are adaptive mental shortcuts known as heuristics. 8 49/77 concept drift Use of a system outside the planned domain of application, and a common cause of performance gaps between laboratory settings and the real world. 8 confirmation bias also called confirmatory bias, a cognitive bias where people tend to prefer information that aligns with, or confirms, their existing beliefs. People can ex- hibit confirmation bias in the search for, interpretation of, and recall of information. In the famous Wason selection task experiments, participants repeatedly showed a preference for confirmation over falsification. They were tasked with identifying an underlying rule that applied to number triples they were shown, and they overwhelm- ingly tested triples that confirmed rather than falsified their hypothesized rule [270]. 8, 9, 27 construct validity A form of validation that seeks to answer whether a test measures what it intends to measure. [271]. 15 consumer bias Arises when an algorithm or platform provides users with a new venue within which to express their biases, and may occur from either side, or party, in a digital interaction [272]. 8 content production bias Arises from structural, lexical, semantic, and syntactic differ- ences in the contents generated by users [144]. 8 data dredging A statistical bias in which testing huge numbers of hypotheses of a dataset may appear to yield statistical significance even when the results are statistically nonsignificant. 8, 27 data generation bias Arises from the addition of synthetic or redundant data samples to a dataset [273]. 8 deployment bias Arises when systems are used as decision aids for humans, since the human intermediary may act on predictions in ways that are typically not modeled in the system [90]. However, it is still individuals using the deployed system. 8, 26 detection bias Systematic differences between groups in how outcomes are determined and may cause an over- or underestimation of the size of the effect [274]. 8 Dunning–Kruger effect A cognitive bias, the tendency of people with low ability in a given area or task to overestimate their self-assessed ability. Typically measured by comparing self-assessment with objective performance, often called subjective ability and objective ability, respectively [275]. 8, 26 ecological fallacy Occurs when an inference is made about an individual based on their membership within a group. 8, 23 emergent bias Use of a system outside the planned domain of application, and a common cause of performance gaps between laboratory settings and the real world. 8 50/77 epistemic uncertainty An epistemic uncertainty, also known as systematic uncertainty, refers to deficiencies by a lack of knowledge or information. This may be because the methodology on which a model is built neglects certain effects or because particular data have been deliberately hidden. 9, 20–22 error propagation Arises when applications that are built with machine learning are used to generate inputs for other machine learning algorithms. If the output is biased in any way, this bias may be inherited by systems using the output as input to learn other models [82]. 8 evaluation bias Arises when the testing or external benchmark populations do not equally represent the various parts of the user population or from the use of performance metrics that are not appropriate for the way in which the model will be used [90]. 8 exclusion bias When specific groups of user populations are excluded from testing and subsequent analyses [276]. 8 feedback loop bias Effects that may occur when an algorithm learns from user behavior and feeds that behavior back into the model [272]. 8 funding bias Arises when biased results are reported in order to support or satisfy the funding agency or financial supporter of the research study [85], but it can also be the individual researcher. 8 governance a framework of policies, rules, and processes for ensuring direction, manage- ment and accountability. ii groupthink A psychological phenomenon that occurs when people in a group tend to make non-optimal decisions based on their desire to conform to the group, or fear of dissenting with the group. In groupthink, individuals often refrain from expressing their personal disagreement with the group, hesitating to voice opinions that do not align with the group. 8 heuristics in the context of human decision making, often referred to as “mental short- cuts,” a term that encompasses many methods that may be less than fully rational or optimal, yet are often sufficient for an approximate solution. Although heuristics can reduce cognitive load and aid people when making decisions, such heuristics also result in systematic errors and cognitive biases [79]. 34 historical bias referring to the long-standing biases encoded in society over time. Related to, but distinct from, biases in historical description, or the interpretation, analysis, and explanation of history. A common example of historical bias is the tendency to view the larger world from a Western or European view. 8 51/77 human reporting bias When users rely on automation as a heuristic replacement for their own information seeking and processing [268]. 8 implicit bias An unconscious belief, attitude, feeling, association, or stereotype that can affect the way in which humans process information, make decisions, and take ac- tions. 8 inherited bias Arises when applications that are built with machine learning are used to generate inputs for other machine learning algorithms. If the output is biased in any way, this bias may be inherited by systems using the output as input to learn other models [82]. 8 institutional bias In contrast to biases exhibited at the level of individual persons, insti- tutional bias refers to a tendency exhibited at the level of entire institutions, where practices or norms result in the favoring or disadvantaging of certain social groups. Common examples include institutional racism and institutional sexism [91]. 8 interpretation bias A form of information processing bias that can occur when users in- terpret algorithmic outputs according to their internalized biases and views [272]. 8 language model A computational model that has been trained using statistical methods to find patterns in written and/or spoken language, in order to predict or classify words, text, or speech. 21 linking bias Arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users [144]. 8 loss of situational awareness bias When automation leads to humans being unaware of their situation such that, when control of a system is given back to them in a situation where humans and machines cooperate, they are unprepared to assume their duties. This can be a loss of awareness over what automation is and isn’t taking care of. 8 McNamara fallacy The belief that quantitative information is more valuable than other information. 12 measurement bias Arises when features and labels are proxies for desired quantities, po- tentially leaving out important factors or introducing group or input-dependent noise that leads to differential performance [90]. 8 mode confusion bias When modal interfaces confuse human operators, who misunder- stand which mode the system is using, taking actions which are correct for a differ- ent mode but incorrect for their current situation. This is the cause of many deadly accidents, but also a source of confusion in everyday life. 8 52/77 model A conceptual, mathematical, or physical representation of phenomenon observed in a system of ideas, events, or processes. In computationally-based models used in AI, phenomenon are often abstracted for mathematical representation, which means that characteristics that can not be represented mathematically may not be captured in the model. i, v model selection bias The bias introduced while using the data to select a single seemingly “best” model from a large set of models employing many predictor variables. Model selection bias also occurs when an explanatory variable has a weak relationship with the response variable [277]. 8 popularity bias A form of selection bias that occurs when items that are more popular are more exposed and less popular items are under-represented [130]. 8 population bias Systematic distortions in demographics or other user characteristics be- tween a population of users represented in a dataset or on a platform and some target population [144]. 8 presentation bias Biases arising from how information is presented on the Web, via a user interface, due to rating or ranking of output, or through users’ own self-selected, biased interaction [131]. 8 proxy A variable that can stand in for another, usually not directly observable or measur- able, variable. 20 ranking bias A form of anchoring bias. The idea that top-ranked results are the most relevant and important and will result in more clicks than other results [131, 278]. 8 Rashomon effect or principle Refers to differences in perspective, memory and recall, interpretation, and reporting on the same event from multiple persons or witnesses. 8 representation bias Arises due to non-random sampling of subgroups, causing trends es- timated for one population to not be generalizable to data collected from a new pop- ulation [85]. 8 selective adherence Decision-makers’ inclination to selectively adopt algorithmic advice when it matches their pre-existing beliefs and stereotypes [215]. 8 Simpson’s Paradox A statistical phenomenon where the marginal association between two categorical variables is qualitatively different from the partial association be- tween the same two variables after controlling for one or more other variables. For example, the statistical association or correlation that has been detected between two variables for an entire population disappears or reverses when the population is di- vided into subgroups. 8, 17 53/77 societal bias often referred to as social bias. Can be positive or negative, and take a num- ber of different forms, but is typically characterized as being for or against groups or individuals based on social identities, demographic factors, or immutable physical characteristics. Societal or social biases are often stereotypes. Common examples of societal or social biases are based on concepts like race, ethnicity, gender, sexual orientation, socioeconomic status, education, and more. Societal bias is often recog- nized and discussed in the context of NLP (Natural Language Processing) models. 8 socio-technical A term used to describe how humans interact with technology within the broader societal context. ii streetlight effect A bias whereby people tend to search only where it is easiest to look [279]. 8 sunk cost fallacy A human tendency where people opt to continue with an endeavor or behavior due to previously spent or invested resources, such as money, time, and effort, regardless of whether costs outweigh benefits. For example, in AI, the sunk cost fallacy could lead development teams and organizations to feel that because they have already invested so much time and money into a particular AI application, they must pursue it to market rather than deciding to end the effort, even in the face of significant technical debt and/or ethical debt. 8 survivorship bias tendency for people to focus on the items, observations, or people that “survive” or make it past a selection process, while overlooking those that did not. 8 technochauvinism The belief that technology is always the solution [35]. 12 temporal bias Bias that arises from differences in populations and behaviors over time [144, 280]. 8 uncertainty bias Arises when predictive algorithms favor groups that are better repre- sented in the training data, since there will be less uncertainty associated with those predictions [281]. 8 user interaction bias Arises when a user imposes their own self-selected biases and be- havior during interaction with data, output, results, etc [131]. 8 54/77 References [1] NIST, “U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical Standards and Related Tools,” National Institute of Standards and Technology, Tech. Rep., 2019. [Online]. Available: https://www.nist.gov/system/fil es/documents/2019/08/10/ai standards fedengagement plan 9aug2019.pdf [2] I. Ajunwa, S. A. Friedler, C. Scheidegger, and S. Venkatasubramanian, “Hiring by Algorithm: Predicting and Preventing Disparate Impact,” undefined, 2016. [Online]. Available: /paper/Hiring-by-Algorithm%3A-Predicting-and-Preventing-Ajunwa-F riedler/bd31ad5e998629998f35db9a10d858b36e603248 [3] S. Barocas, A. Biega, B. Fish, J. Niklas, and L. Stark, “When not to design, build, or deploy,” in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, ser. FAT* ’20. New York, NY, USA: Association for Computing Machinery, Jan. 2020, p. 695. [Online]. Available: https://doi.org/10.1145/3351095.3375691 [4] M. Bogen, “All the Ways Hiring Algorithms Can Introduce Bias,” Harvard Business Review, May 2019, section: Hiring. [Online]. Available: https: //hbr.org/2019/05/all-the-ways-hiring-algorithms-can-introduce-bias [5] J. Dastin, “Amazon scraps secret AI recruiting tool that showed bias against women,” Reuters, Oct. 2018. [Online]. Available: https://www.reuters.com/article/ us-amazon-com-jobs-automation-insight-idUSKCN1MK08G [6] E. Harlen and O. Schnuck, “Objective or Biased,” 2021. [Online]. Available: https://web.br.de/interaktiv/ki-bewerbung/en [7] J. Sanchez-Monedero, L. Dencik, and L. Edwards, “What does it mean to solve the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems,” arXiv:1910.06144 [cs], Jan. 2020, arXiv: 1910.06144. [Online]. Available: http://arxiv.org/abs/1910.06144 [8] M. Evans and A. W. Mathews, “New York Regulator Probes UnitedHealth Algorithm for Racial Bias,” Wall Street Journal, Oct. 2019. [Online]. Available: https://www.wsj.com/articles/new-york-regulator-probes-unitedhealth-algorithm- for-racial-bias-11572087601 [9] H. Fry, Hello world: being human in the age of algorithms. WW Norton & Com- pany, 2018. [10] M. A. Gianfrancesco, S. Tamang, J. Yazdany, and G. Schmajuk, “Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data,” JAMA Intern Med, vol. 178, no. 11, p. 1544, Nov. 2018. [Online]. Available: http: //archinte.jamanetwork.com/article.aspx?doi=10.1001/jamainternmed.2018.3763 [11] E. Guo and K. Hao, “This is the Stanford vaccine algorithm that left out frontline doctors,” 2020. [Online]. Available: https://www.technologyreview.com/2020/12/2 1/1015303/stanford-vaccine-algorithm/ [12] H. Ledford, “Millions of black people affected by racial bias in health- care algorithms,” Nature, vol. 574, no. 7780, pp. 608–609, Oct. 2019, 55/77 number: 7780 Publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/d41586-019-03228-6 [13] T. M. Maddox, J. S. Rumsfeld, and P. R. O. Payne, “Questions for Artificial Intelligence in Health Care,” JAMA, vol. 321, no. 1, p. 31, Jan. 2019. [Online]. Available: http://jama.jamanetwork.com/article.aspx?doi=10.1001/jama.2018.189 32 [14] Z. Obermeyer, B. Powers, C. Vogeli, and S. Mullainathan, “Dissecting racial bias in an algorithm used to manage the health of populations,” Science, vol. 366, no. 6464, pp. 447–453, Oct. 2019. [Online]. Available: https://www.sciencemag.org/lookup/doi/10.1126/science.aax2342 [15] T. Simonite, “How an Algorithm Blocked Kidney Transplants to Black Patients  WIRED,” Wired, 2020. [Online]. Available: https://www.wired.com/story/how-algo rithm-blocked-kidney-transplants-black-patients/ [16] M. Singh and K. N. Ramamurthy, “Understanding racial bias in health using the Medical Expenditure Panel Survey data,” arXiv:1911.01509 [cs, stat], Nov. 2019, arXiv: 1911.01509. [Online]. Available: http://arxiv.org/abs/1911.01509 [17] T. M. Cruz, “Perils of data-driven equity: Safety-net care and big data’s elusive grasp on health inequality,” Big Data & Society, vol. 7, no. 1, p. 205395172092809, Jan. 2020. [Online]. Available: http://journals.sagepub.com/doi/10.1177/2053951 720928097 [18] J. Angwin, J. Larson, S. Mattu, L. Kirchner, and ProPublica, “Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks.” ProPublica, 2016. [19] L. Dormehl, “Algorithms Are Great and All, But They Can Also Ruin Lives,” Wired, 2014. [Online]. Available: https://www.wired.com/2014/11/algorithms-great -can-also-ruin-lives/ [20] S. Goel, R. Shroff, J. L. Skeem, and C. Slobogin, “The Accuracy, Equity, and Jurisprudence of Criminal Risk Assessment,” SSRN Journal, 2018. [Online]. Available: https://www.ssrn.com/abstract=3306723 [21] S. Brayne, “Enter the Dragnet,” 2020. [Online]. Available: https://logicmag.io/co mmons/enter-the-dragnet/ [22] A. Chouldechova, “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments,” Big Data, vol. 5, no. 2, pp. 153–163, Jun. 2017. [Online]. Available: http://www.liebertpub.com/doi/10.1089/big.2016.0047 [23] EPIC, “Algorithms in the Criminal Justice System Risk Assessment Tools,” Electronic Privacy Information Center (EPIC), Tech. Rep., 2020. [Online]. Available: https://epic.org/algorithmic-transparency/crim-justice/ [24] K. Hill, “Flawed Facial Recognition Leads To Arrest and Jail for New Jersey Man,” 2020. [Online]. Available: https://www.nytimes.com/2020/12/29/technology/facial -recognition-misidentify-jail.html [25] J. E. Johndrow and K. Lum, “An algorithm for removing sensitive information: Application to race-independent recidivism prediction,” Ann. Appl. Stat., vol. 13, 56/77 no. 1, pp. 189–220, Mar. 2019. [Online]. Available: https://projecteuclid.org/euclid .aoas/1554861646 [26] F. Kamiran, A. Karim, S. Verwer, and H. Goudriaan, “Classifying Socially Sensitive Data Without Discrimination: An Analysis of a Crime Suspect Dataset,” in 2012 IEEE 12th International Conference on Data Mining Workshops. Brussels, Belgium: IEEE, Dec. 2012, pp. 370–377. [Online]. Available: http://ieeexplore.ieee.org/document/6406464/ [27] J. Kleinberg, H. Lakkaraju, J. Leskovec, J. Ludwig, and S. Mullainathan, “Human decisions and machine predictions,” The quarterly journal of economics, vol. 133, no. 1, pp. 237–293, 2018. [28] A. Liptak, “Sent to Prison by a Software Program’s Secret Algorithms,” The New York Times, May 2017. [Online]. Available: https://www.nytimes.com/2017/05/01 /us/politics/sent-to-prison-by-a-software-programs-secret-algorithms.html [29] R. Wexler, “When a Computer Program Keeps You in Jail,” The New York Times, p. 2, 2017. [Online]. Available: https://www.nytimes.com/2017/06/13/opinion/how -computers-are-harming-criminal-justice.html [30] “State v. Loomis,” 2016. [31] M. Aitken, E. Toreini, P. Carmichael, K. Coopamootoo, K. Elliott, and A. van Moorsel, “Establishing a social licence for Financial Technology: Reflections on the role of the private sector in pursuing ethical data practices,” Big Data & Society, vol. 7, no. 1, p. 205395172090889, Jan. 2020. [Online]. Available: http://journals.sagepub.com/doi/10.1177/2053951720908892 [32] J. P. Bajorek, “Voice Recognition Still Has Significant Race and Gender Biases,” Harvard Business Review, 2019, section: Technology. [Online]. Available: https: //hbr.org/2019/05/voice-recognition-still-has-significant-race-and-gender-biases [33] E. Bary, “How artificial intelligence could replace credit scores and reshape how we get loans,” 2018. [Online]. Available: https://www.marketwatch.com/story/ai-based -credit-scores-will-soon-give-one-billion-people-access-to-banking-services-2018 -10-09 [34] R. Benjamin, Race after technology: Abolitionist tools for the new jim code. John Wiley & Sons, 2019. [35] M. Broussard, Artificial Unintelligence: How Computers Misunderstand the World. MIT Press, 2018. [36] J. Boulamwini and T. Gebru, “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification,” in Proceedings of Machine Learning Research, 2018, pp. 77–91. [37] C. Criado-Perez, Invisible Women: Data Bias in a World Designed for Men. Abrams Press, 2019. [38] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness Through Awareness,” arXiv:1104.3913 [cs], Nov. 2011, arXiv: 1104.3913. [Online]. Available: http://arxiv.org/abs/1104.3913 [39] V. Eubanks, Automating inequality: How high-tech tools profile, police, and punish 57/77 the poor. St. Martin’s Press, 2018. [40] M. Hardt, E. Price, and N. Srebro, “Equality of Opportunity in Supervised Learning,” arXiv:1610.02413 [cs], Oct. 2016, arXiv: 1610.02413. [Online]. Available: http://arxiv.org/abs/1610.02413 [41] S. Noble, Algorithms of oppression: How search engines reinforce racism. NYU Press, 2018. [42] C. O’Neil, Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Broadway Books, 2017. [43] A. Pandey and A. Caliskan, “Iterative Effect-Size Bias in Ridehailing: Measuring Social Bias in Dynamic Pricing of 100 Million Rides,” arXiv:2006.04599 [cs], Jun. 2020, arXiv: 2006.04599. [Online]. Available: http://arxiv.org/abs/2006.04599 [44] J. Redden, “The Harm That Data Do,” Scientific American, 2018. [Online]. Available: https://www.scientificamerican.com/article/the-harm-that-data-do/ [45] M. Specia, “Siri and Alexa Reinforce Gender Bias, U.N. Finds,” The New York Times, May 2019. [Online]. Available: https://www.nytimes.com/2019/05/22/world /siri-alexa-ai-gender-bias.html [46] D. M. West, “Brookings survey finds worries over AI impact on jobs and personal privacy, concern U.S. will fall behind China,” Brookings, Tech. Rep., 2018. [47] S. Furman and J. Haney, “Is My Home Smart or Just Connected?” in International Conference on Human-Computer Interaction. Cham: Springer, 2020, pp. 273–287. [48] A. Kerr, M. Barry, and J. D. Kelleher, “Expectations of artificial intelligence and the performativity of ethics: Implications for communication governance,” Big Data & Society, vol. 7, no. 1, p. 205395172091593, Jan. 2020. [Online]. Available: http://journals.sagepub.com/doi/10.1177/2053951720915939 [49] E. Fast and E. Horvitz, “Long-Term Trends in the Public Perception of Artificial Intelligence,” Thirty-First AAAI Conference on Artificial Intelligence, p. 7, 2017. [50] A. Smith and M. Anderson, “Automation in Everyday Life,” Pew Research Center, Tech. Rep., 2017. [51] W. H. Ware, “Records, Computers and the Rights of Citizens,” RAND Corporation, Santa Monica, CA, Tech. Rep., 1973. [Online]. Available: https://www.rand.org/pubs/papers/P5077.html.Alsoavailableinprintform. [52] T. Feathers, “Major Universities Are Using Race as a “High Impact Predictor” of Student Success – The Markup,” 2021, section: News. [Online]. Available: https://themarkup.org/news/2021/03/02/major-universities-are-using-race-as-a-hig h-impact-predictor-of-student-success [53] L. Kirchner and M. Goldstein, “Access Denied: Faulty Automated Background Checks Freeze Out Renters – The Markup,” 2020, section: Locked Out. [Online]. Available: https://themarkup.org/locked-out/2020/05/28/access-denied-faulty-aut omated-background-checks-freeze-out-renters [54] I. Ajunwa, “The Paradox of Automation as Anti-Bias Intervention,” Cardozo L. Rev., vol. 41, p. 1671, 2020. [Online]. Available: https://heinonline.org/HOL/Pag e?handle=hein.journals/cdozo41&id=1711&div=&collection= 58/77 [55] M. Bogen and A. Rieke, “Help Wanted: An Examination of Hiring Algorithms, Equity, and Bias,” 2019. [Online]. Available: https://www.upturn.org/reports/2018/ hiring-algorithms [56] H. Schellmann, “Auditors are testing hiring algorithms for bias, but big questions remain,” 2021. [Online]. Available: https://www.technologyreview.com/2021/02/1 1/1017955/auditors-testing-ai-hiring-algorithms-bias-big-questions-remain/ [57] R. Bartlett, A. Morse, R. Stanton, and N. Wallace, “Consumer-Lending Discrimination in the FinTech Era,” National Bureau of Economic Research, Tech. Rep. w25943, Jun. 2019. [Online]. Available: https://www.nber.org/papers/w25943 [58] M. Henry-Nickie, “How artificial intelligence affects financial consumers,” 2019. [Online]. Available: https://www.brookings.edu/research/how-artificial-intelligence -affects-financial-consumers/ [59] M. Weber, M. Yurochkin, S. Botros, and V. Markov, “Black Loans Mat- ter: Distributionally Robust Fairness for Fighting Subgroup Discrimination,” arXiv:2012.01193 [cs], Nov. 2020, arXiv: 2012.01193. [Online]. Available: http://arxiv.org/abs/2012.01193 [60] H. Suresh and J. V. Guttag, “A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle,” arXiv:1901.10002 [cs, stat], Jun. 2021, arXiv: 1901.10002. [Online]. Available: http://arxiv.org/abs/1901.10002 [61] S. Barocas, M. Hardt, and A. Narayanan, Fairness and Machine Learning. fairml- book.org, 2019. [62] S. Russell and P. Norvig, Artificial Intelligence: A Modern Approach. Online edition, 2021, 4th US edition, http://aima.cs.berkeley.edu/index.html. [63] A. L. Samuel, “Some Studies in Machine Learning Using the Game of Checkers,” IBM Journal of Research and Development, vol. 3, no. 3, pp. 210–229, 1959. [64] S. Milano, M. Taddeo, and L. Floridi, “Recommender systems and their ethical chal- lenges,” AI & SOCIETY, vol. 35, 12 2020. [65] K. De Vries, “Identity, Profiling Algorithms and a World of Ambient Intelligence,” Ethics and Information Technology, vol. 12, pp. 71–85, 03 2010. [66] R. Richardson, J. M. Schultz, and K. Crawford, “Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, And Jus- tice,” New York University Law Review, vol. 94, p. 42, 2018. [67] D. Elliott, R. G. Lowitz, and W. C. NFP, “What Is the Cost of Poor Credit?” Wash- ington, DC: Urban Institute, 2018. [68] W. Haven, “Bias Isn’t the Only Problem with Credit Scores - and, No, AI Can’t Help,” MIT Technology Review, June 2021, https://www.technologyreview.com/2 021/06/17/1026519/racial-bias-noisy-data-credit-scores-mortgage-loans-fairness -machine-learning/. [69] L. Sarkesian and S. Singh, “HUD’s New Rule Paves the Way for Rampant Algo- rithmic Discrimination in Housing Decisions,” New America, Oct. 2020, https: //www.newamerica.org/oti/blog/huds-new-rule-paves-the-way-for-rampant-al gorithmic-discrimination-in-housing-decisions/. 59/77 [70] OECD, “Glossary of statistical terms,” OECD Online Resource, July 2007, https: //stats.oecd.org/glossary/detail.asp?ID=3605. [71] ISO, “Statistics — Vocabulary and symbols — Part 1: General statistical terms and terms used in probability,” ISO, Tech. Rep. ISO 3534-1:2006, 2006. [Online]. Available: https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standa rd/04/01/40145.html [72] B. Cowgill, F. Dell’Acqua, S. Deng, D. Hsu, N. Verma, and A. Chaintreau, “Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics,” arXiv:2012.02394 [cs, econ, q-fin], Dec. 2020, arXiv: 2012.02394. [Online]. Available: http://arxiv.org/abs/2012.02394 [73] S. Barocas and A. D. Selbst, “Big Data’s Disparate Impact,” California Law Review, vol. 104, no. 3, pp. 671–732, 2016, publisher: California Law Review, Inc. [Online]. Available: https://www.jstor.org/stable/24758720 [74] S. Costanza-Chock, “Design Justice, A.I., and Escape from the Matrix of Domination,” Journal of Design and Science, Jul. 2018. [Online]. Available: https://jods.mitpress.mit.edu/pub/costanza-chock [75] M. Elish, S. Barocas, A. Plasek, and K. Ferryman, “The social & economic implications of artificial intelligence technologies in the near-term,” AI Now, New York, Tech. Rep., 2016. [Online]. Available: https://ainowinstitute.org/AI Now 201 6 Primers.pdf [76] A. Jacobs and H. Wallach, “Measurement and Fairness,” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 375–385, Mar. 2021, arXiv: 1912.05511. [Online]. Available: http://arxiv.org/abs/1912.05511 [77] S. Passi and S. Barocas, “Problem Formulation and Fairness,” Proceedings of the Conference on Fairness, Accountability, and Transparency, pp. 39–48, Jan. 2019, arXiv: 1901.02547. [Online]. Available: http://arxiv.org/abs/1901.02547 [78] A. D. Selbst, D. Boyd, S. A. Friedler, S. Venkatasubramanian, and J. Vertesi, “Fairness and Abstraction in Sociotechnical Systems,” in Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* ’19. Atlanta, GA, USA: ACM Press, 2019, pp. 59–68. [Online]. Available: http://dl.acm.org/citation.cfm?doid=3287560.3287598 [79] A. Tversky and D. Kahneman, “Judgment under Uncertainty: Heuristics and Biases,” Science, vol. 185, no. 4157, pp. 1124–1131, 1974, publisher: American Association for the Advancement of Science. [Online]. Available: https://www.jstor.org/stable/1738360 [80] A. Caliskan, J. J. Bryson, and A. Narayanan, “Semantics derived automatically from language corpora contain human-like biases,” Science, vol. 356, no. 6334, pp. 183–186, Apr. 2017. [Online]. Available: https://www.sciencemag.org/lookup/doi/ 10.1126/science.aal4230 [81] D. Danks and A. J. London, “Algorithmic Bias in Autonomous Systems,” in Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. Melbourne, Australia: International Joint Conferences on Artificial 60/77 Intelligence Organization, Aug. 2017, pp. 4691–4697. [Online]. Available: https://www.ijcai.org/proceedings/2017/654 [82] T. Hellstr ̈om, V. Dignum, and S. Bensch, “Bias in Machine Learning – What is it Good for?” arXiv:2004.00686 [cs], Sep. 2020, arXiv: 2004.00686. [Online]. Available: http://arxiv.org/abs/2004.00686 [83] ISO/IEC, “ISO/IEC 2382:2015, Information technology — Vocabulary,” Interna- tional Organization for Standardization, Geneva, Switzerland, Tech. Rep., 2015. [Online]. Available: https://www.iso.org/obp/ui/#iso:std:iso-iec:2382:ed-1:v1:en [84] ——, “ISO/IEC 20546:2019, Information technology — Big data — Overview and vocabulary,” International Organization for Standardization, Geneva, Switzerland, Tech. Rep., 2019. [Online]. Available: https://www.iso.org/obp/ui/#iso:std:iso-iec: 20546:ed-1:v1:en [85] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, “A Survey on Bias and Fairness in Machine Learning,” arXiv:1908.09635 [cs], Sep. 2019, arXiv: 1908.09635. [Online]. Available: http://arxiv.org/abs/1908.09635 [86] M. Mitchell, Artificial Intelligence: A Guide for Thinking Human. Farrar, Straus, and Giroux, 2019. [87] S. Mitchell and J. Shadlen, “Mirror mirror: Reflections on quantitative fairness. Shira Mitchell: Statistician,” Dec 2020. [Online]. Available: https: //shiraamitchell.github.io/fairness/ [88] D. K. Mulligan, J. A. Kroll, N. Kohli, and R. Y. Wong, “This Thing Called Fairness: Disciplinary Confusion Realizing a Value in Technology,” Proc. ACM Hum.-Comput. Interact., vol. 3, no. CSCW, pp. 1–36, Nov. 2019, arXiv: 1909.11869. [Online]. Available: http://arxiv.org/abs/1909.11869 [89] Organisation for Economic Co-operation and Development, “Recommendation of the Council on Artificial Inteliigence,” 2019. [Online]. Available: https: //legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449 [90] H. Suresh and J. Guttag, “A Framework for Understanding Unintended Consequences of Machine Learning,” arXiv:1901.10002 [cs, stat], Feb. 2020, arXiv: 1901.10002. [Online]. Available: http://arxiv.org/abs/1901.10002 [91] D. Chandler and R. Munday, A Dictionary of Media and Communication. Oxford University Press, Jan. 2011, publication Title: A Dictionary of Media and Communication. [Online]. Available: https://www.oxfordreference.com/view/10.10 93/acref/9780199568758.001.0001/acref-9780199568758 [92] M. Ngan, P. J. Grother, and M. Ngan, Face recognition vendor test (FRVT) perfor- mance of automated gender classification algorithms. US Department of Com- merce, National Institute of Standards and Technology, 2015. [93] D. I. Perrett, K. J. Lee, I. Penton-Voak, D. Rowland, S. Yoshikawa, D. M. Burt, S. Henzi, D. L. Castles, and S. Akamatsu, “Effects of sexual dimorphism on facial attractiveness,” Nature, vol. 394, no. 6696, pp. 884–887, 1998. [94] I. M. Scott, A. P. Clark, S. C. Josephson, A. H. Boyette, I. C. Cuthill, R. L. Fried, M. A. Gibson, B. S. Hewlett, M. Jamieson, W. Jankowiak et al., “Human prefer- 61/77 ences for sexually dimorphic faces may be evolutionarily novel,” Proceedings of the National Academy of Sciences, vol. 111, no. 40, pp. 14 388–14 393, 2014. [95] K. Kleisner, P. Tureˇcek, S. C. Roberts, J. Havl ́ıˇcek, J. V. Valentova, R. M. Akoko, J. D. Leong ́omez, S. Apostol, M. A. Varella, and S. A. Saribay, “How and why pat- terns of sexual dimorphism in human faces vary across the world,” Scientific reports, vol. 11, no. 1, pp. 1–14, 2021. [96] O. Keyes, “The misgendering machines: Trans/hci implications of automatic gender recognition,” Proceedings of the ACM on Human-Computer Interaction, vol. 2, no. CSCW, 2018. [Online]. Available: https://doi.org/10.1145/3274357 [97] Organization of Scientific Area Committees for Forensic Science, “OSAC Preferred Terms,” 2021. [Online]. Available: https://www.nist.gov/system/files/documents/2 021/04/28/OSAC%20Preferred%20Terms April%202021.pdf [98] D. Kahneman, S. P. Slovic, P. Slovic, and A. Tversky, Judgment under uncertainty: Heuristics and biases. Cambridge university press, 1982. [99] A. I. Al-Alawi, M. Naureen, E. I. AlAlawi, and A. A. N. Al-Hadad, “The Role of Artificial Intelligence in Recruitment Process Decision-Making,” in 2021 Interna- tional Conference on Decision Aid Sciences and Application (DASA). IEEE, 2021, pp. 197–203. [100] A. Rieke, U. Janardan, M. Hsu, and N. Duarte, “Essential Work: Analyzing the Hiring Technologies of Large Hourly Employers,” Upturn, July 2021, https://www. upturn.org/reports/2021/essential-work/. [101] L. X. Z. Brown and M. Richardson, “Algorithm-driven Hiring Tools: Innovative Recruitment or Expedited Disability Discrimination?” Ctr. for Democracy & Tech., Dec. 2020, https://cdt.org/insights/report-algorithm-driven-hiring-tools-innovative -recruitment-or-expedited-disability-discrimination. [102] A. D’Amour, K. Heller, D. Moldovan, B. Adlam, B. Alipanahi, A. Beutel, C. Chen, J. Deaton, J. Eisenstein, M. D. Hoffman, F. Hormozdiari, N. Houlsby, S. Hou, G. Jerfel, A. Karthikesalingam, M. Lucic, Y. Ma, C. McLean, D. Mincu, A. Mitani, A. Montanari, Z. Nado, V. Natarajan, C. Nielson, T. F. Osborne, R. Raman, K. Ramasamy, R. Sayres, J. Schrouff, M. Seneviratne, S. Sequeira, H. Suresh, V. Veitch, M. Vladymyrov, X. Wang, K. Webster, S. Yadlowsky, T. Yun, X. Zhai, and D. Sculley, “Underspecification Presents Challenges for Credibility in Modern Machine Learning,” arXiv:2011.03395 [cs, stat], Nov. 2020, arXiv: 2011.03395. [Online]. Available: http://arxiv.org/abs/2011.03395 [103] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. Chatterji, A. Chen, K. Creel, J. Q. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. Gillespie, K. Goel, N. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Kohd, M. Krass, R. Krishna, R. Kuditipudi, A. Kumar, 62/77 F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. Roohani, C. Ruiz, J. Ryan, C. R ́e, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tram`er, R. E. Wang, W. Wang, B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang, “On the Opportunities and Risks of Foundation Models,” arXiv:2108.07258 [cs], Aug. 2021, arXiv: 2108.07258. [Online]. Available: http://arxiv.org/abs/2108.07258 [104] D. Schiff, A. Ayesh, L. Musikanski, and J. C. Havens, “IEEE 7010: A new standard for assessing the well-being implications of artificial intelligence,” 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Oct 2020. [Online]. Available: http://dx.doi.org/10.1109/SMC42975.2020.9283454 [105] A. Birhane, P. Kalluri, D. Card, W. Agnew, R. Dotan, and M. Bao, “The Values Encoded in Machine Learning Research,” arXiv:2106.15590 [cs], Jun. 2021, arXiv: 2106.15590. [Online]. Available: http://arxiv.org/abs/2106.15590 [106] N. Schmidt and B. Stephens, “An Introduction to Artificial Intelligence and Solutions to the Problems of Algorithmic Discrimination,” arXiv preprint arXiv:1911.05755, 2019. [107] C. Barabas, C. Doyle, J. Rubinovitz, and K. Dinakar, “Studying Up: Reorienting the Study of Algorithmic Fairness Around Issues of Power,” in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 2020, pp. 167–176. [108] B. Fish and L. Stark, “Reflexive Design for Fairness and Other Human Values in Formal Models,” arXiv:2010.05084 [cs], Oct. 2020, arXiv: 2010.05084. [Online]. Available: http://arxiv.org/abs/2010.05084 [109] K. Robertson, C. Khoo, and Y. Song, “To Surveil and Predict: A Human Rights Analysis of Algorithmic Policing in Canada,” Citizen Lab & Int’l Hum. Rts. Prog., U. Toronto, Sept. 2020, https://citizenlab.ca/wp-content/uploads/2020/09/To-Surv eil-and-Predict.pdf. [110] S. C. Slota, K. R. Fleischmann, S. Greenberg, N. Verma, B. Cummings, L. Li, and C. Shenefiel, “Many hands make many fingers to point: challenges in creating accountable AI,” AI & Soc, Nov. 2021. [Online]. Available: https://link.springer.com/10.1007/s00146-021-01302-0 [111] S. Mitchell, E. Potash, S. Barocas, A. D’Amour, and K. Lum, “Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions,” Annu. Rev. Stat. Appl., vol. 8, no. 1, pp. 141–163, Mar. 2021, arXiv: 1811.07867. [Online]. Available: http://arxiv.org/abs/1811.07867 [112] B. Green, “The Flaws of Policies Requiring Human Oversight of Government Algorithms,” SSRN Journal, 2021. [Online]. Available: https://www.ssrn.com/abstr act=3921216 63/77 [113] B. Green and A. Kak, “The false comfort of human oversight as an antidote to A.I. harm.” [Online]. Available: https://slate.com/technology/2021/06/human-oversight -artificial-intelligence-laws.html [114] M. Boyarskaya, A. Olteanu, and K. Crawford, “Overcoming Failures of Imagination in AI Infused System Development and Deployment,” arXiv:2011.13416 [cs], Dec. 2020, arXiv: 2011.13416. [Online]. Available: http://arxiv.org/abs/2011.13416 [115] D. Boyd and K. Crawford, “CRITICAL QUESTIONS FOR BIG DATA: Provocations for a cultural, technological, and scholarly phenomenon,” Information, Communication & Society, vol. 15, no. 5, pp. 662–679, Jun. 2012. [Online]. Available: http://www.tandfonline.com/doi/abs/10.1080/1369118X.2012.678878 [116] C. D’Ignazio and L. Klein, Data Feminism. MIT Press, 2020. [Online]. Available: https://data-feminism.mitpress.mit.edu/ [117] A. Jacobs, S. L. Blodgett, S. Barocas, H. Daum ́e, and H. Wallach, “The meaning and measurement of bias: lessons from natural language processing,” in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, ser. FAT* ’20. New York, NY, USA: Association for Computing Machinery, Jan. 2020, p. 706. [Online]. Available: https://doi.org/10.1145/3351095.3375671 [118] E. Moss and J. Metcalf, “High Tech, High Risk: Tech Ethics Lessons for the COVID- 19 Pandemic Response,” Patterns, vol. 1, no. 7, p. 100102, Oct. 2020. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S2666389920301367 [119] A. L. Washington and R. Kuo, “Whose side are ethics codes on?: power, responsibility and the social good,” in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. Barcelona Spain: ACM, Jan. 2020, pp. 230–240. [Online]. Available: https://dl.acm.org/doi/10.1145/3351095.3372844 [120] E. Morozov, To save everything, click here: The folly of technological solutionism. Penn State University Press, 2013. [121] B. Aguera y Arcas, M. Mitchell, and A. Todorov, “Physiognomy’s new clothes,” May 2017. [Online]. Available: https://medium.com/@blaisea/physiognomys-new- clothes-f2d4b59fdd6a [122] J. A. Kroll, “Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems,” Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 758–771, Mar. 2021, arXiv: 2101.09385. [Online]. Available: http://arxiv.org/abs/2101.09385 [123] R. Berk, H. Heidari, S. Jabbari, M. Joseph, M. Kearns, J. Morgenstern, S. Neel, and A. Roth, “A convex framework for fair regression,” 2017. [124] R. Tromble, “Where Have All the Data Gone? A Critical Reflection on Academic Digital Research in the Post-API Age,” Social Media + Society, vol. 7, no. 1, p. 2056305121988929, Jan. 2021, publisher: SAGE Publications Ltd. [Online]. Available: https://doi.org/10.1177/2056305121988929 [125] A. Cobham, The Uncounted. John Wiley & Sons, 2020. [126] D. Stone, Counting: How We Use Numbers to Decide What Matters. Liveright Publishing, 2020. 64/77 [127] B. Plank, “What to do about non-standard (or non-canonical) language in NLP,” arXiv:1608.07836 [cs], Aug. 2016, arXiv: 1608.07836. [Online]. Available: http://arxiv.org/abs/1608.07836 [128] Y. Tan and L. E. Celis, “Assessing Social and Intersectional Biases in Contextualized Word Representations,” arXiv:1911.01485 [cs, stat], Nov. 2019, arXiv: 1911.01485. [Online]. Available: http://arxiv.org/abs/1911.01485 [129] A. Paullada, I. D. Raji, E. M. Bender, E. Denton, and A. Hanna, “Data and its (dis)contents: A survey of dataset development and use in machine learning research,” arXiv:2012.05345 [cs], Dec. 2020, arXiv: 2012.05345. [Online]. Available: http://arxiv.org/abs/2012.05345 [130] H. Abdollahpouri, M. Mansoury, R. Burke, and B. Mobasher, “The Unfairness of Popularity Bias in Recommendation,” arXiv:1907.13286 [cs], Sep. 2019, arXiv: 1907.13286. [Online]. Available: http://arxiv.org/abs/1907.13286 [131] R. Baeza-Yates, “Bias on the web,” Commun. ACM, vol. 61, no. 6, pp. 54–61, 2018. [Online]. Available: https://dl.acm.org/doi/10.1145/3209581 [132] A. Lambrecht and C. E. Tucker, “Algorithmic Bias? An Empirical Study into Apparent Gender-Based Discrimination in the Display of STEM Career Ads,” SSRN Journal, 2016. [Online]. Available: http://www.ssrn.com/abstract=2852260 [133] M. Miceli, J. Posada, and T. Yang, “Studying Up Machine Learning Data: Why Talk About Bias When We Mean Power?” arXiv:2109.08131 [cs], Sep. 2021, arXiv: 2109.08131. [Online]. Available: http://arxiv.org/abs/2109.08131 [134] E. H. Simpson, “The interpretation of interaction in contingency tables,” Journal of the Royal Statistical Society. Series B (Methodological), vol. 13, no. 2, pp. 238–241, 1951. [Online]. Available: http://www.jstor.org/stable/2984065 [135] F. P. Calmon, D. Wei, K. N. Ramamurthy, and K. R. Varshney, “Optimized data pre-processing for discrimination prevention,” 2017. [136] F. Kamiran and T. Calders, “Data preprocessing techniques for classification without discrimination,” Knowledge and Information Systems, vol. 33, no. 1, pp. 1–33, 2012. [Online]. Available: https://doi.org/10.1007/s10115-011-0463-8 [137] M. Feldman, S. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, “Certifying and removing disparate impact,” 2015. [Online]. Available: https: //arxiv.org/abs/1412.3756 [138] K. Peng, A. Mathur, and A. Narayanan, “Mitigating dataset harms requires stewardship: Lessons from 1000 papers,” arXiv:2108.02922 [cs], Aug. 2021, arXiv: 2108.02922. [Online]. Available: http://arxiv.org/abs/2108.02922 [139] R. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovic, S. Nagar, K. N. Ramamurthy, J. Richards, D. Saha, P. Sattigeri, M. Singh, K. R. Varshney, and Y. Zhang, “AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias,” arXiv:1810.01943 [cs], Oct. 2018, arXiv: 1810.01943. [Online]. Available: http://arxiv.org/abs/1810.01943 [140] C. G. Northcutt, A. Athalye, and J. Mueller, “Pervasive label errors in test sets desta- 65/77 bilize machine learning benchmarks,” 2021. [141] T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, and A. Kalai, “Quantifying and Reducing Stereotypes in Word Embeddings,” arXiv:1606.06121, vol. 1, p. 5, 2016. [142] P. Parasurama and J. Sedoc, “Gendered Language in Resumes and its Implications for Algorithmic Bias in Hiring,” arXiv:2112.08910 [cs], Dec. 2021, arXiv: 2112.08910. [Online]. Available: http://arxiv.org/abs/2112.08910 [143] A. L. Hoffmann, “Where fairness fails: data, algorithms, and the limits of antidis- crimination discourse,” Information, Communication & Society, vol. 22, no. 7, pp. 900–915, 2019. [144] A. Olteanu, C. Castillo, F. Diaz, and E. Kıcıman, “Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries,” Front. Big Data, vol. 2, p. 13, Jul. 2019. [Online]. Available: https://www.frontiersin.org/article/10.3389/fdata.2019.0 0013/full [145] T. N. Bond and K. Lang, “The sad truth about happiness scales,” Journal of Political Economy, vol. 127, no. 4, pp. 1629–1640, 2019. [146] D. Kahneman, A. M. Rosenfield, L. Gandhi, and T. Blaser, “Noise: How to Overcome the High, Hidden Cost of Inconsistent Decision Making,” Harvard Business Review, Oct. 2016, section: Decision making and problem solving. [Online]. Available: https://hbr.org/2016/10/noise [147] M. M. Malik, “A Hierarchy of Limitations in Machine Learning,” arXiv:2002.05193 [cs, econ, math, stat], Feb. 2020, arXiv: 2002.05193. [Online]. Available: http://arxiv.org/abs/2002.05193 [148] G. Friedman and T. McCarthy, “Employment Law Red Flags in the Use of Artificial Intelligence in Hiring,” 2019. [Online]. Available: https://www.americanbar.org/gr oups/business law/publications/blt/2020/10/ai-in-hiring/ [149] E. H ̈ullermeier and W. Waegeman, “Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods,” Machine Learning, vol. 110, no. 3, p. 457–506, Mar 2021. [Online]. Available: http://dx.doi.org/10.1007/s10994-021-05946-3 [150] Y. Nesterov, Introductory lectures on convex optimization, 2004th ed., ser. Applied Optimization. New York, NY: Springer, Dec. 2003. [151] M. J. Kochenderfer and T. A. Wheeler, Algorithms for Optimization. London, England: MIT Press, Mar. 2019. [152] L. Breiman, “Statistical modeling: The two cultures (with comments and a rejoinder by the author),” Statistical science, vol. 16, no. 3, pp. 199–231, 2001. [153] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Transformers for image recognition at scale,” ArXiv, vol. abs/2010.11929, 2021. [154] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, “On the dangers of stochastic parrots: Can language models be too big?” in Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, ser. FAccT ’21. 66/77 New York, NY, USA: Association for Computing Machinery, 2021, p. 610–623. [Online]. Available: https://doi.org/10.1145/3442188.3445922 [155] C. Wagner, D. Garcia, M. Jadidi, and M. Strohmaier, “It’s a man’s Wikipedia? As- sessing gender inequality in an online encyclopedia,” in Proceedings of the interna- tional AAAI conference on web and social media, vol. 9, no. 1, 2015, pp. 454–463. [156] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun, “The loss surfaces of multilayer networks,” in Artificial intelligence and statistics. PMLR, 2015, pp. 192–204. [157] IEEE, “IEEE Standard for Floating-Point Arithmetic,” IEEE Std 754-2008, pp. 1–70, 2008. [158] K. Novak, Numerical Methods for Scientific Computing: The Definitive Manual for Math Geeks, 2nd ed. Equal Share Press, 2022. [159] M. J. Wolf, K. W. Miller, and F. S. Grodzinsky, “Why we should have seen that coming: comments on Microsoft’s Tay “experiment,” and wider implications,” The ORBIT Journal, vol. 1, no. 2, pp. 1–12, 2017. [160] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cas- sirer, R. Powell, G. van den Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Bud- den, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Men- sch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d’Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. John- son, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving, “Scaling language models: Methods, analysis & insights from training gopher,” 2022. [161] A. Tamkin, M. Brundage, J. Clark, and D. Ganguli, “Understanding the capabilities, limitations, and societal impact of large language models,” 2021. [162] A. F. Ansari, M. L. Ang, and H. Soh, “Refining deep generative models via discrim- inator gradient flow,” 2021. [163] J. Z. Forde, A. F. Cooper, K. Kwegyir-Aggrey, C. De Sa, and M. Littman, “Model Selection’s Disparate Impact in Real-World Deep Learning Applications,” arXiv:2104.00606 [cs], Apr. 2021, arXiv: 2104.00606. [Online]. Available: http://arxiv.org/abs/2104.00606 [164] S. L. Blodgett, S. Barocas, H. Daum ́e III, and H. Wallach, “Language (Technology) is Power: A Critical Survey of ”Bias” in NLP,” arXiv:2005.14050 [cs], May 2020, arXiv: 2005.14050. [Online]. Available: http://arxiv.org/abs/2005.14050 [165] G. Neff and P. Nagy, “Automation, Algorithms, and Politics Talking to Bots: Symbiotic Agency and the Case of Tay,” International Journal of 67/77 Communication, vol. 10, no. 0, p. 17, Oct. 2016, number: 0. [Online]. Available: https://ijoc.org/index.php/ijoc/article/view/6277 [166] D. Hovy and S. Prabhumoye, “Five sources of bias in natural language processing,” Language and Linguistics Compass, vol. 15, no. 8, p. e12432, 2021, eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12432. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12432 [167] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann, “Shortcut Learning in Deep Neural Networks,” Nature Machine Intelligence, vol. 2, no. 11, pp. 665–673, 2020. [168] M. Hashemi and M. Hall, “RETRACTED ARTICLE: Criminal Tendency Detection from Facial Images and the Gender Bias Effect,” Journal of Big Data, vol. 7, no. 1, pp. 1–16, 2020. [169] BBC, “Facial recognition to ’predict criminals’ sparks row over AI bias,” BBC On- line News, June 2020, https://www.bbc.com/news/technology-53165286. [170] S. Levin, “New AI can guess whether you’re gay or straight from a photograph,” The Guardian, Sept. 2017, https://www.theguardian.com/technology/2017/sep/07/new-a rtificial-intelligence-can-tell-whether-youre-gay-or-straight-from-a-photograph. [171] J. D. West and C. T. Bergstrom, “Misinformation In and About Science,” Proceed- ings of the National Academy of Sciences, vol. 118, no. 15, 2021. [172] A. E. Miller, “Searching for gaydar: Blind spots in the study of sexual orientation perception,” Psychology & Sexuality, vol. 9, no. 3, pp. 188–203, 2018. [173] L. F. Barrett, R. Adolphs, S. Marsella, A. M. Martinez, and S. D. Pollak, “Emotional Expressions Reconsidered: Challenges to Inferring Emotion from Human Facial Movements,” Psychological science in the public interest, vol. 20, no. 1, pp. 1–68, 2019. [174] B. M. Booth, L. Hickman, S. K. Subburaj, L. Tay, S. E. Woo, and S. K. D’Mello, “Bias and Fairness in Multimodal Machine Learning: A Case Study of Automated Video Interviews,” in Proceedings of the 2021 International Conference on Multimodal Interaction. Montr ́eal QC Canada: ACM, Oct. 2021, pp. 268–277. [Online]. Available: https://dl.acm.org/doi/10.1145/3462244.3479897 [175] A. Narayanan, “How to recognize AI snake oil,” CITP (Princeton U.), Feb. 2022, https://www.cs.princeton.edu/∼arvindn/talks/MIT-STS-AI-snakeoil.pdf. [176] T. H. Davenport and J. G. Harris, Competing on analytics. Boston, MA: Harvard Business Review Press, Feb. 2007. [177] J. Kleinberg, J. Ludwig, S. Mullainathan, and C. R. Sunstein, “Algorithms as discrimination detectors,” Proc Natl Acad Sci USA, vol. 117, no. 48, pp. 30 096–30 100, Dec. 2020. [Online]. Available: http://www.pnas.org/lookup/doi/10 .1073/pnas.1912790117 [178] M. H. Jarrahi, G. Newlands, M. K. Lee, C. T. Wolf, E. Kinder, and W. Sutherland, “Algorithmic management in a work context,” Big Data & Society, vol. 8, no. 2, p. 20539517211020332, 2021. [179] S. Brayne, Predict and surveil. New York, NY: Oxford University Press, Jan. 2021. 68/77 [180] B. Cowgill, “Bias and Productivity in Humans and Algorithms: Theory and Evi- dence from Resume Screening,” Columbia Business School, Columbia University, p. 35, 2020. [181] C. Thompson, “Who’s homeless enough for housing? in san francisco an algorithm decides,” Nov 2021. [Online]. Available: https://www.codastory.com/authoritarian- tech/san-francisco-homeless-algorithm/ [182] Office of Inspector General, “Advisory concerning the Chicago police de- partment’s predictive risk models,” City of Chicago, Tech. Rep., Jan. 2020, https://igchicago.org/wp-content/uploads/2020/01/OIG-Advisory-Concerning- CPDs-Predictive-Risk-Models-.pdf. [183] D. Hunter and N. Evans, “Facebook emotional contagion experiment controversy,” Research Ethics, vol. 12, no. 1, pp. 2–3, 2016. [184] Y. Wang and M. Kosinski, “Deep neural networks are more accurate than humans at detecting sexual orientation from facial images,” Journal of Personality and Social Psychology, vol. 114, p. 246–257, 2018. [185] M. Roberts, D. Driggs, M. Thorpe, J. D. Gilbey, M. Yeung, S. Ursprung, A. I. Avil ́es- Rivero, C. Etmann, C. McCague, L. Beer, J. R. Weir-McCall, Z. Teng, E. Gkrania- Klotsas, J. H. F. Rudd, E. Sala, and C.-B. Sch ̈onlieb, “Common pitfalls and rec- ommendations for using machine learning to detect and prognosticate for covid-19 using chest radiographs and ct scans,” Nat. Mach. Intell., vol. 3, pp. 199–217, 2021. [186] L. Wynants, B. Calster, G. Collins, R. Riley, G. Heinze, E. Schuit, M. Bonten, D. Dahly, J. Damen, T. Debray, V. Jong, M. Vos, P. Dhiman, M. Haller, M. Harhay, L. Henckaerts, P. Heus, M. Kammer, N. Kreuzberger, A. Lohmann, K. Luijken, J. Ma, G. Martin, D. McLernon, C. Navarro, J. Reitsma, J. Sergeant, C. Shi, N. Skoetz, L. Smits, K. Snell, M. Sperrin, R. Spijker, E. Steyerberg, T. Takada, I. Tzoulaki, S. Kuijk, B. Bussel, I. Horst, F. Royen, J. Verbakel, C. Wallisch, J. Wilkinson, R. Wolff, L. Hooft, K. Moons, and M. Smeden, “Prediction models for diagnosis and prognosis of covid-19: systematic review and critical appraisal,” BMJ, vol. 369, p. m1328, Apr. 2020, publisher: British Medical Journal Publishing Group Section: Research. [Online]. Available: https://www.bmj.com/content/369/bmj.m1328 [187] M. Hutson, Has artificial intelligence become alchemy? American Association for the Advancement of Science, 2018. [188] R. Dijkgraaf, Quanta Magazine, Oct 2021. [Online]. Available: https://www.quanta magazine.org/science-has-entered-a-new-era-of-alchemy-good-20211020/ [189] L. Wynants, B. Van Calster, G. S. Collins, R. D. Riley, G. Heinze, E. Schuit, M. M. Bonten, D. L. Dahly, J. A. Damen, T. P. Debray et al., “Prediction models for diag- nosis and prognosis of covid-19: systematic review and critical appraisal,” bmj, vol. 369, 2020. [190] M. Sap, D. Card, S. Gabriel, Y. Choi, and N. A. Smith, “The risk of racial bias in hate speech detection,” in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association 69/77 for Computational Linguistics, Jul. 2019, pp. 1668–1678. [Online]. Available: https://aclanthology.org/P19-1163 [191] E. Ntoutsi, P. Fafalios, U. Gadiraju, V. Iosifidis, W. Nejdl, M.-E. Vidal, S. Ruggieri, F. Turini, S. Papadopoulos, E. Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, C. Wagner, F. Karimi, H. Alani, B. Berendt, T. Kruegel, C. Heinze, K. Broelemann, G. Kasneci, T. Tiropanis, and S. Staab, “Bias in Data-driven AI Systems - An Intro- ductory Survey,” arXiv:2001.09762v1 [cs.CY], p. 19, 2020. [192] “Leveraging responsible AI to counteract bias in health care,” Aug. 2021. [Online]. Available: https://www.statnews.com/2021/08/06/leverage-responsible-ai-countera ct-bias-health-care/ [193] A. Słowik and L. Bottou, “Algorithmic Bias and Data Bias: Understanding the Relation between Distributionally Robust Optimization and Data Curation,” arXiv:2106.09467 [cs, stat], Jun. 2021, arXiv: 2106.09467. [Online]. Available: http://arxiv.org/abs/2106.09467 [194] K. R. Varshney, Trustworthy Machine Learning. Chappaqua, NY, USA: Indepen- dently Published, 2022. [195] CFPB, “Using publicly available information to proxy for unidentified race and eth- nicity,” Consumer Financial Protection Bureau, 2014, https://files.consumerfinance .gov/f/201409 cfpb report proxy-methodology.pdf. [196] N. Gill, P. Hall, K. Montgomery, and N. Schmidt, “A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrim- ination Testing,” Information, vol. 11, no. 3, p. 137, 2020. [197] S. Venkatasubramanian, C. Scheidegger, S. Friedler, and A. Clauset, Fairness in Networks: Social Capital, Information Access, and Interventions. New York, NY, USA: Association for Computing Machinery, 2021, p. 4078–4079. [Online]. Available: https://doi.org/10.1145/3447548.3470821 [198] S. A. Friedler, C. Scheidegger, and S. Venkatasubramanian, “The (im)possibility of fairness: Different value systems require different mechanisms for fair decision making,” Commun. ACM, vol. 64, no. 4, p. 136–143, mar 2021. [Online]. Available: https://doi.org/10.1145/3433949 [199] M. J. Kusner, J. R. Loftus, C. Russell, and R. Silva, “Counterfactual Fairness,” arXiv:1703.06856 [cs, stat], Mar. 2018, arXiv: 1703.06856. [Online]. Available: http://arxiv.org/abs/1703.06856 [200] L. Wang, “Race as proxy: Situational racism and self-fulfilling stereotypes,” DePaul Law Review, vol. 53, p. 1013, 2004. [201] A. Agan and S. Starr, “Ban the Box, Criminal Records, and Racial Discrimination: A Field Experiment*,” The Quarterly Journal of Economics, vol. 133, no. 1, pp. 191–235, 08 2017. [Online]. Available: https://doi.org/10.1093/qje/qjx028 [202] K. Fiscella and A. M. Fremont, “Use of geocoding and surname analysis to estimate race and ethnicity,” Health services research, vol. 41, pp. 1482–500, 2006. [203] S. Jabbari, M. Joseph, M. Kearns, J. Morgenstern, and A. Roth, “Fairness in Reinforcement Learning,” arXiv:1611.03071 [cs], Aug. 2017, arXiv: 1611.03071. 70/77 [Online]. Available: http://arxiv.org/abs/1611.03071 [204] A. D’Amour, H. Srinivasan, J. Atwood, P. Baljekar, D. Sculley, and Y. Halpern, “Fairness is not static: Deeper understanding of long term fairness via simulation studies,” in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, ser. FAT* ’20. New York, NY, USA: Association for Computing Machinery, 2020, p. 525–534. [Online]. Available: https://doi.org/10.1145/3351095.3372878 [205] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Nee- lakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCan- dlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” 2020. [206] N. Kilbertus, M. Rojas-Carulla, G. Parascandolo, M. Hardt, D. Janzing, and B. Sch ̈olkopf, “Avoiding discrimination through causal reasoning,” 2018. [207] J. Kleinberg, S. Mullainathan, and M. Raghavan, “Inherent trade-offs in the fair determination of risk scores,” 2016. [208] K. T. Rodolfa, H. Lamba, and R. Ghani, “Empirical observation of negligible fairness–accuracy trade-offs in machine learning for public policy,” Nature Machine Intelligence, vol. 3, no. 10, pp. 896–904, 2021. [209] R. Richardson, “Defining and demystifying automated decision systems,” March 2021, forthcoming. [Online]. Available: https://ssrn.com/abstract=3D3811708 [210] R. Binns, M. Van Kleek, M. Veale, U. Lyngs, J. Zhao, and N. Shadbolt, “’It’s Reducing a Human Being to a Percentage’; Perceptions of Justice in Algorithmic Decisions,” Montreal QC, Canada, Jan. 2018. [Online]. Available: https://osf.io/9wqxr [211] C. M. Scaparrotti, Joint Publication 3-13 Information Operations. Citeseer, 2012. [212] G. Raman, B. AlShebli, M. Waniek, T. Rahwan, and J. C.-H. Peng, “How weaponizing disinformation can bring down a city’s power grid,” PLOS ONE, vol. 15, pp. 1–14, 08 2020. [Online]. Available: https: //doi.org/10.1371/journal.pone.0236517 [213] E. Ferrara, O. Varol, C. Davis, F. Menczer, and A. Flammini, “The rise of social bots,” Commun. ACM, vol. 59, no. 7, p. 96–104, jun 2016. [Online]. Available: https://doi.org/10.1145/2818717 [214] W. Phillips, “The Oxygen of Amplification: Better Practices for Reporting on Extremists, Antagonists, and Manipulators Online,” Data & Society, May 2018, https://datasociety.net/wp-content/uploads/2018/05/FULLREPORT Oxygen o f Amplification DS.pdf. [215] S. Alon-Barkat and M. Busuioc, “Decision-makers Processing of AI Algorithmic Advice: Automation Bias versus Selective Adherence,” arXiv:2103.02381 [cs], Mar. 2021, arXiv: 2103.02381. [Online]. Available: http://arxiv.org/abs/2103.02381 [216] B. J. Dietvorst, J. P. Simmons, and C. Massey, “Algorithm aversion: people erro- 71/77 neously avoid algorithms after seeing them err,” J Exp Psychol Gen, vol. 144, no. 1, pp. 114–126, Feb. 2015. [217] ——, “Overcoming Algorithm Aversion: People Will Use Imperfect Algorithms If They Can (Even Slightly) Modify Them,” Management Science, vol. 64, no. 3, pp. 1155–1170, Mar. 2018. [Online]. Available: http://pubsonline.informs.org/doi/10. 1287/mnsc.2016.2643 [218] M. Veale, M. Van Kleek, and R. Binns, “Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making,” in Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18. Montreal QC, Canada: ACM Press, 2018, pp. 1–14. [Online]. Available: http://dl.acm.org/citation.cfm?doid=3173574.3174014 [219] S. Picard, M. Watkins, M. Rempal, and A. Kerodal, “Beyond the Algorithm: Pretrial Reform, Risk Assessment, and Racial Fairness,” Center for Court Innovation, Tech. Rep., 2020. [Online]. Available: https://www.courtinnovation.org/publications/bey ond-algorithm [220] B. Knowles and J. T. Richards, “The Sanction of Authority: Promoting Public Trust in AI,” arXiv:2102.04221 [cs], Jan. 2021, arXiv: 2102.04221. [Online]. Available: http://arxiv.org/abs/2102.04221 [221] C. Prunkl, C. Ashurst, M. Anderljung, H. Webb, J. Leike, and A. Dafoe, “Institutionalizing ethics in AI through broader impact requirements,” Nature Machine Intelligence, vol. 3, no. 2, pp. 104–110, Feb. 2021, number: 2 Publisher: Nature Publishing Group. [Online]. Available: https://www.nature.com/articles/s4 2256-021-00298-y [222] E. Moss, E. Watkins, R. Singh, M. Elish, and J. Metcalf, “Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” [Online]. Available: https://datasociety.net/library/assembling-accountability-algorithmic-im pact-assessment-for-the-public-interest/ [223] G. Can., “Algorithmic Impact Assessment Tool,” Gov’t Can. Online Resource, Apr. 2021, https://www.canada.ca/en/government/system/digital-government/digital-go vernment-innovations/responsible-use-ai/algorithmic-impact-assessment.html. [224] M. Kop, “AI Impact Assessment & Code of Conduct,” Futurium, May 2019, https: //futurium.ec.europa.eu/en/european-ai-alliance/best-practices/ai-impact-assessm ent-code-conduct. [225] D. Reisman, J. Schultz, K. Crawford, and M. Whittaker, “Algorithmic Impact As- sessments: A Practical Framework For Public Agency Accountability,” AI Now, Apr. 2018, https://ainowinstitute.org/aiareport2018.pdf. [226] A. D. Selbst, “An Institutional View Of Algorithmic Impact Assessments,” Harvard Journal of Law & Technology, vol. 35, no. 1, 2021. [227] E. Moss and J. Metcalf, “Ethics Owners,” Sep. 2020, publisher: Data & Society Research Institute. [Online]. Available: https://datasociety.net/library/ethics-owner s/ [228] K. Crawford, “Artificial Intelligence’s White Guy Problem,” The New York Times, 72/77 p. 2, Jun. 2016. [Online]. Available: https://nyti.ms/28YaKg7 [229] D. Rock and H. Grant, “Why Diverse Teams Are Smarter,” Nov. 2016, https://hbr.or g/2016/11/why-diverse-teams-are-smarter. [230] A. Rosen and I. Ihara, “Giving you more characters to express yourself,” Twitter Blog, Sept. 2017, https://blog.twitter.com/en us/topics/product/2017/Giving-you-m ore-characters-to-express-yourself. [231] W. Knight, “Twitter’s Photo-Cropping Algorithm Favors Young, Thin Females,” Wired, Aug. 2021, https://www.wired.com/story/twitters-photo-cropping-algorith m-favors-young-thin-females. [232] K. Yee and I. F. Peradejordi, “Sharing learnings from the first algorithmic bias bounty challenge,” Twitter Engineering Blog, Sept. 2021, https://blog.twitter.c om/engineering/en us/topics/insights/2021/learnings-from-the-first-algorithmic-bia s-bounty-challenge. [233] C. Herring, “Does diversity pay?: Race, gender, and the business case for diversity,” American Sociological Review, vol. 74, pp. 208 – 224, 2009. [234] N. Ellemers and F. Rink, “Diversity in work groups,” Current opinion in psychology, vol. 11, pp. 49–53, 2016. [235] K. Talke, S. Salomo, and A. Kock, “Top management team diversity and strategic innovation orientation: The relationship and consequences for innovativeness and performance,” Journal of Product Innovation Management, vol. 28, pp. 819–832, 2011. [236] R. Lorenzo and M. Reeves, “How and Where Diversity Drives Financial Perfor- mance,” Harvard Bus. Rev., Jan. 2018, https://hbr.org/2018/01/how-and-where-dive rsity-drives-financial-performance. [237] S. M. West, M. Whittaker, and K. Crawford, “Discriminating Systems: Gender, Race, and Power in AI,” AI Now Institute, Tech. Rep., 2019. [Online]. Available: https://ainowinstitute.org/discriminatingsystems.pdf [238] D. Walsh, “How can human-centered ai fight bias in machines and people?” MIT Sloan Mgmt. Rev., Feb. 2021, https://mitsloan.mit.edu/ideas-made-to-matter/how- can-human-centered-ai-fight-bias-machines-and-people. [239] M. Li, “To Build Less-Biased AI, Hire a More Diverse Team,” Harvard Bus. Rev., Oct. 2020, https://hbr.org/2020/10/to-build-less-biased-ai-hire-a-more-diverse-te am. [240] P. Hall, N. Gill, and B. Cox, Responsible machine learning: Actionable strategies for mitigating risks and driving adoption. Sebastopol, CA: O’Reilly Media Inc., 2020. [241] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru, “Model Cards for Model Reporting,” in Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* ’19. Atlanta, GA, USA: ACM Press, 2019, pp. 220–229. [Online]. Available: http://dl.acm.org/citation.cfm?doid=3287560.3287596 [242] T. Gebru, J. Morgenstern, B. Vecchione, J. Wortman Vaughan, H. Wallach, 73/77 H. Daumee III, and K. Crawford, “Datasheets for Datasets,” arXiv:1803.09010v6 [cs.DB], 2020. [243] D. Broniatowski, “Psychological Foundations of Explainability and Interpretability in Artificial Intelligence,” NIST, Tech. Rep., 2021. [244] S. Gaube, H. Suresh, M. Raue, A. Merritt, S. J. Berkowitz, E. Lermer, J. F. Coughlin, J. V. Guttag, E. Colak, and M. Ghassemi, “Do as AI say: susceptibility in deployment of clinical decision-aids,” npj Digit. Med., vol. 4, no. 1, pp. 1–8, Feb. 2021. [Online]. Available: https://www.nature.com/articles/s41746-021-00385-9 [245] J. Zerilli, A. Knott, J. MacLaurin, and C. Gavaghan, “Algorithmic decision-making and the control problem,” Minds and Machines, vol. 29, pp. 555 – 578, 2019. [246] NIST, “Usability and Biometrics: Ensuring Successful Biometric Systems,” NIST Online Resource, June 2008, https://www.nist.gov/system/files/usability and biome trics final2.pdf. [247] M. Theofanos et al., “Usability Handbook for Public Safety Communications: En- suring Successful Systems for First Responders,” NIST (Handbook 161), May 2017, https://nvlpubs.nist.gov/nistpubs/hb/2017/NIST.HB.161.pdf. [248] ISO, “Ergonomics of human-system interaction — Part 210: Human-centered de- sign for interactive systems,” ISO 9241-210:2019 (2nd ed.), July 2019, https: //www.iso.org/standard/77520.html. [249] E. A. Vogels, “Some digital divides persist between rural, urban and suburban Amer- ica,” Pew Research Center, Aug. 2021, https://www.pewresearch.org/fact-tank/2021 /08/19/some-digital-divides-persist-between-rural-urban-and-suburban-america/. [250] ——, “Digital divide persists even as americans with lower incomes make gains in tech adoption,” Pew Research Center, June 2021, https://www.pewresearch.org/fact -tank/2021/06/22/digital-divide-persists-even-as-americans-with-lower-incomes- make-gains-in-tech-adoption/. [251] X. Ferrer, T. van Nuenen, J. M. Such, M. Cot ́e, and N. Criado, “Bias and Discrimination in AI: a cross-disciplinary perspective,” IEEE Technol. Soc. Mag., vol. 40, no. 2, pp. 72–80, Jun. 2021, arXiv: 2008.07309. [Online]. Available: http://arxiv.org/abs/2008.07309 [252] S. Russell, D. Dewey, and M. Tegmark, “Research priorities for robust and beneficial artificial intelligence,” AI Magazine, vol. 36, no. 4, pp. 105–114, Dec. 2015. [Online]. Available: https://ojs.aaai.org/index.php/aimagazine/article/view/2577 [253] G. Margetis, S. Ntoa, M. Antona, and C. Stephanidis, HUMAN-CENTERED DESIGN OF ARTIFICIAL INTELLIGENCE. John Wiley & Sons, Ltd, 2021, ch. 42, pp. 1085–1106. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/ 10.1002/9781119636113.ch42 [254] B. Shneiderman, Human-Centered AI. London, England: Oxford University Press, Jan. 2022. [255] S. Ejaz, “A Broken System: How the Credit Reporting System Fails Consumers and What To Do About It,” Consumer Reports, June 2021, https://advocacy.consumerr eports.org/wp-content/uploads/2021/06/A-Broken-System-How-the-Credit-Repor 74/77 ting-System-Fails-Consumers-and-What-to-Do-About-It.pdf. [256] S. Ammermann, “Adverse Action Notice Requirements Under the ECOA and the FCRA,” Consumer Compliance Outlook, 2nd Q. 2013, https://consumercompliance outlook.org/2013/second-quarter/adverse-action-notice-requirements-under-ecoa- fcra. [257] A. Smith, “Using Artificial Intelligence and Algorithms,” FTC, Apr. 2020, https: //www.ftc.gov/news-events/blogs/business-blog/2020/04/using-artificial-intelligen ce-algorithms. [258] J. A. Kroll, J. Huey, S. Barocas, E. W. Felten, J. R. Reidenberg, D. G. Robinson, and H. Yu, “Accountable Algorithms,” University of Pennsylvania Law Review, vol. 165, no. 633, p. 74, 2017. [259] GAO, “Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities,” GAO@100 (GAO-21-519SP), June 2021, https://www.gao.go v/assets/gao-21-519sp.pdf. [260] I. D. Raji, A. Smart, R. N. White, M. Mitchell, T. Gebru, B. Hutchinson, J. Smith-Loud, D. Theron, and P. Barnes, “Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing,” in Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, ser. FAT* ’20. New York, NY, USA: Association for Computing Machinery, 2020, p. 33–44. [Online]. Available: https://doi.org/10.1145/3351095.3372873 [261] R. Carrier and S. Brown, “Taxonomy: AI Audit, Assurance, and Assessment,” ForHumanity, Feb. 2021, https://forhumanity.center/web/wp-content/uploads/ 2021/09/ForHumanity.center Taxonomy AI Audit Assurance Assessment.pdf. [262] E. Mulvaney, “NYC Targets Artificial Intelligence Bias in Hiring Under New Law,” Bloomberg Law, 2021, https://news.bloomberglaw.com/daily-labor-report/nyc-tar gets-artificial-intelligence-bias-in-hiring-under-new-law. [263] R. N. Landers and T. S. Behrend, “Auditing the AI auditors: A framework for eval- uating fairness and bias in high stakes ai predictive models,” 2022. [264] D. Sull, S. Turconi, and C. Sull, “When It Comes to Culture, Does Your Company Walk the Talk?” MIT Sloan Mgmt. Rev., July 2020, https://sloanreview.mit.edu/ar ticle/when-it-comes-to-culture-does-your-company-walk-the-talk. [265] C. Johnson, M. Badger, D. Waltermire, J. Snyder, and C. Skorupka, “Guide to cyber threat information sharing,” National Institute of Standards and Technology, NIST Special Publication 800-150, Nov 2016. [Online]. Available: https://doi.org/10.6028/NIST.SP.800-150 [266] S. McGregor, “Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,” arXiv:2011.08512 [cs], Nov. 2020, arXiv: 2011.08512. [Online]. Available: http://arxiv.org/abs/2011.08512 [267] K. Leino, E. Black, M. Fredrikson, S. Sen, and A. Datta, “Feature-Wise Bias Amplification,” arXiv:1812.08999 [cs, stat], Oct. 2019, arXiv: 1812.08999. [Online]. Available: http://arxiv.org/abs/1812.08999 [268] I. Misra, C. L. Zitnick, M. Mitchell, and R. Girshick, “Seeing through the 75/77 Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Las Vegas, NV, USA: IEEE, Jun. 2016, pp. 2930–2939. [Online]. Available: http://ieeexplore.ieee.org/document/7780689/ [269] H. Miller, J. Thebault-Spieker, S. Chang, I. Johnson, L. Terveen, and B. Hecht, “”blissfully happy” or ”ready to fight”: Varying interpretations of emoji,” in Proceedings of the 10th International Conference on Web and Social Media, ICWSM 2016. AAAI press, Jan. 2016, pp. 259–268. [Online]. Available: https://experts.umn.edu/en/publications/blissfully-happy-or-ready-to-fight-varying -interpretations-of-emo [270] P. C. Wason, “Reasoning about a rule,” Quarterly Journal of Experimental Psychology, vol. 20, no. 3, pp. 273–281, 1968. [Online]. Available: https: //doi.org/10.1080/14640746808400161 [271] L. J. Cronbach and P. E. Meehl, “Construct validity in psychological tests.” Psycho- logical bulletin, vol. 52 4, pp. 281–302, 1955. [272] S. Silva and M. Kenney, “Algorithms, platforms, and ethnic bias,” Commun. ACM, vol. 62, no. 11, pp. 37–39, Oct. 2019. [Online]. Available: https: //dl.acm.org/doi/10.1145/3318157 [273] E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Communication- Efficient On-Device Machine Learning: Federated Distillation and Augmentation under Non-IID Private Data,” arXiv:1811.11479 [cs, stat], Nov. 2018, arXiv: 1811.11479. [Online]. Available: http://arxiv.org/abs/1811.11479 [274] Centre for Evidence-Based Medicine, “Catalogue of Bias,” Mar. 2017. [Online]. Available: https://catalogofbias.org/ [275] J. Kruger and D. Dunning, “Unskilled and unaware of it: how difficulties in recog- nizing one’s own incompetence lead to inflated self-assessments.” Journal of per- sonality and social psychology, vol. 77 6, pp. 1121–34, 1999. [276] M. Delgado-Rodriguez, “Bias,” Journal of Epidemiology & Community Health, vol. 58, no. 8, pp. 635–641, Aug. 2004. [Online]. Available: https: //jech.bmj.com/lookup/doi/10.1136/jech.2003.008466 [277] P. M. Lukacs, K. P. Burnham, and D. R. Anderson, “Model selection bias and freed- man’s paradox,” Annals of the Institute of Statistical Mathematics, vol. 62, pp. 117– 125, 2009. [278] K. Lerman and T. Hogg, “Leveraging Position Bias to Improve Peer Recommendation,” PLOS ONE, vol. 9, no. 6, p. e98914, Jun. 2014, publisher: Public Library of Science. [Online]. Available: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0098914 [279] A. Kaplan, The conduct of inquiry, A. Kaplan, Ed. Somerset, NJ: Transaction, Apr. 1998. [280] Z. Tufekci, “Big questions for social media big data: Representativeness, validity and other methodological pitfalls,” arXiv:1403.7400 [cs.SI], 2014. [281] B. Goodman and S. Flaxman, “European Union regulations on algorithmic 76/77 decision-making and a ”right to explanation”,” AIMag, vol. 38, no. 3, pp. 50–57, Oct. 2017, arXiv: 1606.08813. [Online]. Available: http://arxiv.org/abs/1606.08813 77/77|1|1|1|1|1
