{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6561740d",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751e7bdf",
   "metadata": {},
   "source": [
    "Set paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "214fea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Folder containing PDFs\n",
    "PDF_FOLDER = \"/Users/winnie/Documents/GitHub/MedAI/Analysis&NLP/focus_area_classification/test_pdf_folder\"\n",
    "LABEL_CSV = \"/Users/winnie/Documents/GitHub/MedAI/Analysis&NLP/focus_area_classification/labeled_data.csv\"\n",
    "TEXT_FOLDER = \"/Users/winnie/Documents/GitHub/MedAI/Analysis&NLP/focus_area_classification/clean_texts\"\n",
    "\n",
    "# Ensure output text folder exists\n",
    "os.makedirs(TEXT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Your 5 AI governance focus areas (labels)\n",
    "LABELS = [\n",
    "    \"bias_and_fairness\",\n",
    "    \"reliability_and_monitoring\",\n",
    "    \"privacy_and_security\",\n",
    "    \"transparency_and_explainability\",\n",
    "    \"responsible_implementation\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002240d2",
   "metadata": {},
   "source": [
    "Extract text from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e136b042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/winnie/Documents/GitHub/MedAI/Analysis&NLP/focus_area_classification/labeled_data.csv already exists. Proceeding without changes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "# Step 1: Extract and clean text from PDFs, save as .txt\n",
    "def clean_text(text):\n",
    "    # Normalize Unicode to remove ambiguous characters\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    # Remove pipes and carriage returns/line feeds\n",
    "    text = re.sub(r'[|]', '', text)\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_and_save_clean_txts(pdf_folder, text_folder):\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            doc = fitz.open(pdf_path)\n",
    "            raw_text = \"\\n\".join(page.get_text() for page in doc)\n",
    "            cleaned = clean_text(raw_text)\n",
    "\n",
    "            base_name = os.path.splitext(filename)[0]\n",
    "            txt_path = os.path.join(text_folder, base_name + \".txt\")\n",
    "\n",
    "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(cleaned)\n",
    "\n",
    "# Step 2: Generate labeled CSV using .txt files\n",
    "def create_labeled_csv_from_txts(text_folder, label_csv, labels):\n",
    "    data = []\n",
    "    for filename in os.listdir(text_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            txt_path = os.path.join(text_folder, filename)\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            entry = {\"filename\": filename, \"text\": text}\n",
    "            for label in labels:\n",
    "                entry[label] = 0\n",
    "            data.append(entry)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(label_csv, index=False, encoding=\"utf-8\", sep='|')  # Use pipe as delimiter\n",
    "    print(f\"Labeled CSV saved to {label_csv} with '|' as the delimiter.\")\n",
    "\n",
    "# ---- Run the full workflow ---- #\n",
    "LABELS = [\n",
    "    \"bias_and_fairness\",\n",
    "    \"reliability_and_monitoring\",\n",
    "    \"privacy_and_security\",\n",
    "    \"transparency_and_explainability\",\n",
    "    \"responsible_implementation\"\n",
    "]\n",
    "\n",
    "if not os.path.exists(LABEL_CSV):\n",
    "    extract_and_save_clean_txts(PDF_FOLDER, TEXT_FOLDER)\n",
    "    create_labeled_csv_from_txts(TEXT_FOLDER, LABEL_CSV, LABELS)\n",
    "else:\n",
    "    print(f\"{LABEL_CSV} already exists. Proceeding without changes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818fbc28",
   "metadata": {},
   "source": [
    "Load and prepare the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1312ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Increase allowable field size\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "# Load pipe-delimited CSV with large text fields\n",
    "df = pd.read_csv(\"labeled_data.csv\", sep='|', encoding='utf-8', engine='python')\n",
    "\n",
    "# Fill NaNs in text column\n",
    "df['text'] = df['text'].fillna(\"\")\n",
    "\n",
    "# Separate features and multi-label targets\n",
    "X_text = df['text']\n",
    "Y = df[LABELS].values\n",
    "\n",
    "### somewhere in here, add code to automate tagging of 1s and 0s (one-hot encoding?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b027753",
   "metadata": {},
   "source": [
    "Vectorize using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3af6717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(X_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4225bf",
   "metadata": {},
   "source": [
    "Split into training and testing groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab6bf257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split both features and labels\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e794663",
   "metadata": {},
   "source": [
    "View class balance/distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42a9fae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 'bias_and_fairness' class distribution:\n",
      "1    3\n",
      "0    2\n",
      "dtype: int64\n",
      "\n",
      "Label 'reliability_and_monitoring' class distribution:\n",
      "1    4\n",
      "0    1\n",
      "dtype: int64\n",
      "\n",
      "Label 'privacy_and_security' class distribution:\n",
      "1    3\n",
      "0    2\n",
      "dtype: int64\n",
      "\n",
      "Label 'transparency_and_explainability' class distribution:\n",
      "1    4\n",
      "0    1\n",
      "dtype: int64\n",
      "\n",
      "Label 'responsible_implementation' class distribution:\n",
      "1    4\n",
      "0    1\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(LABELS):\n",
    "    counts = pd.Series(Y_train[:, i]).value_counts()\n",
    "    print(f\"Label '{label}' class distribution:\\n{counts}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4288e3cd",
   "metadata": {},
   "source": [
    "Train XGBoost classifers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "978dc698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winnie/anaconda3/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:29:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "models = []\n",
    "Y_pred = []\n",
    "\n",
    "for i, label in enumerate(LABELS):\n",
    "    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "    model.fit(X_train, Y_train[:, i])\n",
    "    preds = model.predict(X_test)\n",
    "    Y_pred.append(preds)\n",
    "    models.append(model)\n",
    "\n",
    "Y_pred = np.array(Y_pred).T  # Shape: (samples, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e9660",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69c607c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "                                 precision    recall  f1-score   support\n",
      "\n",
      "              bias_and_fairness       1.00      1.00      1.00         2\n",
      "     reliability_and_monitoring       0.50      1.00      0.67         1\n",
      "           privacy_and_security       0.00      0.00      0.00         0\n",
      "transparency_and_explainability       0.50      1.00      0.67         1\n",
      "     responsible_implementation       1.00      1.00      1.00         2\n",
      "\n",
      "                      micro avg       0.60      1.00      0.75         6\n",
      "                      macro avg       0.60      0.80      0.67         6\n",
      "                   weighted avg       0.83      1.00      0.89         6\n",
      "                    samples avg       0.60      1.00      0.75         6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winnie/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(Y_test, Y_pred, target_names=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf23271e",
   "metadata": {},
   "source": [
    "#### Using the trained model to predict focus area classification for unlabeled documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc3f4f4",
   "metadata": {},
   "source": [
    "Clean the unlabeled documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "991ad522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 1. Load your saved TF-IDF vectorizer and trained models\n",
    "# (Assumes you've stored them in variables `vectorizer` and `models`)\n",
    "# If saved to disk, you'd need to use `joblib.load(...)`\n",
    "\n",
    "# 2. Define the same cleaning function\n",
    "def clean_text(text):\n",
    "    import unicodedata\n",
    "    import re\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = re.sub(r'[|]', '', text)\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83db7f17",
   "metadata": {},
   "source": [
    "Extract and clean text from unlabeled documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Extract and clean text from new PDFs\n",
    "def extract_texts_from_folder(folder_path):\n",
    "    docs = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            path = os.path.join(folder_path, filename)\n",
    "            doc = fitz.open(path)\n",
    "            raw_text = \"\\n\".join(page.get_text() for page in doc)\n",
    "            cleaned = clean_text(raw_text)\n",
    "            docs.append(cleaned)\n",
    "            filenames.append(filename)\n",
    "    return filenames, docs\n",
    "\n",
    "unlabeled_folder = \"unlabeled_pdfs\"\n",
    "filenames, texts = extract_texts_from_folder(unlabeled_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3927e",
   "metadata": {},
   "source": [
    "Vectorize the text from unlabeled documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Vectorize new text using the *trained* vectorizer\n",
    "X_unlabeled = vectorizer.transform(texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4698a0",
   "metadata": {},
   "source": [
    "Predict using the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675ce96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Predict using your trained models (one per label)\n",
    "predictions = []\n",
    "for model in models:\n",
    "    preds = model.predict(X_unlabeled)\n",
    "    predictions.append(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc543d",
   "metadata": {},
   "source": [
    "Display predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c664c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'predicted_labels.csv'\n"
     ]
    }
   ],
   "source": [
    "# 6. Combine into a labeled DataFrame\n",
    "LABELS = [\n",
    "    \"bias_and_fairness\",\n",
    "    \"reliability_and_monitoring\",\n",
    "    \"privacy_and_security\",\n",
    "    \"transparency_and_explainability\",\n",
    "    \"responsible_implementation\"\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "predictions = np.array(predictions).T  # shape: (n_samples, n_labels)\n",
    "\n",
    "df_preds = pd.DataFrame(predictions, columns=LABELS)\n",
    "df_preds.insert(0, 'filename', filenames)\n",
    "df_preds.insert(1, 'text', texts)\n",
    "\n",
    "# 7. Save to CSV for review\n",
    "df_preds.to_csv(\"predicted_labels.csv\", index=False, encoding='utf-8', sep='|')\n",
    "print(\"Predictions saved to 'predicted_labels.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
