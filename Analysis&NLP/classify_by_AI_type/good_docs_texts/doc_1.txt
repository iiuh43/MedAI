© 2024 American Medical Association.  All rights r eserved . CLRPD Report 2 -I-23, Generative AI in Medicine and Health Care  
 
EXECUTIVE SUMMARY  
 
This report provides information on the fundamentals of generative AI in medicine and health care: 
terminologies and components of artificial intelligence (AI) and augmented intelligence, definitions, prominent models (Open AI ChatGPT, Google Bard and Med-PaLM, and Microsoft 
Bing), promises, challenges, and pitfalls, AMA partnerships and resources, and potential ethical 
and regulatory frameworks . The report concludes with insight from CLRPD members on the trend.  
 
Generative AI models are commercial natural language processing tools known as large language 
models (LLMs). At their core, all AI innovations utilize sophisticated statistical techniques to 
discern patterns within extensive datasets using increasingly powerful computational technologies. 
Three components —big data, advanced statistical methods, and computing resources—have not 
only become available recently but are also being democratized and made accessible to at a pace 
unprecedented in previous technologi cal innovations.   
 While LLMs show promise to make a significant contribution to health care in the future, 
physicians currently considering using generative AI models in a clinical setting or direct patient 
care should exercise caution and be aware of the real challenges th at remain to ensure reliability: 
confident responses that are not justified by the model’s training data, the “black box” nature of AI, 
biased and discriminatory tendencies in outputs, lack of knowledge -based reasoning, lack of 
current ethical and regulato ry frameworks, patient privacy and security concerns, and potential 
liability.  
 
Generative AI systems are not sentient, they simply use massive amounts of text to predict one word after another, and their outputs may mix truth with patently false statements. As such, 
physicians will need to learn how to integrate these tools into clin ical practice, defining clear 
boundaries between full, supervised, and proscribed autonomy. Physicians should be clear -eyed 
about the risks inherent to any new technology, especially ones that carry existential implications, while cautiously optimistic abo ut a future of improved health care system efficiency, better patient 
outcomes, and reduced burnout. Extant AI -assistant programs and rapidly developing systems are 
incredibly sophisticated, and as physicians have already begun to demonstrate on social med ia, 
they might soon be able to reliably perform test result notifications, work letters, prior authorizations, and the like —the mundane necessities that not only cumulatively consume valuable 
time but are substantial contributor s to physician burnout.  
 
Projecting further into an AI -enhanced future, imagine that instead of writing follow -up care 
instructions, physicians could ask a generative AI system to create a synopsis of the patient’s treatment  course. With the time saved, physicians could step away from the computer, face the 
patient, and explain the most salient follow -up items, prepped with materials that are compatible 
with best practices in health literacy. Likewise, these programs might help actualize the admirable intentions behind the provisions in the 21st Century Cures Act that have given patients access, but 
not accessibility, to their jargon -laden electronic medical records.  
 
Given  opportunities to offer clinical insight into the development and deployment of these systems, 
Generative AI may provide physicians with technological tools that reduce administrative burden 
and enable them to get back to the reason why they decided to pursue medicine in the first place—
to improve patients’ lives —meanwhile, improving physicians’ wellbeing.
© 2024 American Medical Association.  All rights r eserved . REPORT OF THE COUNCIL ON LONG RANGE PLANNING AND DEVELOPMENT  
 
 
CLRPD Report 2 -I-23  
 
 
Subject:  Generative AI in Medicine and Health Care  
 
Presented by:   
Gary Thal, MD, Chair  
 
 
BACKGROUND  1 
 2 
The functions of the Council on Long Range Planning and Development (CLRPD) include to study 3 
and make recommendations concerning the long-range objectives of the American Medical 4 
Association (AMA), and to serve in an advisory role to the Board of Trustees concerning strategies 5 
by which the AMA attempts to reach its long -range objectives. To accomplish its role, the Council 6 
studies anticipated changes in the environment in which medicine and the AMA must function and 7 
develops memos to the Board, which include CLRPD deliberations and insight on emerging issues, 8 
such as generative artificial intelligence (AI).  9 
 10 
This informational report  presents material  on the fundamentals of generative AI in medicine and 11 
health care including terminologies and components, definition, prominent models, promises and 12 
pitfalls, AMA partnerships and resources, potential ethical and regulatory frameworks, and CLRPD 13 
insight.  14 
 15 
TERMINOLIGIES AND COMPONENTS OF AI  16 
 17 
CLRPD Report 1 -A-18, A Primer on Artificial and Augmented Intelligence1 defines the relative 18 
terminologies of artificial intelligence (AI), which are not well understood:  19 
 20 
• Algorithms are a sequence of instructions used to solve a problem. Developed by 21 
programmers to instruct computers in new tasks, algorithms are the building blocks of the 22 
advanced digital world. Computer algorithms organize enormous amounts of data into 23 
information and services, based on certain instructions and rules.  24 
 25 
• Artificial Intelligence  is the ability of a computer to complete tasks in a manner typically 26 
associated with a rational human being —a quality that enables an entity to function 27 
appropriately and with foresight in its environment. True AI is widely regarded as a 28 
program or algorithm that can beat the Turing Test, which states that an artificial 29 
intelligence must be able to exhibit intelligent behavior that is indistinguishable from that 30 
of a human.  31 
 32 
• Augmented Intelligence is an alternative conceptualization that focuses on AI’s assistive 33 
role, emphasizing the fact that its design enhances human intelligence rather than replaces 34 
it.  35 
 36 
• Machine Learning is a part of the discipline of artificial intelligence and refers to 37 
constructing algorithms that can make accurate predictions about future outcomes. 38 
Machine learning can be supervised or unsupervised.  39 
 CLRPD Rep. 2 -I-23 -- page 2 of 18 
 
o In supervised learning, algorithms are presented with “training data” that contain 1 
examples with their desired conclusions, such as pathology slides that contain 2 
cancerous cells as well as slides that do not.  3 
o Unsupervised learning does not typically leverage labeled training data. Instead, 4 
algorithms are tasked with identifying patterns in data sets on their own by 5 
defining signals and potential abnormalities based on the frequency or clustering of 6 
certain data.  7 
 8 
• Deep Learning  is a subset of machine learning that employs artificial neural networks 9 
(ANNs) and algorithms structured to mimic biological brains with neurons and synapses. 10 
ANNs are often constructed in layers, each of which performs a slightly different function 11 
that contributes to the result. Deep learning is the study of how these layers interact and the 12 
practice of applying these principles to data.   13 
 14 
• Cognitive Computing, a term coined by IBM, is often used interchangeably with machine 15 
learning and artificial intelligence. However, cognitive computing systems do not 16 
necessarily aspire to imitate intelligent human behavior, but instead to supplement human 17 
decision -making power by identifying potentially useful insights with a high degree of 18 
certainty. Clinical decision support and augmented intelligence come to mind when 19 
considering this definition.  20 
  21 
• Natural Language Processing (NLP) forms the foundation for many cognitive computing 22 
exercises. The ingestion of source materials, such as medical literature, clinical notes, or 23 
audio dictation records requires a computer to understand what is written, spoken, or 24 
otherwise being communicated. One commonly used application of NLP is optical 25 
character recognition (OCR) technology that can turn static text, such as a PDF of a lab 26 
report or a scan of a handwritten clinical note, into machine readable data. Once data is in a 27 
workable format, the algorithm parses the meaning of each element to complete a task such 28 
as translating into a different language, querying a database, summarizing information, or 29 
supplying a response to a conversation partner. In the health care field, where acronyms 30 
and abbreviations are common, accurately parsing through this “incomplete” data can be 31 
challenging.  32 
 33 
DEFINTION OF GENERATIVE AI  34 
 35 
Generative AI is a broad term used to describe any type of artificial intelligence that can be used to 36 
create new text, images, video, audio, code, or synthetic data. Progress with generative AI was 37 
relatively slow until around 2012,  when a single idea shifted the entire field. It was called a neural 38 
network —inspired by the inner workings of the human brain—a mathematical system that learns 39 
skills by finding statistical patterns in enormous amounts of data. By analyzing thousands of cat 40 
photos, for instance, it can learn to recognize a cat. Neural networks enable Siri and Alexa to 41 
understand what you are saying, identify people and objects in Google Photos and instantly 42 
translate dozens of languages.243 
 44 
The next big change was large language models (LLMs), which consist of a neural network. 45 
Around 2018, companies like Google, Microsoft , and OpenAI began building neural networks that 46 
were trained on vast amounts of text from the internet, including Wikipedia articles, digital books, 47 
and academic papers. Somewhat to the experts’ surprise, these systems learned to write unique 48 
prose and computer code and carry -on sophisticated conversations, which is termed  generative AI.3 49 
 
 CLRPD Rep. 2 -I-23 -- page 3 of 18 
 
LLMs are a class of technologies that drive generative AI systems. The first LLMs appeared about 1 
five years ago, but were not very sophisticated; however, today they can  draft emails, 2 
presentations, and memos. Every AI system needs a goal. Researchers call this an  objective 3 
function. It can be simple, such as “win as many chess games as possible” or complicated, such as 4 
“predict the three-dimensional shapes of proteins, using only their amino acid sequences.”4 Most 5 
LLMs have the same basic objective function, which is, given a sequence of text, to guess what 6 
comes next.  Though trained on simple tasks along the lines of predicting the next word in a 7 
sentence, neural language models with sufficient training and parameter counts are found to 8 
capture much of the syntax and semantics of human language. In addition, LLMs demonstrate 9 
considerable general knowledge about the world and can memorize a great quantity of facts during 10 
training.  11 
 12 
Training the model involves feeding algorithms large amounts of data, which serves as the 13 
foundation for the AI model to learn from. This can consist of text, code, graphics, or any other 14 
types of content relevant to the task at hand. Once the training data ha s been collected, the AI 15 
model analyzes the patterns and relationships within the data to understand the underlying rules 16 
governing the content. Continuously, the AI model fine -tunes its parameters as it learns, improving 17 
its ability to simulate human -generated content. The more content the AI model generates, the 18 
more sophisticated and convincing its outputs become.5 19 
 20 
Typing in the precise words and framing to generate the most helpful answers is an art. Beginning 21 
a prompt with “act as i f” will instruct the model to emulate an expert. For example, typing “Act as 22 
if you are a tutor for the SATs” or “Act as if you are a personal trainer” will guide the systems to 23 
model themselves around people in those professions. These prompts provide additional context for 24 
the generative AI model to  produce  its response by helping the tool to draw on specific statistical 25 
patterns in its training data.6 26 
 27 
Generative AI outputs are calibrated combinations of the data used to train the algorithms. Because 28 
the amount of data used to train these algorithms is so incredibly massive —multiple terabytes of 29 
text data —the models can appear to be “creative” when producing outputs. Moreover, the models 30 
usually have random elements, which means they can produce a variety of outputs from one input 31 
request —making them seem even more lifelike. The unmanageably huge volume and complexity 32 
of data (unmanageable by humans, anyway) that is now being generated has increased the potential 33 
of the technologies.7 34 
 35 
Tech companies are confronting a challenge: how to balance asking users for more data to  deliver 36 
new AI features without scaring away privacy -conscious businesses and  consumers that 37 
consistently tell pollsters they want transparency about when  AI is used and trained. But when 38 
companies provide such detail, it  is often written in legalese and buried in fine print  that is often 39 
being rewritten to give tech companies more rights.  Video conferencing company Zoom 40 
encountered a  massive backlash over concerns the contents of video chat might be used to train AI 41 
systems. The move prompted  an apologetic post from Zoom ’s CEO, but the company is far from 42 
alone in seeking more consumer data to train AI models.  Companies are deploying different 43 
approaches to ensure they have access to user data. At the same time, many are also adding in 44 
language to prevent anyone else from scraping their websites to train AI systems.8 45 
 46 
According to the JAMA Forum article, “ChatGPT and Physicians’ Malpractice Risk,”9 most LLMs 47 
are trained on indiscriminate assemblages of web text with little regard to how sources vary in 48 
reliability. They treat articles published in the New England Journal of Medicine  and Reddit 49 
discussions as equally authoritative. In contrast, Google searches let physicians distinguish expert 50 
from inexpert summaries of knowledge and selectively rely on the best. Other decision -support 51 
 CLRPD Rep. 2 -I-23 -- page 4 of 18 
 
tools provide digests based on the best available evidence. Although efforts are underway10 to train 1 
LLMs on exclusively authoritative, medically relevant texts, they are still nascent and prior efforts 2 
have  faltered.11 3 
 4 
Generative AI models have been observed to experience confabulations or delusions — confident 5 
responses by an AI model that does not seem to be justified by its  training data. Such phenomena 6 
are termed  by the tech industry as “hallucinations,” in loose analogy with the phenomenon 7 
of hallucination in human psychology; however, one key difference is that human hallucinations 8 
are usually associated with false percepts, while an AI hallucination is associated with the category 9 
of unjustified responses or beliefs .12 10 
 11 
GENERATIVE AI MODELS   12 
 13 
There are several types of generative AI models, each designed to address specific challenges and 14 
applications. These generative AI models can be broadly categorized into the following types:13 15 
 16 
• Transformer -based models : These models, such as OpenAI’s ChatGPT and GPT -3.5, are 17 
neural networks designed for natural language processing. They are trained on large 18 
amounts of data to learn the relationships between sequential data — like words and 19 
sentences — making them useful for text -generation tasks.  20 
 21 
• Generative adversarial networks (GANs):  GANs are made up of two neural networks, a 22 
generator, and a discriminator that work in a competitive or adversarial capacity. The 23 
generator creates data, while the discriminator evaluates the quality and authenticity of said 24 
data. Over time, both networks get better at their roles, leading to more realistic outputs.  25 
 26 
• Variational autoencoders (VAEs): VAEs use an encoder and a decoder to generate content. 27 
The encoder takes the input data, such as images or text, and simplifies it into a more 28 
compact form. The decoder takes this encoded data and restructures it into something new 29 
that resembles the original input.  30 
 31 
• Multimodal models : Multimodal models can process multiple types of input data, 32 
including text, audio, and images. They combine different modalities to create more 33 
sophisticated outputs, such as DALL -E 214 and OpenAI’s GPT -415, which is also capable 34 
of accepting image and text inputs.  35 
 36 
OpenAI ChatGPT  37 
 38 
Researchers have been working on generative AI for a long time. OpenAI, developer of ChatGPT 39 
(Generative Pretrained Transformer), is over seven years old. Launched in November 2022, 40 
ChatGPT is a LLM that leverages huge amounts of data to mimic human conversation and assess 41 
language patterns. Currently, the basic system is free via a simple web interface that lets users pose 42 
questions and give directions to a bot that can answer with conversation, term papers, sonnets, 43 
recipes— almost anything.16 44 
 45 
GPT -4 is the newest version of OpenAI’s language model systems, and it is much more advanced 46 
than its predecessor GPT -3.5, which ChatGPT runs on. GPT-4 is a multimodal model that accepts 47 
both text and images as input and output text. This can be useful for uploading worksheets, graphs, 48 
and charts to be analyzed.  GPT -4 has advanced intellectual capabilities that allow it to outperform 49 
 CLRPD Rep. 2 -I-23 -- page 5 of 18 
 
GPT -3.5 in a series of simulated benchmark exams. It has also reduced the number of 1 
“hallucinations” produced by the chatbot.17 2 
 3 
ChatGPT has passed a series of benchmark exams. Christian Terwiesch, a professor at Wharton, 4 
the University of Pennsylvania’s business school, used ChatGPT to take an MBA exam.  ChatGPT 5 
not only passed the exam but also scored a B to B -. The professor was impressed at its basic 6 
operations management, process analysis questions, and explanations. Although ChatGPT could 7 
pass many of these benchmark exams, its scores were usually in the lower percentile. However, 8 
with GPT-4, scores were much higher. For example, ChatGPT in the 3.5 series scored in the lower 9 
10th percentile of a simulated Bar Exam, while GPT -4 scored in the top 10th percentile.18 10 
 11 
Google Bard and Med-PaLM  12 
 13 
Bard is Google’s AI chat service, a rival to ChatGPT.19 On February 6, 2023, Google introduced its 14 
experimental AI chat service. Over a month after the announcement, Google began rolling 15 
out access to Bard via a waitlist. Bard uses a lightweight version of Google’s Language Model for 16 
Dialogue Applications (LaMDA)20 and draws on all the information from the web to respond -- a 17 
stark contrast from ChatGPT, which does not have internet access.  Google's chat service had a 18 
rough launch, with a demo of Bard delivering inaccurate information about the James Webb Space 19 
Telescope.21 ChatGPT’s advanced capabilities exceed those of Google Bard. Even though Google 20 
Bard has access to the internet and ChatGPT does not, it fails to produce answers much more often 21 
than ChatGPT.  22 
 23 
In April 2023, Google announced a new version of its medical LLM, called Med -PaLM 2.22 An AI 24 
platform for analyzing medical data, it aims to assist physicians with routine tasks and provide 25 
more reliable answers to patient questions than “Dr. Google.” PaLM 2, the Pathways Language 26 
Model , is more critical than Bard for medicine. With 540 billion parameters, it draws knowledge 27 
from scientific papers and websites, can reason logically, and perform complex mathematical 28 
calculations.23 Google is actively developing its large language model (LLM), Med-PaLM 2, which 29 
they anticipate will excel at healthcare discussions over general -purpose algorithms, given its 30 
training on questions and answers from medical licensing exams. They are collaborating with 31 
Mayo Clinic and other health systems and partnering with the healthcare technology vendor, 32 
CareCloud.24 33 
 34 
Microsoft Bing AI  35 
 36 
In early February 2023,  Microsoft unveiled25 a new version of Bing26 -- and its standout feature is 37 
its integration with GPT-4. When it was announced, Microsoft shared that Bing Chat  was powered 38 
by a next -generation version of OpenAI’s large language model, making it “more powerful than 39 
ChatGPT.”27 40 
 41 
Five weeks after launch, Microsoft revealed that, since its launch, Bing Chat had been running on 42 
GPT -4, the most advanced Open AI model, before the model even launched.  Because Bing’s 43 
ChatGPT is linked to the internet, the biggest difference from ChatGPT is that Bing’s version has 44 
information on current events, while ChatGPT is limited to knowledge before 2021.  Another major 45 
advantage of the new Bing is that it links to the sites it sourced its information from using 46 
footnotes, whereas ChatGPT does not.  47 
 48 
Building a generative AI model has for the most part been a major undertaking, to the extent that 49 
only a few well -resourced tech heavyweights have tried. OpenAI, the company behind ChatGPT, 50 
former GPT models, and DALL -E (a tool for AI-generated art), has billions in funding from high- 51 
 CLRPD Rep. 2 -I-23 -- page 6 of 18 
 
profile donors. DeepMind is a subsidiary of Alphabet, the parent company of Google, and Meta has 1 
released its Make-A -Video product based on generative AI. These companies employ some of the 2 
world’s best computer scientists and engineers. However, when you are asking a model to train 3 
using nearly the entire internet, it is going to be costly. OpenAI has not released exact costs, but 4 
estimates indicate that GPT -3 was trained on a vast amount of text data that was equivalent to one 5 
million feet of bookshelf space, or a quarter of the entire Library of Congress at an estimated cost 6 
of several million dollars. These are not resources that your garden -variety start -up can access.28 7 
 8 
PROMISES AND PITFALLS 9 
 10 
The latest McKinsey Global Survey breaks down how corporate leaders worldwide are using 11 
generative AI. By interviewing thousands of managers and executives across the globe, McKinsey 12 
gained a high -level  view on where AI is being deployed already (especially in marketing, product 13 
development, and service operations), as well as the biggest perceived risks of implementing AI 14 
(including inaccurate outputs, cybersecurity threats, and intellectual property infringement).29 In 15 
June, McKinsey  projected  that generative AI could add $4.4 trillion to global GDP, 75% of which 16 
would emerge from use cases in customer operations, marketing and sales, software engineering, 17 
and R&D.30 18 
 19 
In the medical device industry, product developers are integrating AI capabilities into a wide 20 
variety of health care technologies, from imaging and surgical systems to vital sign monitors, 21 
endoscopes, and diagnostic devices. New players range from Big Tech behemoths to 22 
entrepreneurial startups to the individual visionaries who, in the digital age, create algorithms that 23 
could lead to the next breakthrough technology.  24 
 25 
AMA surveys of physicians conducted in 2016, 2019, and 2022 show growing use of and plans to 26 
use AI in the short term. In the latest survey, nearly one in five physicians say their practice 27 
incorporates AI for practice efficiencies and clinical applications, while just over one in 10 use 28 
biometrics, precision and personalized medicine, or digital therapeutics. More than twice as many 29 
expect to adopt such advanced technologies within one year. However, unlike other health care 30 
technologies, AI -enabled medical devices can perform in mysterious and unexpected ways— 31 
introducing a whole new set of uncertainties. This so -called “black box conundrum” —knowing 32 
what goes in and what comes out of the system, but not what happens in between —can be 33 
disconcerting.31 34 
 35 
In 2021, two experts explained the fundamentals of machine learning, what it means in the clinical 36 
setting and the possible risks of using the technology, “Machine Learning: An Introduction and 37 
Discussion of Medical Applications”  that took place during the June 2021 AMA Sections Meetings 38 
and was hosted by AMA Medical Student Section:32 39 
 40 
• A key aspect of machine learning is that it continuously improves the model by weighing 41 
the data with minimal human interaction, explained Herbert Chase, MD, MA, professor of 42 
clinical medicine in biomedical informatics at Vagelos College of Physicians and Surgeons 43 
at Columbia University. It may be able to pick up factors leading to disease that a 44 
physician does not. For example, people who all worked in a factory that had heavy metals 45 
in the atmosphere or people in the same zip code are experiencing the same thing. People 46 
with a certain disease are taking the same vitamins or they all had a previous surgery. “The 47 
EHR has hundreds of different attributes, thousands of different values that can be mined. 48 
This is classic data mining in an unsupervised way to make the prediction model better and 49 
there are many examples in the literature now of how this approach has dramatically 50 
 CLRPD Rep. 2 -I-23 -- page 7 of 18 
 
improved the prediction for coronary artery disease, heart failure and many other chronic 1 
conditions,” Dr. Chase said.  2 
 3 
• While machine learning can help medicine in tremendous ways, physicians must also be 4 
mindful that bias in machine learning is a problem, Ravi Parikh, MD, MPP, assistant 5 
professor of medical ethics and health policy and medicine at the University of 6 
Pennsylvania, explained during the educational session. There are three distinct things you 7 
need to specify for a supervised machine- learning algorithm. You start with a population. 8 
A series of variables is derived from the population. Those variables are then used for a 9 
predictive algorithm to predict an outcome.  10 
 11 
• “Any amount of those three steps could be biased and could generate bias in the context of 12 
the algorithm,” Dr. Parikh said. So, how can bias be addressed? Dr. Parikh said physicians 13 
can identify bias and potentially flawed decision making in real time, use unbiased data 14 
sources and track algorithm outputs continuously to monitor bias.  15 
 16 
• Drs. Parikh and Chase said physicians do not  need to worry about machine learning 17 
eliminating physicians’ jobs. “The workforce will just be the same as it always has been … 18 
but you will be operating at a higher level and I think that will make the profession to some 19 
extent more interesting,” Dr. Chase said.  20 
 21 
Augmented intelligence promises to be a transformational force in health care, especially within 22 
primary care. Experts outline ways that innovations driven by this technology can aid rather than 23 
subvert the patient -physician relationship. Steven Y. Lin, MD, and Megan R. Mahoney, MD, 24 
associate clinical professor of medicine and clinical professor of medicine, respectively, in the 25 
Division of Primary Care and Population Health at Stanford University School of Medicine, and 26 
AMA vice president of professional satisfaction Christine A. Sinsky, MD—reviewed promising 27 
inventions in 10 distinct problem areas:33 28 
 29 
• Risk prediction and intervention:  Drawing on EHR data, AI -driven predictive modeling 30 
can outperform traditional predictive models in forecasting in-hospital mortality, 30 -day 31 
unplanned readmission, prolonged length of stay and final discharge diagnoses.  32 
 33 
• Population health management : With the move from fee -for-service to value-based 34 
payments, AI could help identify and close care gaps and optimize performance with 35 
Medicare quality payment programs.  36 
 37 
• Medical advice and triage:  Some companies have developed “AI doctors” to provide health 38 
advice to patients with common symptoms, freeing up primary care appointments for 39 
patients requiring more complex care. “Rather than replacing physicians for some 40 
conditions, AI support can be integrated into team -based care models that make it easier 41 
for primary care physicians to manage a patient panel,” the authors wrote. Risk -adjusted 42 
paneling and resourcing EHR data on utilization can be used to create algorithms for 43 
weighing panel sizes in primary care. This can be used to determine the level of staffing 44 
support needed for primary care practices based on the complexity and intensity of care 45 
provided.  46 
 47 
• Device integration : Wearable devices can track vital signs and other health measures, but 48 
their data’s volume and its incompatibility with EHRs make it unwieldy without the help 49 
 CLRPD Rep. 2 -I-23 -- page 8 of 18 
 
of AI. Apple’s Health Kit is a tool that integrates data from multiple wearable devices into 1 
the EHR, enabling care teams to map trends and spot deviations that suggest illness.  2 
 3 
• Digital health coaching : Companies are now offering digital health coaching for diabetes, 4 
hypertension and obesity, and similar programs integrated in health systems have shown 5 
reductions in cost per patient through reduced office and hospital visits.  6 
 7 
• Chart review and documentation:  Technology companies with expertise in automatic 8 
speech recognition are teaming up with health systems to develop AI -driven digital scribes 9 
that can listen in on patient- physician conversations and automatically generate clinical 10 
notes in the EHR.  11 
 
• Diagnostics:  AI-powered algorithms for diagnosing disease “are now outperforming 12 
physicians in detecting skin cancer, breast cancer, colorectal cancer, brain cancer and 13 
cardiac arrhythmias,” the authors wrote, citing numerous tools, such as IDx-DR, Aysa, and 14 
Tencent. “This could reduce the need for unnecessary referrals, increase continuity with 15 
patients and enhance mastery for primary care physicians.”34 16 
• Clinical decision -making:  Next generation platforms do much more than provide alerts and 17 
best practice advisories. eClinicalWorks, for example, is developing a new version of its 18 
EHR that will feature an AI assistant that provides evidence-based clinical suggestions in 19 
real time.  20 
 21 
• Practice management : AI can also automate repetitive clerical tasks. Eligibility checks, 22 
insurance claims, prior authorizations, appointment reminders, billing, data reporting and 23 
analytics can all now be automated using AI, and some companies have developed AI - 24 
powered category auditors to help optimize coding for quality payment programs.  25 
 26 
AMA partners with technology and health care leaders to bring physicians critical insights on AI’s 27 
potential applications and ensure that physicians have a voice in shaping AI’s role in medicine.  28 
 29 
• Health2047, the innovation subsidiary of the American Medical Association (AMA), has 30 
launched a startup that develops augmented intelligence technologies to support clinical 31 
decision making.35 Called RecoverX, the startup creates technologies that leverage 32 
research, medical charts, patient conversations, and test results to provide evidence-based 33 
clinical insights and suggested actions for clinicians in real time. For example, one of the 34 
technologies on the core RecoverX platform, called Diagnostic Glass, provides decision - 35 
making support to clinicians in more than 30 specialties.36 36 
 37 
• To develop actionable guidance for trustworthy AI in health care, the AMA reviewed 38 
literature on the challenges health care AI poses and reflected on existing guidance. These 39 
findings are published in a paper in Journal of Medical Systems:  Trustworthy Augmented 40 
Intelligence in Health Care.37 41 
 42 
• The AMA Intelligent Platform’s CPT® Developer Program allows developers to access 43 
the latest content and resources,  Access the Developer Portal on the AMA Intelligent 44 
Platform.38 45 
 46 
• Kimberly Lomis, MD, AMA vice president of undergraduate medical innovations, co- 47 
authored a discussion paper,  Artificial Intelligence for Health Professions 48 
Educators  in NAM Perspectives.39 49 
 CLRPD Rep. 2 -I-23 -- page 9 of 18 
 
The technological capacity exists to use AI algorithms and tools to  transform health care, but real 1 
challenges remain in ensuring that tools are developed, implemented and maintained responsibly, 2 
according to a  JAMA  Viewpoint column, “Artificial Intelligence in Health Care: A Report From the 3 
National Academy of Medicine.”40 The NAM report recommends that people developing, using, 4 
implementing, and regulating health care AI do seven key things:41 5 
 6 
• Promotion of population-representative data with accessibility, standardization and quality 7 
is imperative : This is the way to ensure accuracy for all populations. While there is a lot of 8 
data now, there are issues with data quality, appropriate consent, interoperability, and scale 9 
of data transfers.  10 
 11 
• Prioritize ethical, equitable and inclusive medical AI while addressing explicit and implicit 12 
bias:  Underlying biases need to be scrutinized to understand their potential to worsen or 13 
address existing inequity and whether and how it should be deployed.  14 
 15 
• Contextualize the dialogue of transparency and trust, which means accepting differential 16 
needs:  AI developers, implementers, users, and regulators should collaboratively define 17 
guidelines for clarifying the level of transparency needed across a spectrum and there 18 
should be a clear separation of data, performance, and algorithmic transparency.  19 
 20 
• Focus in the near term on augmented intelligence rather than AI autonomous agents : Fully 21 
autonomous AI concerns the public and faces technical and regulatory challenges. 22 
Augmented intelligence —supporting data synthesis, interpretation and decision-making by 23 
clinicians and patients—is where opportunities are now.  24 
 25 
• Develop and deploy appropriate training and educational programs : Curricula must be 26 
multidisciplinary and engage AI developers, implementers, health care system leadership, 27 
frontline clinical teams, ethicists, humanists, patients, and caregivers.  28 
 29 
• Leverage frameworks and best practices for learning health care systems, human factors, 30 
and implementation science:  Health care delivery systems should have a robust and mature 31 
information technology governance strategy before embarking on a substantial AI 32 
deployment and integration.  33 
 34 
• Balance innovation with safety through regulation and legislation to promote trust : AI 35 
developers, health system leaders, clinical users, and informatics and health IT experts 36 
should evaluate deployed clinical AI for effectiveness and safety based on clinical data.  37 
 38 
The AMA recently developed a ChatGPT primer for physicians with questions regarding the 39 
technology and use in medical practice. The primer outlines considerations for physicians and 40 
patients when considering utilizing the tool  and is available on the AMA website.42  41 
 42 
Researchers from the University of Arizona Health Sciences found that patients are almost evenly 43 
split about whether they would prefer a human clinician or an AI -driven diagnostic tool, with 44 
preferences varying based on patient demographics and clinician support of the technology.43 The 45 
results of the study,  demonstrated that many patients do not believe that the diagnoses provided by 46 
AI are as trustworthy as those given by human health care providers. However, patients’ trust in 47 
their clinicians supported one of the study’s additional findings: that patients were more likely to 48 
trust AI if a physician supported its use.44 49 
 
 CLRPD Rep. 2 -I-23 -- page 10 of 18 
 
Health systems are watching to see where generative AI could add the most value since OpenAI 1 
launched ChatGPT in late 2022:  45 2 
 3 
• UC San Diego Health, Madison Wisconsin -based UW Health, and Palo, Alto -based 4 
Stanford Health Care are starting to use the integration to automatically draft message 5 
responses.  6 
 7 
• OpenAI’s GPT -4 has shown the potential to increase the power and accessibility of self - 8 
service reporting through SlicerDicer, making it easier for health care organizations to 9 
identify operational improvements, including ways to reduce costs and find answers to 10 
questions locally and in a broader context.46 11 
 12 
• AI already supports health systems to automate business office and clinical functions, 13 
connect patients, support clinical trials, and provide insight for precision medicine and care 14 
decisions.  15 
 16 
• Epic Systems and Microsoft have expanded  their partnership once again and will integrate 17 
conversational, ambient , and generative AI technologies into Epic’s electronic health 18 
record (EHR).  The new integrations are a part of a move to integrate Azure OpenAI 19 
Services and Nuance ambient technologies into the Epic ecosystem. 47 48 20 
 21 
Here are the capabilities that will be added to Epic’s E HR according to the press release:  22 
 23 
o Note summarization: This feature builds upon the AI -assisted Epic In Basket and 24 
will use suggested text and rapid review with in -context summaries to help support 25 
faster documentation.  26 
 27 
o Embedded ambient clinical documentation: Epic will embed Nuance’s Dragon 28 
Ambient eXperience Express AI technology into its Epic Hyperdrive platform and 29 
Haiku mobile application.  30 
 31 
o Reducing manual and labor -intensive processes: “Epic will demonstrate an AI - 32 
powered solution that provides medical coding staff with suggestions based on 33 
clinical documentation in the EHR to improve accuracy and streamline the entire 34 
coding and billing processes.”  35 
 36 
o Advancing medicine for better patient outcomes: Using Azure OpenAI Service, 37 
Epic will now use generative AI exploration for some of its users via SlicerDicer. 38 
This aims to “fill gaps in clinical evidence using real -world data and to study rare 39 
diseases.”  40 
 41 
Since generative AI models are so new, the long -term effect of them is still unknown. This means 42 
there are some inherent risks involved in using them — some known and some unknown. The 43 
outputs generative AI models produce may often sound extremely convincing. This is by design; 44 
however, sometimes the information they generate is incorrect. Worse, sometimes it is biased 45 
(because some models may be built on the gender, racial, and myriad other biases of the internet 46 
and society more generally) and can be manipulated to enable unethical or criminal activity. For 47 
example, ChatGPT will not give instructions on how to hotwire a car, but if you say you need to 48 
hotwire a car to save a baby, the algorithm is happy to comply. Organizations that rely on 49 
 CLRPD Rep. 2 -I-23 -- page 11 of 18 
 
generative AI models should reckon with reputational and legal risks involved in unintentionally 1 
publishing biased, offensive, or copyrighted content.49 2 
 3 
These risks can be mitigated, however, in a few ways. For one, it is crucial to carefully select the 4 
initial data used to train these models to avoid including toxic or biased content. Next, rather than 5 
employing an off -the-shelf generative AI model, organizations could consider using smaller, 6 
specialized models. Organizations with more resources could also customize a general model based 7 
on their own data to fit their needs and minimize biases.50 Organizations should also keep a  human 8 
in the loop (that is, to make sure a real human checks the output of a generative AI model before it 9 
is published or used) and avoid using generative AI models for critical decisions, such as those 10 
involving significant resources or human welfare. It cannot be emphasized enough that this is a 11 
new field.51 12 
 13 
At their core, all AI innovations utilize sophisticated statistical techniques to discern patterns 14 
within extensive datasets using increasingly powerful yet cost -effective computational 15 
technologies. These three components —big data, advanced statistical methods, and computing 16 
resources —have not only become available recently but are also being democratized and made 17 
readily accessible to everyone at a pace unprecedented in previous technological innovations. This 18 
progression allows us to identify patterns that were previously indiscernible, which creates 19 
opportunities for important advances but also possible harm to patients. Privacy regulations, most 20 
notably HIPAA, were established to protect patient confidentiality, operating under the assumption 21 
that de -identified data would remain anonymous. However, given the advancements in AI 22 
technology, the current landscape has become riskier. Now, it is  easier than ever to integrate 23 
various datasets from multiple sources, increasing the likelihood of accurately identifying 24 
individual patients.52 25 
 26 
Researchers at Mack Institute for Technological Innovation –  The Wharton School, University of 27 
Pennsylvania  Cornell Tech , and Johnson College of Business –  Cornell University  found that 28 
despite their remarkable performance, LLMs sometimes produce text that is semantically or  29 
syntactically plausible but is, in fact, factually incorrect or nonsensical (i.e., hallucinations). The 30 
models are optimized to generate the most statistically likely sequences of words with an injection 31 
of randomness. They are not designed to exercise any judgment on the veracity or feasibility of the  32 
output. Further, the underlying optimization algorithms provide no performance guarantees,  and 33 
their output can thus be of inconsistent quality. Hallucinations and inconsistency are critical flaws 34 
that limit the use of LLM -based solutions to low -stakes settings or in conjunction with expensive 35 
human supervision.  To achieve high variability in quality and high productivity, most research on 36 
ideation and brainstorming recommends enhancing performance by generating many ideas while 37 
postponing evaluation or judgment of ideas (Girotra et al., 2010). This is hard for human ideators to 38 
do, but LLMs are designed to do exactly this — quickly generate many somewhat plausible 39 
solutions without exercising much judgment. Further, the hallucinations and inconsistent behavior 40 
of LLMs increase the variability in  quality, which, on average, improves the quality of the best 41 
ideas. For ideation, an LLM’s lack of  judgment and inconsistency could be prized features, not 42 
bugs.  Thus, the researchers hypothesize that LLMs will be excellent ideators.53 43 
 44 
The landscape of risks and opportunities is likely to change rapidly in the coming weeks, months, 45 
and years. New use cases are being tested monthly, and new models are likely to be developed in 46 
the coming years. As generative AI becomes increasingly, and seamlessly, incorporated into 47 
business, society, and our personal lives, we can also expect a new regulatory climate to take 48 
shape. As organizations begin experimenting—and creating value —with these tools, physicians 49 
will do well to keep a finger on the pulse of benefits and drawbacks with the use of generative AI 50 
in medicine and health care.54 51 
 CLRPD Rep. 2 -I-23 -- page 12 of 18 
 
 
ETHICS FRAMEWORK FOR USE OF GENERATIVE AI IN HEALTH CARE 1 
 2 
A new paper  published by leading Australian AI ethicist Stefan Harrer PhD proposes for the first 3 
time a comprehensive ethical framework for the responsible use, design, and governance of 4 
Generative AI applications in health care and medicine. The study highlights and explains many 5 
key applications for health care:55 6 
 7 
• assisting clinicians with the generation of medical reports or preauthorization letters,  8 
• helping medical students to study more efficiently,  9 
• simplifying medical jargon in clinician -patient communication,  10 
• increasing the efficiency of clinical trial design,  11 
• helping to overcome interoperability and standardization hurdles in EHR mining,  12 
• making drug discovery and design processes more efficient.  13 
 14 
However, the paper also highlights that the inherent danger of LLM -driven generative AI arising 15 
from the ability of LLMs to produce and disseminate false, inappropriate, and dangerous content at 16 
unprecedented scale is increasingly being marginalized in an ongoing hype around the recently 17 
released latest generation of powerful LLM systems authoritatively and convincingly.  18 
 19 
Dr. Harrer proposes a regulatory framework with 10 principles for mitigating the risks of 20 
generative AI in health care:  21 
 22 
1. Design AI as an assistive tool for augmenting the capabilities of human decision 23 
makers, not for replacing them.  24 
2. Design AI to produce performance, usage and impact metrics explaining when and 25 
how AI is used to assist decision making and scan for potential bias.  26 
3. Study the value systems of target user groups and design AI to adhere to them.  27 
4. Declare the purpose of designing and using AI at the outset of any conceptual or 28 
development work.  29 
5. Disclose all training data sources and data features.  30 
6. Design AI systems to label any AI -generated content clearly and transparently as such.  31 
7. Ongoingly audit AI against data privacy, safety, and performance standards.  32 
8. Maintain databases for documenting and sharing the results of AI audits, educate users 33 
about model capabilities, limitations, and risks, and improve performance and 34 
trustworthiness of AI systems by retraining and redeploying updated algorithms.  35 
9. Apply fair -work and safe-work standards when employing human developers.  36 
10. Establish legal precedence to define under which circumstances data may be used for 37 
training AI, and establish copyright, liability, and accountability frameworks for 38 
governing the legal dependencies of training data, AI -generated content, and the 39 
impact of decisions humans make using such data.  40 
 41 
Dr. Harrer said, “Without human oversight, guidance and responsible design and operation, LLM - 42 
powered generative AI applications will remain a party trick with substantial potential for creating 43 
and spreading misinformation or harmful and inaccurate content at unprecedented scale.” He 44 
predicts that the field will move from the current competitive LLM arms race to a phase of more 45 
nuanced and risk-conscious experimentation with research -grade generative AI applications in 46 
health, medicine, and biotech, which will deliver first commercial product offerings for niche 47 
applications in digital health data management within the next 2 years. “I am inspired by thinking 48 
about the transformative role generative AI and LLMs could one day play in health care and 49 
 CLRPD Rep. 2 -I-23 -- page 13 of 18 
 
medicine, but I am also acutely aware that we are by no means there yet and that despite the 1 
prevailing hype, LLM -powered generative AI may only gain the trust and endorsement of 2 
clinicians and patients if the research and development community aims for equal levels of ethical 3 
and technical integrity as it progresses this transformative technology to market maturity.” 4 
 5 
“Ethical AI requires a lifecycle approach from data curation to model testing, to ongoing 6 
monitoring. Only with the right guidelines and guardrails can we ensure our patients benefit from 7 
emerging technologies while minimizing bias and unintended consequences,” said John Halamka, 8 
MD, MS, President of Mayo Clinic Platform, and a co -founder of the Coalition for Health AI 9 
(CHAI).56 10 
 11 
“This study provides important ethical and technical guidance to users, developers, providers, and 12 
regulators of generative AI and incentivizes them to responsibly and collectively prepare for the 13 
transformational role this technology could play in health and medicine,” said Brian Anderson, 14 
MD, Chief Digital Health Physician at MITRE.57 15 
 16 
REGULATORY FRAMEWORK FOR USE OF GENERATIVE AI IN MEDICINE  17 
 18 
AMA’s President  Jesse Ehrenfeld, MD, MPH co -chairs the AI committee of the Association for the 19 
Advancement of Medical Instrumentation (AAMI)58 and co -authored an article, “Artificial 20 
Intelligence in Medicine & ChatGPT: De -Tether the Physician,” published in the  Journal of 21 
Medical Systems.  He says, “A competitive marketplace requires regulatory flexibility from the 22 
Federal Drug Administration ( FDA). Regulation of AI systems is still in its infancy but AI that 23 
improves physician workflow should require less regulatory oversight than algorithms that make 24 
diagnoses, recommend treatments, or otherwise impact clinical decision making. While AI 25 
algorithms may one day independently learn to read CT scans, identify skin lesions, and provide 26 
medical diagnoses, the low-hanging fruit is in improving physician efficiency, e.g., de -tethering 27 
clinicians from the computer. This should be embraced by the health care industry now.” 28 
Physicians have a critical role to play in this endeavor. Without physician knowledge, expertise and 29 
guidance on design and deployment, most of these digital innovations will fail, he predicted. They 30 
will not be able to achieve their most basic task of streamlining workflows and improving patient 31 
outcomes.  32 
 33 
Dr. Ehrenfeld said, the AMA is working closely with the FDA to support efforts that create new 34 
pathways and approaches to regulate AI tools:  35 
 36 
• Any regulatory framework should ensure that only safe, clinically validated, high -quality 37 
tools enter the marketplace. “We can ’t allow AI to  introduce additional bias” into clinical 38 
care, cautioning that this could erode public confidence in the tools that come to the 39 
marketplace.59 40 
 41 
• There also needs to be a balance between strong oversight and ensuring the regulatory 42 
system is not  overly burdensome to developers, entrepreneurs, and manufacturers, “while 43 
also thinking about how we limit liability in appropriate ways for physicians,” added Dr. 44 
Ehrenfeld.  45 
 46 
• The FDA has a medical device action plan  on AI and machine -learning software that 47 
would enable the agency to track and evaluate a software product from premarket 48 
development to post market performance.60 The AMA has weighed in on the plan, saying 49 
the agency must guard against bias in AI and focus on patient outcomes.61 50 
 
 CLRPD Rep. 2 -I-23 -- page 14 of 18 
 
In April 2023, the European Union (EU) proposed new copyright rules for generative AI.62 In its 1 
most recent AI Act, the EU requires that AI -generated content be disclosed to consumers to prevent 2 
copyright infringement, illegal content, and other malfeasance related to end- user lack of 3 
understanding about these systems.63 As more chatbots mine, analyze, and present content in 4 
accessible ways for users, findings are often not attributable to any one or multiple sources, and 5 
despite some permissions of content use granted under the fair use doctrine in the United States 6 
that protects copyright -protected work, consumers are often left in the dark around the generation 7 
and explanation of the process and results.64 8 
 9 
In the United States, the U .S. Food and Drug Administration (FDA) published a regulatory 10 
framework for AI applications in medicine in April 2019 and an action plan in January 2021. The 11 
FDA’s leadership role in formulating regulatory guidance is a manifestation of the broader U .S. 12 
national approach to the regulation of AI. In contrast to the EU, the U .S. policy sustains from broad 13 
and comprehensive regulation of AI and instead delegates responsibilities to specific federal 14 
agencies, with an overarching mandate to avoid overregulation and promote innovation.65 15 
 16 
CLRPD DISCUSSION  17 
 18 
Generative AI systems are not sentient, they simply use massive amounts of text to predict one 19 
word after another, and their outputs may mix truth with patently false statements. As such, 20 
physicians will need to learn how to integrate these tools into clinical practice, defining clear 21 
boundaries between full, supervised, and proscribed autonomy. Physicians should be clear -eyed 22 
about the risks inherent to any new technology, especially ones that carry existential implications, 23 
while cautiously optimistic about a future of improved health care system efficiency, better patient 24 
outcomes, and reduced burnout.  25 
 26 
Extant AI -assistant programs and rapidly developing systems are incredibly sophisticated, and as 27 
physicians have already begun to demonstrate on social media, they might soon be able to reliably 28 
perform test result notifications, work letters, prior authorizations, and the like —the mundane 29 
necessities that not only cumulatively consume valuable time but are a substantial contributor to 30 
physician burnout.  31 
 32 
Projecting further into an AI -enhanced future, imagine that instead of writing discharge 33 
instructions, physicians could ask a generative AI system to create a synopsis of the patient’s 34 
hospital course. With the time saved, physicians could step away from the computer, go to the 35 
patient’s room, and explain the most salient follow -up items face-to -face, prepped with materials 36 
that are compatible with best practices in health literacy. Integrating AI into routine clinical 37 
practice will require careful validation, training, and ongoing monitoring to ensure its accuracy, 38 
safety, and effectiveness in supporting physicians to deliver care. While AI can be an asset in the 39 
medical field, it cannot replace the human element. However, AI can and should be used to 40 
enhance the practice of medicine, empowering physicians with the latest technological tools to 41 
serve our patients better. Moreover, Generative AI may provide physicians with a future that 42 
enables them to fully experience the reason why they decided to pursue medicine in the first 43 
place—to interact with their patients. 44 
 45 
The AMA has addressed the importance of AI, has advocated for the use of the expression 46 
augmented intelligence, and has assumed thought leadership with its reports and guidelines for 47 
physicians. AMA policy states, “ as a leader in American medicine, our AMA has a unique 48 
opportunity to ensure that the evolution of AI in medicine benefits patients, physicians , and the 49 
health care community. ” 50 
 
 CLRPD Rep. 2 -I-23 -- page 15 of 18 
 
Relevant AMA Policy  1 
 2 
Augmented Intelligence in Health Care H-480.93966 3 
Augmented Intelligence in Health Care H-480.94067 4 
Augmented Intelligence in Medical Education H -295.85768 5 
Professionalism in Health Care Systems  E-11.2.1 69 6 
Assessing the Potentially Dangerous Intersection Between AI and Misinformation H-480.93570 7 
Three AI -related resolutions were introduced for consideration by the House of Delegates at the 8 
2023 AMA Annual Meeting. They were combined into one measure , RES 609 -A-23 Encouraging 9 
Collaboration Between Physicians and Industry in AI (Augmented Intelligence) Development, 10 
urging  physicians to educate patients on benefits and risks and directing the AMA to work with the 11 
federal government to protect patients from false or misleading AI-generated medical advice.  The 12 
HOD action was referral. A BOT report  is scheduled for  consideration by the HOD  at the 2024 13 
AMA Annual M eeting .  14 
Specifically, the AMA was directed to:  15 
 16 
• Study and develop recommendations on the benefits of and unforeseen consequences to the 17 
medical profession of large -language models (LLMs) such as generative pretrained 18 
transformers (GPTs) and other augmented intelligence-generated medical advice or 19 
content.  20 
 21 
• Propose appropriate state and federal regulations with a report back at the 2024 AMA 22 
Annual Meeting.  23 
 24 
• Work with the federal government and other appropriate organizations to protect patients 25 
from false or misleading AI -generated medical advice.  26 
 27 
• Encourage physicians to educate patients about the benefits and risks  of LLMs including 28 
GPTs.  29 
 30 
• Support publishing groups and scientific journals to establish guidelines to regulate the use 31 
of augmented intelligence in scientific publications that include detailing the use of 32 
augmented intelligence in the methods and exclusion of augmented intelligence systems as 33 
authors and the responsibility of authors to validate veracity of any text generated by 34 
augmented intelligence. 35 
 
 REFERENCES 
 
1 AMA. “CLRPD Report 1 -A-18: A Primer on Artificial and Augmented Intelligence. ” https://www.ama -
assn.org/sites/ama -assn.org/files/corp/media -browser/public/hod/a18- clrpd01.pdf . Accessed August 22, 
2023.  
2 McKinsey. “What is ChatGPT, DALL -E, and generative AI? ” https://www.mckinsey.com/featured -
insights/mckinsey -explainers/what -is-generative -ai.  A ccessed August 22, 2023.  
3 Friedland A.  “Wha t are Generative AI, Large Language Models, and Foundation Models? ” Center for 
Security and Emerging Technology. https://cset.georgetown.edu/article/what -are-generative -ai-large-
language -models -and-foundation- models/ .  A ccessed August 22, 2023.  
4 Callaway E.  “DeepMind’s AI Makes Gigantic Leap in Solving Protein Structures. ” Scientific American. 
https://www.scientificamerican.com/article/deepminds -ai-makes -gigantic -leap-in-solving -protein -structures/ . 
 Accessed August 22, 2023.  
 CLRPD Rep. 2 -I-23 -- page 16 of 18 
 
 
5 Gordon R. “3 Questions: Jacob Andreas on Large Language Models. ” MIT News | Massachusetts Institute 
of Technology. https://news.mit.edu/2023/3 -questions -jacob -andreas -large -language -models -0511 . 
 Published May 11, 2023.  Accessed August 22, 2023.  
6 Chen BX. “Get the Best from  ChatGPT with These Golden Prompts. ” 
 The New York Times. https://www.nytimes.com/2023/05/25/technology/ai -chatbot -chatgpt -prompts.html .  
Published May 25, 2023.  Accessed August 22, 2023.  
7 Pifer R. “Hurtling into the Future: The Potential and Thorny Ethics of G enerative AI in H ealthcare. ” 
Healthcare Dive. https://www.healthcaredive.com/news/generative -AI-healthcare- gpt-potential/648104/ . 
 Accessed August 22, 2023.  
8 AXIOS. “ Terms -of-service land grab: Tech firms seek private data to train AI .” Accessed August 24 , 2023 . 
9 Mello M , Guha N. “ ChatGPT and Physicians’ Malpractice Risk. ” JAMA. 
https://jamanetwork.com/journals/jama -health -forum/fullarticle/2805334 ? Accessed August 22, 2023.  
10 Bolton E, Hall D,  Yasunaga M,  et. All. “Stanford CRFM Introduces PubMed GPT 2.7B. ” Stanford HAI. 
https://hai.stanford.edu/news/stanford -crfm -introduces -pubmedgpt -27b. Accessed August 22, 2023.  
11 Heaven WD. “Why Meta’s latest Large Language Model Survived Only Three Days Online. ” MIT 
Technology Review. https://www.technologyreview.com/2022/11/18/1063487/meta -large- language -model-
ai-only-survived- three -days-gpt-3- science/ . Accessed August 22, 2023.  
12 Field H. “OpenAI is Pursuing a New Way to F ight A.I. “ Hallucinations.” CNBC. 
https://www.cnbc.com/2023/05/31/openai -is-pursuing -a-new-way-to-fight -ai-hallucinations.html . 
 Published May 31, 2023. Accessed August 22, 2023.  
13 Hu L. “Generative AI and Future. Medium. ” https://pub.towardsai.net/generative -ai-and-future -
c3b1695876f2.  P ublished November 15, 2022. Accessed August 22, 2023. 
14 “OpenAI. DALL·E 2. OpenAI. ” https://openai.com/dall-e-2 .  Published 2023. Accessed August 22, 2023. 
15 Crouse M. “ GPT -4 Cheat Sheet: What is GPT -4, and What is it C apable Of?” TechRepublic. 
https://www.techrepublic.com/article/gpt -4-cheat -sheet/ .  Published July 7, 2023. Accessed August 22, 2023.  
16 Agomuoh F. “ChatGPT: How to U se the AI Chatbot Faking Over the W orld.” 
https://www.digitaltrends.com/computing/how -to-use-openai -chatgpt -text-generation -chatbot/ . 
 Published January 19, 2023. Accessed August 22, 2023.  
17 “OpenAI GPT -4.” https://openai.com/research/gpt-4 . Accessed August 22, 2023.  
18 NBC News. “ChatGPT passes MBA exam given by a Wharton Professor ”. 
https://www.nbcnews.com/tech/tech -news/chatgpt -passes -mba-exam -wharton -professor- rcna67036 . 
Accessed August 22, 2023.  
19 Digital Trends. “How to Use Google Bard ”. https://www.digitaltrends.com/computing/how -to-use-google -
bard/ . Accessed August 22, 2023.  
20 Google Technology. “ AI LaMDA ”. https://blog.google/technology/ai/lamda/ . Accessed August 22, 2023.  
21 ZdNet. “Google’s ChatGPT Competi tor Spaces Out at the Start ”. https://www.zdnet.com/article/googles -
chatgpt -competitor -spaces -out-at-the-start/. Accessed August 22, 2023.  
22 Google Technology. “ AI MedPalm Research ”. https://blog.google/technology/health/ai -llm-medpalm -
research -thecheckup/ . Accessed August 22, 2023.  
23 Cloud Google . “MedPalm2 Large Language Model”. https://cloud.google.com/blog/topics/healthcare -life-
sciences/sharing -google -med-palm -2-medical -large -language -model . A ccessed August 22, 2023.  
24 Forbes . “Big Techs New Weapon Against Physician Burnout. 
https://www.forbes.com/sites/davidchou/2023/08/02/big -techs -new-weapon -against -physician -burnout -
generative -ai/?sh=79c0241b1f7b . A ccessed August 22, 2023.   
25 Zdnet.  “Microsoft Taps ChatGPT  to Improve Bing and Edge Browser.” 
https://www.zdnet.com/article/microsoft- taps-chatgpt -to-improve -bing-and-the-edge -browser/ . Accessed 
August 22, 2023.  
26 ZdNet. “What is the New Bing . Here’s everything You Need to Know.” 
https://www.zdnet.com/article/what -is-the-new-bing-heres -everything -you-need -to-know/ . Accessed August 
22, 2023.  
27 Digital Trends. “How to Use Microsoft ChatGPT Bing Edge.” 
https://www.digitaltrends.com/computing/how -to-use-microsoft -chatgpt -bing-edge/  Accessed August 22, 
2023.  
28 McKinsey. “What is Generative AI? ” https://www.mckinsey.com/featured -insights/mckinsey -
explainers/what -is-generative -ai. A ccessed August 22, 2023.  
 CLRPD Rep. 2 -I-23 -- page 17 of 18 
 
 
29 McKinsey. The state of AI in 2023: Generative AI’s breakout year . McKinsey.  “The state of AI in 2023: 
Generative AI’s breakout year | McKinsey .” Accessed August 24, 2023.  
30 McKinsey.  The economic potential of generative AI . McKinsey.  “The-economic -potential -of-generative -
ai-the-next-productivity -frontier -vf.pdf.” (mckinsey.com). Accessed August 24, 2023.  
31AMA. AMA digital health care 2022 study findings.  https://www.ama -assn.org/about/research/ama -digital-
health -care-2022- study -findings . A ccessed August 22, 2023.  
32 AMA. “Digital Machine Learning 101, Promise and Pitfalls .” https://www.ama -assn.org/practice-
management/digital/machine -learning -101-promise -pitfalls -and-medicine-s- future . A ccessed August 22, 
2023.  
33 AMA. “ 10 Ways AI Could Transform Primary Care.” https://www.ama -assn.org/practice-
management/digital/10 -ways -health -care-ai-could- transform -primary -care. A ccessed August 22, 2023.  
34 AMA. “Ophthalmo logist Doing Health Care AI the Right Way.” https://www.ama -assn.org/practice-
management/digital/ophthalmologist- doing- health -care-ai-right- way. A ccessed August 22, 2023.  
35 AMA. “Health2047’s New Company Plans to Use AI Research  to Improve Clinical Decision Making .” 
https://health2047.com/media_coverage/health2047s -new-company -plans -to-use- ai-research -access -to-
improve -clinical -decision -making/ . A ccessed August 22, 2023.  
36 Recoverx. “Scaling equitable, science -based healthcare. ”  https://www.recoverx.com/ . Accessed August 
22, 2023.  
37 Crigger E. Rinebold K. Hanson C. Kao A. Blake D. Irons M. “Trustworthy Augmented Intelligence in 
Health Care.” https://rdcu.be/de3cZ . Accessed August 22, 2023.  
38 AMA. CPT Developer Program. https://platform.ama -assn.org/ama/#/dev -program . Accessed August 22, 
2023.  
39 National Academy of Medicine.  “Artificial Intelligence for Health Professions Educators .” 
https://nam.edu/artificial -intelligence -for-health -professions -educators/ . Accessed August 22, 2023.   
40 JAMA. “Artificial Intelligence in Health Care.  A Report From the National Academy of Medicine .” 
 https://jamanetwork.com/journals/jama/fullarticle/2757958 . Accessed August 22, 2023.  
41 AMA. “7 Tips on Responsible Use of Health Care AI.” https://www.ama -assn.org/practice-
management/digital/7 -tips-responsible -use-health -care-ai. A ccessed August 22, 2023.  
42 AMA. “ChatGPT, What Physicians Should Consider.” https://www.ama -assn.org/system/files/chatgpt-
what -physicians -should- consider.pdf . A ccessed August 22, 2023.  
43 Health Sciences, Arizona Edu. “ Would You Trust AI Doctor , New Research Shows Patients Are Split.”   
https://healthsciences.arizona.edu/newsroom/news -releases/0523/would -you-trust-ai-doctor -new-research -
shows -patients -are-split. A ccessed August 22, 2023.  
44 Robertson, C . Woods A. Bergstrand K. Findley J. Balser C. Slepian M.  “Diverse patients’ attitudes 
towards Artificial Intelligence (AI) in diagnosis.” https://doi.org/10.1371/journal.pdig.0000237 . Accessed 
August 22, 2023.  
45 Fierce Healthcare. “Epic Moves Forward Bringing Generative AI  Healthcare. Here’s Why a Handful of 
Health Systems Are.” https://www.fiercehealthcare.com/health -tech/epic -moves -forward -bring -generative -
ai-healthcare- heres -why-handful- health -systems- are. A ccessed August 22, 2023.  
46 UCDavis Edu. “Epic Slicer Dicer.” https://health.ucdavis.edu/data/epic -slicer -dicer.html . Accessed August 
22, 2023.  
47 Microsoft. “Azure AI Overview.” https://learn.microsoft.com/en -us/azure/ai -services/openai/overview . 
Accessed August 22, 2023.  
48 Microsoft Blog.  “Microsoft and Epic expand AI collaboration to accelerate generative AI’s impact in 
healthcare, addressing the industry’s most pressing needs.” Accessed August 22, 2023.  
49 Science Direct . “Opinion Paper: “So what if ChatGPT wrote it?” Multidisciplinary perspectives on 
opportunities, challenges , and implications of generative conversational AI for research, practice, and 
policy .” 
https://www.sciencedirect.com/science/article/pii/S0268401223000233 . Accessed August 22, 2023.  
50 Frontier.  “AI chatbots not yet ready for clinical use .”  
https://www.frontiersin.org/articles/10.3389/fdgth.2023.1161098/full?utm_source=S . Accessed August 22, 
2023.  
51 JAMA . “Garbage in, Garbage out—Words of Caution on Big Data and Machine Learning in Medical 
Practice.” https://jamanetwork.com/journals/jama -health -
 CLRPD Rep. 2 -I-23 -- page 18 of 18 
 
 
forum/fullarticle/2801776?widget=personalizedcontent&previousarticle=2806091 . Accessed August 22, 
2023.  
52 The Brookings Institute. “Around the Halls, What Should the Regulation of Generative AI Look Like?”  
https://www.brookings.edu/blog/techtank/2023/06/02/around- the-halls -what -should- the-regulation -of-
generative -ai-look-
ike/?utm_campaign=Brookings%20Brief&utm_medium=email&utm_content=260949068&utm_source=hs_
email . A ccessed August 22, 2023.  
53 Girotra  K. Meincke  L. T erwiesch  C. Ulrich  K. “Working Paper, Ideas are Dimes a Dozen:  
Large Language Models for Idea Generation in Innovation.” 
https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4526071 . Accessed August 22, 2023.  
54 Al-Medfa  MK. Al-Ansari  A. Darwish  AH. Qreeballa  TA. Jahrami  H. “Physicians’ attitudes and 
knowledge toward artificial intelligence in medicine: Benefits and drawbacks .” 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10073828/ . Accessed August 22, 2023.  
55 Harner S. “ Attention is not all you need: the complicated case of ethically using large language models in 
healthcare and medicine .” 
 https://www.thelancet.com/journals/ebiom/article/PIIS2352 -3964(23)00077- 4/fulltext . Accessed August 22, 
2023.  
56 Coalition For Health AI. “Coalition for Health AI (CHAI ™) Updates Progress and Plans to Issue 
Guidelines for the Responsible Use of AI in Healthcare.”  https://www.coalitionforhealthai.org/updates/october -6th-2022
. Accessed August 22, 2023.  
57 “MITRE’s Brian Anderson, M.D., Explains Coalition’s AI Blueprint”  
https://www.hcinnovationgroup.com/analytics -ai/artifical- intelligence -machine -
learning/article/53063115/mitres -brian -anderson -md-explains -coalitions -ai-blueprint . Accessed August 22, 
2023.  
58 Digiorgio A. Ehrenfeld J. “ Artificial Intelligence in Medicine & ChatGPT: De -Tether the Physician .” 
https://link.springer.com/article/10.1007/s10916 -023-01926 -3. Accessed August 23, 2023.  
59 AMA. “ Feds Warn Algorithms Can Induce Bias in Clinical Decisions.” https://www.ama -
assn.org/delivering- care/health -equity/feds -warned -algorithms -can-introduce -bias-clinical -decisions . 
Accessed August 22, 2023.  
60 FDA. “Artificial Intelligence and Machine Learning Software Regulation.” https://www.fda.gov/medical-
devices/software- medical -device- samd/artificial -intelligence -and-machine -learning -software -medical -
device#regulation. A ccessed August 22, 2023.  
61 AMA. “FDA Must Guard Agains t Bias AI Focus in Patient Outcomes.” https://www.ama -
assn.org/practice- management/digital/fda -must -guard- against -bias-ai-focus -patient- outcomes . A ccessed 
August 22, 2023.  
62 Reuters. “ Lawmaker s Committee Reaches Deal on AI Act.” https://www.reuters.com/technology/eu -
lawmakers -committee -reaches -deal-artificial -intelligence -act-2023 -04-27/. A ccessed August 22, 2023.  
63 The AI Act. “What is the AI Act?” https://artificialintelligenceact.eu/ . Accessed August 22, 2023.  
64 U.S. Copyright Office Fair Use Index . “2013 Joint Strategic Plan on Intellectual Property Enforcement .”  
https://www.copyright.gov/fair -use/. Accessed August 22, 2023.  
65 Vokinger K. Gasser U.  “Regulating AI in medicine in the United States and Europe.”  
“https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7611759/ . Accessed August 22, 2023.  
66 AMA PolicyFinder. Augmented Intelligence in Health Care H -480.939 . Accessed August 22, 2023.  
67 AMA PolicyFinder. Augmented  Intelligence in Health Care H -480.940 . Accessed August 22, 2023.  
68 AMA PolicyFinder. Augmented Intelligence in Medical Education H -295.857 . Accessed August 22, 2023.  
69 AMA PolicyFinder. Professionalism in Health Care Systems: Augmented Intelligence E -11.2.1 . Accessed 
August 22, 2023.  
70 AMA PolicyFinder. Assessing the Potentially Dangerous Intersection Between AI and Misinformation H -
480.935 . A ccessed August 22, 2023.  
 
 
