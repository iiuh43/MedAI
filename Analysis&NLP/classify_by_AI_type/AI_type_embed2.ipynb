{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb515db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text saved to: /Users/winnie/Documents/GitHub/MedAI/Analysis&NLP/classify_by_AI_type/061104_1.txt\n"
     ]
    }
   ],
   "source": [
    "# convert pdf to text file\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "def pdf_to_text_file(pdf_path, output_txt_path):\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    \n",
    "    # Extract all text\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    \n",
    "    # Save to a text file\n",
    "    with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_text)\n",
    "    \n",
    "    doc.close()\n",
    "    print(f\"Text saved to: {output_txt_path}\")\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"/Users/winnie/Documents/GitHub/MedAI/Analysis&NLP/classify_by_AI_type/061104_1.pdf\"\n",
    "output_txt_path = \"/Users/winnie/Documents/GitHub/MedAI/Analysis&NLP/classify_by_AI_type/061104_1.txt\"\n",
    "\n",
    "pdf_to_text_file(pdf_path, output_txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499448fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text pre-processing\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_pdf_text(text):\n",
    "    # Step 1: Remove page numbers (assuming they appear on a line by themselves)\n",
    "    text = re.sub(r'\\n\\d+\\n', '\\n', text)\n",
    "\n",
    "    # Step 2: Remove common headers and footers (heuristic)\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        if len(line.strip()) < 3:  # Skip short lines (e.g., single letters or digits)\n",
    "            continue\n",
    "        if re.match(r'^(Page|PAGE)\\s*\\d+', line.strip()):  # Page indicators\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "    text = '\\n'.join(cleaned_lines)\n",
    "\n",
    "    # Step 3: Fix hyphenated line breaks (e.g., \"inter-\\nview\" â†’ \"interview\")\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 4: Merge lines that are artificially split (end of line not a sentence end)\n",
    "    # First, normalize line endings\n",
    "    text = re.sub(r'\\r\\n?', '\\n', text)\n",
    "\n",
    "    # Then merge lines that are not paragraph breaks\n",
    "    text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', text)  # convert single line breaks to spaces\n",
    "\n",
    "    # Step 5: Normalize multiple newlines (paragraph breaks)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "\n",
    "    # Step 6: Normalize whitespace\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # collapse tabs and spaces\n",
    "    text = re.sub(r' +\\n', '\\n', text)   # trim trailing spaces on lines\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d260cce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/winnie/Documents/GitHub/MedAI/Analysis&NLP/classify_by_AI_type/061104_1.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "clean_text = preprocess_pdf_text(raw_text)\n",
    "\n",
    "with open(\"/Users/winnie/Documents/GitHub/MedAI/Analysis&NLP/classify_by_AI_type/cleaned_061104_1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5facf4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 147\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    encoding = tiktoken.encoding_for_model(\"text-embedding-3-large\")\n",
    "    tokens = encoding.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        decoded = encoding.decode(chunk)\n",
    "        chunks.append(decoded)\n",
    "    return chunks\n",
    "\n",
    "# Run the chunking\n",
    "chunks = chunk_text(clean_text)\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e36b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    \"Clinical Decision Support\": (\n",
    "        \"AI used by healthcare providers for their decision making. Topics include: Diagnostic Support, Treatment Recommendations, and Personalized Care Plans.\"\n",
    "    ),\n",
    "    \"Clinical Documentation AI\": (\n",
    "        \"AI used in the automation of documentation tasks. Topics include: Summary Reports, Clinical Notes, Auto-documentation, and Structured Data Extraction from Free Text.\"\n",
    "    ),\n",
    "    \"Medical Imaging AI\": (\n",
    "        \"AI used in the evaluation of medical imaging outputs. Topics include: Radiology, Pathology, and Ophthalmology.\"\n",
    "    ),\n",
    "    \"Predictive Analytics\": (\n",
    "        \"AI used in the prediction of medical outcomes. Topics include: Risk Stratification, Early Warning Systems, Readmission Prediction, Mortality Prediction, and Treatment Effectiveness Prediction.\"\n",
    "    ),\n",
    "    \"Operational and Administrative Automation\": (\n",
    "        \"AI used in administrative tasks. Topics include: Scheduling Optimization, Resource Allocation, Revenue Cycle Management, Supply Chain, and Workflow Optimization.\"\n",
    "    ),\n",
    "    \"Patient-facing AI\": (\n",
    "        \"AI used directly by patients for monitoring or education. Topics include: Chatbots/Virtual Assistants, Symptom Checkers, Patient Education Tools, and Wearable/Remote Monitoring Assistants.\"\n",
    "    ),\n",
    "    \"Robotics and Surgical AI\": (\n",
    "        \"AI used in robotic-assisted procedures. Topics include: Robotic-assisted surgeries, Precision tools, and Rehabilitation robotics.\"\n",
    "    ),\n",
    "    \"Education and Training AI\": (\n",
    "        \"AI used for medical education and staff training. Topics include: AI-based simulations, Virtual patients, and Curriculum personalization tools.\"\n",
    "    ),\n",
    "    \"Research and Clinical Trial AI\": (\n",
    "        \"AI used in medical research. Topics include: Patient Recruitment, Trial Design, Data Analysis, and Drug Discovery.\"\n",
    "    ),\n",
    "    \"Public Health AI\": (\n",
    "        \"AI used to monitor and analyze population health. Topics include: Epidemiological Modeling, Public Health Surveillance, and Health Equity Analysis.\"\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d04ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, AzureOpenAI\n",
    "import json\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load from .env file\n",
    "\n",
    "AZURE_ENDPOINT = os.getenv(\"AZURE_ENDPOINT_embeddings\")\n",
    "AZURE_API_KEY = os.getenv(\"AZURE_API_KEY_embeddings\")\n",
    "AZURE_API_VERSION = \"2025-04-01-preview\"\n",
    "\n",
    "openai = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    api_key=AZURE_API_KEY,\n",
    "    api_version=AZURE_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff0188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model=\"embedding3large\"):\n",
    "    response = openai.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "# Create embeddings for each category\n",
    "category_embeddings = {label: get_embedding(desc) for label, desc in categories.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42ff6b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical Decision Support: 3072 dimensions\n",
      "Clinical Documentation AI: 3072 dimensions\n",
      "Medical Imaging AI: 3072 dimensions\n",
      "Predictive Analytics: 3072 dimensions\n",
      "Operational and Administrative Automation: 3072 dimensions\n",
      "Patient-facing AI: 3072 dimensions\n",
      "Robotics and Surgical AI: 3072 dimensions\n",
      "Education and Training AI: 3072 dimensions\n",
      "Research and Clinical Trial AI: 3072 dimensions\n",
      "Public Health AI: 3072 dimensions\n",
      "\n",
      "Clinical Decision Support:\n",
      "[-0.009869870729744434, 0.01331905648112297, -0.021285202354192734, -0.03554558381438255, 0.013571950607001781, 0.007853747345507145, 0.015749644488096237, -0.004632868338376284, 0.02700340375304222, 0.00822606310248375]\n",
      "\n",
      "Clinical Documentation AI:\n",
      "[-0.0009404133888892829, -0.01200233306735754, -0.02751505747437477, -0.02618955634534359, 0.03326860070228577, 0.022650033235549927, -0.0018362185219302773, 0.003503108164295554, -0.002281390130519867, 0.021091477945446968]\n",
      "\n",
      "Medical Imaging AI:\n",
      "[-0.018428850919008255, 0.021242313086986542, -0.028403853997588158, -0.0046913474798202515, 0.018213465809822083, -0.002032692776992917, 0.006676924880594015, 0.0003270733868703246, 0.016961542889475822, 0.000692007364705205]\n",
      "\n",
      "Predictive Analytics:\n",
      "[-0.03135932609438896, 0.01925157941877842, -0.016612038016319275, -0.021116331219673157, 0.022495195269584656, 0.0017399961361661553, -0.02266591228544712, -0.003926482051610947, -0.02212749794125557, -0.022442666813731194]\n",
      "\n",
      "Operational and Administrative Automation:\n",
      "[-0.012871040031313896, 0.0015916485572233796, -0.016440685838460922, -0.024682795628905296, 0.0049082632176578045, 0.0033247768878936768, 0.0029511249158531427, 0.01469939574599266, -0.001315944129601121, 0.007973660714924335]\n",
      "\n",
      "Patient-facing AI:\n",
      "[-0.03125328570604324, 0.007037981878966093, -0.02329763025045395, -0.03421980142593384, 0.0350288487970829, 0.019462136551737785, -0.0005730767734348774, -0.013544089160859585, 0.023537348955869675, -0.025769727304577827]\n",
      "\n",
      "Robotics and Surgical AI:\n",
      "[0.022508999332785606, 0.008200408890843391, -0.02929118648171425, -0.046562597155570984, 0.06841163337230682, 0.01700461469590664, -0.00992754939943552, 0.0024643351789563894, -0.004816336091607809, 0.01425242330878973]\n",
      "\n",
      "Education and Training AI:\n",
      "[-0.038117870688438416, 0.02185683138668537, -0.03188493475317955, -0.030278220772743225, 0.02692628651857376, 0.01918359287083149, -0.018615704029798508, 0.01009043212980032, 0.02558274194598198, -0.005509223323315382]\n",
      "\n",
      "Research and Clinical Trial AI:\n",
      "[-0.017451852560043335, -0.007547926157712936, -0.023487284779548645, -0.028213828802108765, 0.02181481570005417, 0.014579568058252335, -0.00448658037930727, 0.00911859329789877, 0.0012879830319434404, 0.007446123752743006]\n",
      "\n",
      "Public Health AI:\n",
      "[-0.03207394480705261, 0.014173142611980438, -0.027890386059880257, -0.018571242690086365, 0.044034626334905624, -0.012021023780107498, -0.005205981899052858, -0.001439774059690535, -0.0017397968331351876, -0.02712608128786087]\n"
     ]
    }
   ],
   "source": [
    "for category, embedding in category_embeddings.items():\n",
    "    print(f\"{category}: {len(embedding)} dimensions\")\n",
    "\n",
    "for category, embedding in category_embeddings.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(embedding[:10])  # print only the first 10 numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba57803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification complete.\n",
      "Number of categories with matches: 10\n"
     ]
    }
   ],
   "source": [
    "#combined text embedding with cosine similarity comparison\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def classify_chunks(chunks, category_embeddings, threshold=0.75):\n",
    "    classification = defaultdict(list)\n",
    "\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk_embedding = get_embedding(chunk)\n",
    "        for label, cat_embedding in category_embeddings.items():\n",
    "            sim = cosine_similarity(chunk_embedding, cat_embedding)\n",
    "            if sim >= threshold:\n",
    "                classification[label].append((idx, sim, chunk[:300]))\n",
    "    return classification\n",
    "\n",
    "# Classify\n",
    "results = classify_chunks(chunks, category_embeddings, threshold=0.5)\n",
    "\n",
    "print(\"Classification complete.\")\n",
    "print(f\"Number of categories with matches: {len(results)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to visualize the cosine similarity by chunk and category\n",
    "for category, matches in results.items():\n",
    "    print(f\"\\n=== {category} ({len(matches)} matching chunks) ===\")\n",
    "    for idx, sim, preview in matches:\n",
    "        print(f\"[Chunk {idx}] (similarity={sim:.2f}): {preview}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to visualize the cosine similarity by chunk and category\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for category, matches in results.items():\n",
    "    for idx, sim, preview in matches:\n",
    "        rows.append({\n",
    "            \"Category\": category,\n",
    "            \"Chunk Index\": idx,\n",
    "            \"Similarity\": round(sim, 4),\n",
    "            \"Chunk Preview\": preview\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.head())  # or display full table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a68455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings for 147 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate embeddings for each chunk\n",
    "chunk_embeddings = []\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    embedding = get_embedding(chunk)\n",
    "    chunk_embeddings.append((idx, chunk, embedding))  # store index, text, and vector\n",
    "\n",
    "print(f\"Created embeddings for {len(chunk_embeddings)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81dfc0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def classify_from_embeddings(chunk_embeddings, category_embeddings, threshold=0.75):\n",
    "    classification = defaultdict(list)\n",
    "\n",
    "    for idx, text, chunk_embedding in chunk_embeddings:\n",
    "        for label, cat_embedding in category_embeddings.items():\n",
    "            sim = cosine_similarity(chunk_embedding, cat_embedding)\n",
    "            if sim >= threshold:\n",
    "                classification[label].append((idx, sim, text[:300]))  # preview\n",
    "    return classification\n",
    "\n",
    "# Run classification\n",
    "results = classify_from_embeddings(chunk_embeddings, category_embeddings, threshold=0.60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cb250b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Category  Chunk Index  Similarity  \\\n",
      "0    Medical Imaging AI            2      0.6020   \n",
      "1    Medical Imaging AI            5      0.6668   \n",
      "2  Predictive Analytics            5      0.6003   \n",
      "\n",
      "                                       Chunk Preview  \n",
      "0   in developing medical imaging-based machine l...  \n",
      "1  AI/ML) in medical imaging provide important me...  \n",
      "2  AI/ML) in medical imaging provide important me...  \n"
     ]
    }
   ],
   "source": [
    "# alternative way to visualize the cosine similarity by chunk and category\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for category, matches in results.items():\n",
    "    for idx, sim, preview in matches:\n",
    "        rows.append({\n",
    "            \"Category\": category,\n",
    "            \"Chunk Index\": idx,\n",
    "            \"Similarity\": round(sim, 4),\n",
    "            \"Chunk Preview\": preview\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.head())  # or display full table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "402f95ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Medical Imaging AI (2 matching chunks) ===\n",
      "[Chunk 2] (sim=0.60):  in developing medical imaging-based machine learning methods, also known as medical imaging artificial intelligence (AI), for the detection, diagnosis, prognosis, and risk assessment of disease with ...\n",
      "\n",
      "[Chunk 5] (sim=0.67): AI/ML) in medical imaging provide important methods for leveraging large amounts of data to build models to detect disease and provide diagnosis, prognosis, and risk assessment tools to support decisi...\n",
      "\n",
      "\n",
      "=== Predictive Analytics (1 matching chunks) ===\n",
      "[Chunk 5] (sim=0.60): AI/ML) in medical imaging provide important methods for leveraging large amounts of data to build models to detect disease and provide diagnosis, prognosis, and risk assessment tools to support decisi...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to visualize the cosine similarity by chunk and category\n",
    "for label, matches in results.items():\n",
    "    print(f\"\\n=== {label} ({len(matches)} matching chunks) ===\")\n",
    "    for idx, sim, preview in matches:\n",
    "        print(f\"[Chunk {idx}] (sim={sim:.2f}): {preview[:200]}...\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
