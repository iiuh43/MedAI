{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2960cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import fitz \n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "csv_path = r\"C:\\Users\\joeva\\Downloads\\ai_gov\\ai_filtered.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(row, save_dir):\n",
    "    url = row['PDF Link']\n",
    "    filename = os.path.join(save_dir, os.path.basename(url))\n",
    "    if os.path.exists(filename):\n",
    "        return f\"Already exists: {filename}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"Accept\": \"application/pdf\",\n",
    "        \"Referer\": \"https://www.hhs.gov/\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return f\"Downloaded: {filename}\"\n",
    "        else:\n",
    "            return f\"Failed ({response.status_code}): {url}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {url} -> {e}\"\n",
    "\n",
    "def batch_download(csv_path, save_dir, max_threads=10):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        futures = [executor.submit(download_pdf, row, save_dir) for _, row in df.iterrows()]\n",
    "        for f in concurrent.futures.as_completed(futures):\n",
    "            print(f.result())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_dir = r\"C:\\Users\\joeva\\Downloads\\ai_gov\\data\\raw\"\n",
    "    batch_download(csv_path, save_dir, max_threads=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5979d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(path):\n",
    "    doc = fitz.open(path)\n",
    "    return \" \".join([page.get_text() for page in doc])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def process_pdfs_from_csv(csv_path, input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        pdf_name = os.path.basename(row[\"PDF Link\"])\n",
    "        pdf_path = os.path.join(input_dir, pdf_name)\n",
    "        out_path = os.path.join(output_dir, pdf_name.replace(\".pdf\", \".txt\"))\n",
    "\n",
    "        if os.path.exists(out_path):\n",
    "            print(f\"Already processed: {out_path}\")\n",
    "            continue\n",
    "\n",
    "        if os.path.exists(pdf_path):\n",
    "            try:\n",
    "                raw_text = extract_text_from_pdf(pdf_path)\n",
    "                cleaned = clean_text(raw_text)\n",
    "                with open(out_path, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                    f.write(cleaned)\n",
    "                print(f\"Processed: {pdf_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {pdf_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Missing PDF file: {pdf_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_pdfs_from_csv(r\"C:\\Users\\joeva\\Downloads\\ai_gov\\ai_filtered.csv\", r\"C:\\Users\\joeva\\Downloads\\ai_gov\\data\\raw\", r\"C:\\Users\\joeva\\Downloads\\ai_gov\\data\\processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34114218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_folder(folder_path, filenames):\n",
    "    docs = []\n",
    "    for filename in filenames:\n",
    "        txt_path = os.path.join(folder_path, filename.replace(\".pdf\", \".txt\"))\n",
    "        if os.path.exists(txt_path):\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "                text = f.read().strip()\n",
    "                if text:\n",
    "                    docs.append(preprocess(text))\n",
    "                else:\n",
    "                    docs.append(\"\") \n",
    "        else:\n",
    "            docs.append(\"\") \n",
    "    return docs\n",
    "\n",
    "raw_df = pd.read_csv(csv_path)\n",
    "raw_df = raw_df.copy()\n",
    "raw_df['filename'] = raw_df['URL'].apply(lambda url: os.path.basename(url))\n",
    "docs = load_documents_from_folder(r\"C:\\Users\\joeva\\Downloads\\ai_gov\\data\\processed\", raw_df['filename'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d6e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_words=100):\n",
    "    words = text.split()\n",
    "    chunks = [\" \".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]\n",
    "    return chunks\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered = [w for w in words if w.lower() not in stop_words]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "def get_sentence_transformer_embeddings(texts, model, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch, show_progress_bar=False, convert_to_numpy=True)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def get_avg_embedding_for_doc(text, model, max_words=100, emb_size=384):\n",
    "    filtered_text = remove_stopwords(text)\n",
    "    chunks = chunk_text(filtered_text, max_words=max_words)\n",
    "    if not chunks:\n",
    "        return np.zeros(emb_size)\n",
    "    chunk_embeddings = get_sentence_transformer_embeddings(chunks, model)\n",
    "    return np.mean(chunk_embeddings, axis=0) if chunk_embeddings else np.zeros(emb_size)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    csv_path = r\"C:\\Users\\joeva\\Downloads\\ai_gov\\ai_filtered.csv\"\n",
    "    folder = r\"C:\\Users\\joeva\\Downloads\\ai_gov\\data\\processed\"\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    filenames = [os.path.basename(url) for url in df[\"URL\"].dropna()]\n",
    "    docs = load_documents_from_folder(folder, filenames)\n",
    "\n",
    "    print(f\"Loaded and preprocessed {len(docs)} documents\")\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "    emb_size = model.get_sentence_embedding_dimension()\n",
    "\n",
    "    batch_size = 10\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(docs), batch_size), desc=\"Embedding batches\", unit=\"batch\"):\n",
    "        batch_docs = docs[i:i+batch_size]\n",
    "        for doc in batch_docs:\n",
    "            avg_emb = get_avg_embedding_for_doc(doc, model=model, emb_size=emb_size)\n",
    "            embeddings.append(avg_emb)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    print(f\"Generated embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18074b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_df['is_labeled'] = hand_df['Focus Area'].notna() & (hand_df['Focus Area'].str.strip() != '')\n",
    "\n",
    "hand_df['is_unclassified'] = hand_df.apply(\n",
    "    lambda row: 1 if row['is_labeled'] and not row['theme_list'] else 0,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embeddings \n",
    "\n",
    "labeled_mask = hand_df['is_labeled'].values\n",
    "X_labeled = embeddings[labeled_mask]\n",
    "df_labeled = hand_df[labeled_mask].copy()\n",
    "y_labeled = df_labeled['is_unclassified'].values\n",
    "\n",
    "X_train = X_labeled[:246]\n",
    "y_train = y_labeled[:246]\n",
    "\n",
    "unlabeled_mask = ~hand_df['is_labeled'].values\n",
    "X_unlabeled = embeddings[unlabeled_mask]\n",
    "df_unlabeled = hand_df[unlabeled_mask].copy()\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1.0, 10.0],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs']\n",
    "}\n",
    "lr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "grid = GridSearchCV(lr, param_grid, cv=3, scoring='f1', verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_clf = grid.best_estimator_\n",
    "\n",
    "y_proba_unlabeled = best_clf.predict_proba(X_unlabeled)[:, 1]\n",
    "threshold = 0.5\n",
    "y_pred_unlabeled = (y_proba_unlabeled >= threshold).astype(int)\n",
    "\n",
    "df_unlabeled['predicted_is_unclassified'] = y_pred_unlabeled\n",
    "df_unlabeled['predicted_label'] = ['Unclassified' if pred == 1 else 'Classified' for pred in y_pred_unlabeled]\n",
    "\n",
    "hand_df.loc[df_unlabeled.index, 'predicted_is_unclassified'] = df_unlabeled['predicted_is_unclassified']\n",
    "hand_df.loc[df_unlabeled.index, 'predicted_label'] = df_unlabeled['predicted_label']\n",
    "\n",
    "hand_df.to_csv(\"updated_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"updated_with_predictions.csv\")\n",
    "\n",
    "labeled_df = df[(df['is_labeled'] == True) & (df['Focus Area'] != 'Unclassified')]\n",
    "\n",
    "retained_unlabeled_df = df[(df['is_labeled'] == False) & (df['predicted_label'] != 'Unclassified')]\n",
    "\n",
    "eliminated_df = df[(df['is_labeled'] == False) & (df['predicted_label'] == 'Unclassified')]\n",
    "\n",
    "final_df = pd.concat([labeled_df, retained_unlabeled_df], ignore_index=True)\n",
    "\n",
    "cols_to_drop = [\n",
    "    'Name',\n",
    "    'Notes/Questions',\n",
    "    'hand_order',\n",
    "    'filename',\n",
    "    'theme_list',\n",
    "    'is_labeled',\n",
    "    'is_unclassified',\n",
    "    'predicted_is_unclassified',\n",
    "    'predicted_label'\n",
    "]\n",
    "\n",
    "final_df_clean = final_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "eliminated_df_clean = eliminated_df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "final_df_clean.to_csv(\"FINAL_dataset.csv\", index=False)\n",
    "eliminated_df_clean.to_csv(\"eliminated_docs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
